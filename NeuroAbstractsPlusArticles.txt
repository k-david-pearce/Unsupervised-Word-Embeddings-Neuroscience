J Neurosci. 2008 Nov 12; 28(46): 11825–11829. 
doi: 10.1523/JNEUROSCI.3463-08.2008
PMCID: PMC2613689
NIHMSID: NIHMS79805
PMID: 19005047
Symposium
Habenula: Crossroad between the Basal Ganglia and the Limbic System
Okihide Hikosaka,1 Susan R. Sesack,2 Lucas Lecourtier,3 and Paul D. Shepard4
There is a growing awareness that emotion, motivation, and reward values are important determinants of our behavior. The habenula is uniquely positioned both anatomically and functionally to participate in the circuit mediating some forms of emotive decision making. In the last few years there has been a surge of interest in this structure, especially the lateral habenula (LHb). The new studies suggest that the LHb plays a pivotal role in controlling motor and cognitive behaviors by influencing the activity of dopamine and serotonin neurons. Further, dysfunctions of the LHb have also been implicated in psychiatric disorders, such as depression, schizophrenia, and drug-induced psychosis.
The habenula is a pair of small nuclei located above the thalamus at its posterior end close to the midline (Fig. 1). It is regarded as part of the epithalamus, which includes the pineal body and the habenula. In many vertebrates, the habenula is divided into the medial habenula (MHb) and the lateral habenula (LHb). The habenula is a phylogenetically well preserved structure that was thought to have evolved in close relation to the pineal body (Concha and Wilson, 2001; Butler and Hodos, 2005). However, recent studies using behaving animals and human subjects have instead suggested that the habenula is involved in motivational control of behavior. There have been extensive reviews on the neural connections and the functions of the habenula (Sutherland, 1982; Klemm, 2004; Lecourtier and Kelly, 2007; Geisler and Trimble, 2008). In this article we focus on the functions of the LHb, particularly in relation to its efferent connection to dopamine (DA) neurons.
The LHb is comprised of multiple subnuclei that collectively form medial and lateral divisions (Andres et al., 1999). The medial division of the LHb receives afferents (Fig. 2) primarily from limbic brain regions that are directly or indirectly innervated by the cerebral cortex: the lateral hypothalamic and lateral preoptic areas, basal forebrain structures including the ventral pallidum, substantia innominata, and diagonal band, and parts of the extended amygdala, including the bed nucleus of the stria terminalis (Herkenham and Nauta, 1977; Hikosaka, 2007a; Geisler and Trimble, 2008). The lateral division of the LHb is mainly innervated by the basal ganglia, in particular the globus pallidus internal segment (rodent entopeduncular nucleus), which receives cortical input by way of the striatum. Through these parallel circuits, extensive information processed by the cerebral cortex ultimately funnels to the LHb, which serves as a convergence point for limbic and basal ganglia circuits. The efferents of the LHb (Fig. 2) primarily descend to brainstem structures but include less dense ascending projections to forebrain regions (Herkenham and Nauta, 1979; Araki et al., 1988; Hikosaka, 2007a; Geisler and Trimble, 2008). Within the brainstem, LHb efferents mainly target the nuclei containing monoamine neurons: the dopaminergic ventral tegmental area (VTA) and substantia nigra pars compacta (SNc), serotonergic dorsal and median raphe, and cholinergic laterodorsal tegmentum. Some DA neurons in the VTA project back to the LHb (Gruber et al., 2007). Hence, the LHb forms a node of connection between the cortex and brainstem monoamine neurons that operates in parallel to the medial forebrain bundle. LHb neurons are heterogeneous in their neurochemical expression patterns, although the majority appear to have a glutamatergic phenotype (Geisler and Trimble, 2008). In their physiological regulation of brainstem DA and serotonin neurons, LHb cells exert a relatively short latency and potent inhibitory influence (Wang and Aghajanian, 1977; Christoph et al., 1986; Park, 1987; Shepard et al., 2006; Ji and Shepard, 2007; Lecourtier and Kelly, 2007; Matsumoto and Hikosaka, 2007).
The results of physiological experiments indicate a probable disynaptic circuit involving LHb projections to intermediate, short-range GABA cells that ultimately inhibit monoamine cell firing. Hence, it can be hypothesized that LHb projections to the VTA synapse onto GABA cells and not DA neurons, thus providing an indirect inhibitory action through local GABA connections. However, preliminary ultrastructural analyses have so far failed to support this hypothesis (Bell et al., 2007). Specifically, LHb axons with excitatory morphology have been found to synapse directly onto DA neurons, and conversely, excitatory-type synapses onto GABA cells have not yet been observed (Bell et al., 2007). Additional investigation is necessary before a disynaptic inhibitory circuit involving VTA GABA neurons can be ruled out. Nevertheless, it must be considered that the dominant inhibition of DA neurons induced by the LHb may involve GABA cells that are extrinsic to the VTA. Similar circuit questions remain to be addressed in the raphe nuclei and other brainstem targets of the LHb.
Motor functions of the LHb
The inhibitory effects of the LHb on DA neurons are crucial for reward-based motor control. DA neurons carry signals that are essential for initiation and learning of body movements. First, they represent reward prediction error (a difference between the actual reward value and the expected reward value), which is thought to act as a teaching signal for motor behavior that would maximize the gain of reward (Schultz, 1998). Second, they encode novel environmental events that would cause immediate changes in motor behavior (Redgrave and Gurney, 2006). DA neurons in the SNc mainly project to the dorsal and ventral striatum and are thought to modulate the effectiveness of the inputs to the striatum, especially those from the cerebral cortex (Reynolds and Wickens, 2002). Indeed, the sensorimotor activity of many neurons in the striatum and other basal ganglia areas is modulated by the expected reward value of the impending action (for review, see Hikosaka et al., 2006). Several lines of evidence suggest that these reward modulations are caused, at least in part, by the DA inputs to the striatum (Nakamura and Hikosaka, 2006). Thus, the basal ganglia contribute to the selection of body movements (or actions) based on their reward outcomes.
A recent study suggested that the LHb plays an instructive role in reward-based action selection (Matsumoto and Hikosaka, 2007). Monkeys were trained to make a saccade to one of two targets, but the amount of reward after the saccade was determined by the position of the target. LHb neurons were inhibited by the target indicating a large reward and excited by the target indicating a small reward. They responded to the reward only when it was not predicted: inhibited by unexpectedly large rewards and excited by unexpectedly small rewards. These response patterns were exactly opposite to those of DA neurons. The excitation of LHb neurons in response to a small reward-predicting target started before the inhibition of DA neurons in the same condition. Further, weak electrical stimulation in the LHb induced clear inhibitions in DA neurons, consistent with other reports (Ji and Shepard, 2007). These results suggest that the LHb suppresses less rewarding motor behaviors by inhibiting DA neurons.
Cognitive functions of the LHb
In addition to its functions in motivational and motor control of behavior, a role for the LHb in cognition has recently been emphasized. In rats, studies of brain metabolic activity showed a selective activation of the LHb after the retrieval phase of an odor discrimination task for food reward (Tronel and Sara, 2002). Also, there was a decreased metabolism in the LHb during a water-maze task in aged memory-impaired rats compared with young unimpaired animals (Villarreal et al., 2002). Lesions of the habenula (including LHb and MHb) in rats induced impairments of spatial reference memory, a model of declarative memory (Lecourtier et al., 2004), and attention (Lecourtier and Kelly, 2005). In the latter study, using the five-choice serial reaction time task (5-CSRTT), two types of deficits were observed, i.e., an immediate occurrence of an impulsive mode of behavior, represented by a large increase in premature responding, and a late-occurring decline of accuracy. Finally, habenula lesions depress synaptic plasticity in the hippocampus–nucleus accumbens pathway (Lecourtier et al., 2006), involved in many cognitive processes.
Several lines of evidence point to alterations of subcortical DA transmissions as being one of the effectors of habenula lesion-induced cognitive deficits. In fact, the LHb tonically inhibits DA transmission, as evidenced by a marked and sustained increase in DA release in the striatum and nucleus accumbens (Lecourtier et al., 2008). The latter findings might explain the occurrence of impulsivity in the 5-CSRTT after habenula lesions, as well as the postlesion depressed plasticity of the hippocampus–nucleus accumbens pathway, given that similar plasticity changes occur with increased DA tone (Beurrier and Malenka, 2002; Hunt et al., 2005).
On the other hand, impaired spatial reference memory induced by habenula lesions might involve other neurotransmitter systems such as serotonin and acetylcholine. Indeed, the LHb participates in the regulation of the cholinergic medial septum (MS)–hippocampus pathway, which is involved in spatial reference memory. Also, the LHb modulates the activity of raphe serotonin neurons (Wang and Aghajanian, 1977) and serotonin transmission (Reisine et al., 1982), whereas serotonin regulates the neuronal activity of the MS and consequently of the MS-hippocampus pathway (Cassel and Jeltsch, 1995; Koenig et al., 2008). Finally, the fact that habenula lesions induced a hypersensitivity to stress could also account for some cognitive deficits (Thornton and Davies, 1991; Heldt and Ressler, 2006).
Clinical implications
Given the reciprocal connections between the LHb, DA and serotonin neurons, it is not surprising that these pathways have been linked to neuropsychiatric conditions in which monoamine-containing neurons have been implicated. Early studies showing that serotonin neurons in the dorsal raphe are inhibited by habenular stimulation (Wang and Aghajanian, 1977; Reisine et al., 1982; Park, 1987) led to the suggestion that increased activation of the LHb may contribute to major depressive disorders (for review, see Shumake and Gonzalez-Lima, 2003). In support of this notion, patients experiencing transient depressive episodes elicited by a reduction in dietary tryptophan show increased regional cerebral blood flow in the habenula that correlates directly with ratings of depressed mood and inversely with plasma tryptophan levels (Morris et al., 1999). Enhanced LHb metabolism and reduced brain serotonin levels have been observed in several animal models of depressed behavior and can be attenuated or blocked by antidepressant drugs (Caldecott-Hazard et al., 1988) or lesions of the LHb (Yang et al., 2008). Together, these data suggest that habenular efferents play a prominent role in reducing brain serotonin activity associated with some forms of depressive mood and behavior and have led to the proposition that functional inactivation of the LHb by deep brain stimulation may have utility in the treatment of major depressive illness in humans (Sartorius and Henn, 2007; Hauptman et al., 2008).
Habenular efferents also exert a powerful inhibitory influence over DA-containing neurons in the SN and VTA. Lesions of the LHb increase DA turnover in terminal fields (Lisoprawski et al., 1980; Nishikawa et al., 1986; Lecourtier et al., 2008), whereas local stimulation transiently inhibits the spontaneous firing of SN and VTA DA neurons (Christoph et al., 1986; Ji and Shepard, 2007). A functional magnetic resonance imaging (fMRI) study conducted in healthy human volunteers has provided evidence that the habenula participates in a circuit encoding the neural representation of negative outcomes (Ullsperger and von Cramon, 2003). Subsequent studies showing that neural activity in the LHb is transiently increased in primates experiencing the loss of an anticipated reward, and that these changes are both antecedent and opposite to those occurring in DA neurons, provide compelling support for the role of LHb in the generation of a teaching signal used to modify action selection (Matsumoto and Hikosaka, 2007). Many patients with DA-related disorders, including Parkinson's disease and schizophrenia, do not make optimal use of negative feedback to guide action selection and problem solving (Frank et al., 2004; Cools et al., 2006; Prentice et al., 2008), which could be attributable to disruption in the habenulomesencephalic reward circuit. Preliminary fMRI results indicate that in contrast to healthy volunteers, schizophrenics do not engage habenular activity in response to informative error (Shepard et al., 2006; Holcomb et al., 2008). These differences may partially account for the maladaptive behaviors (e.g., perseveration) exhibited by patients making inconsistent use of negative feedback.
Conclusions
Recent studies reviewed above have suggested that the LHb controls cognitive and motor behaviors based on the motivational values associated with the behaviors. Such functions are, at least partly, mediated by the inhibitory effects of the LHb on midbrain DA neurons. A hypothetical scheme in Figure 3 shows how the LHb might control motor and cognitive behaviors through its influences on DA neurons.
Introduction
The habenula is a pair of small nuclei located above the thalamus at its posterior end close to the midline (Fig. 1). It is regarded as part of the epithalamus, which includes the pineal body and the habenula. In many vertebrates, the habenula is divided into the medial habenula (MHb) and the lateral habenula (LHb). The habenula is a phylogenetically well preserved structure that was thought to have evolved in close relation to the pineal body (Concha and Wilson, 2001; Butler and Hodos, 2005). However, recent studies using behaving animals and human subjects have instead suggested that the habenula is involved in motivational control of behavior. There have been extensive reviews on the neural connections and the functions of the habenula (Sutherland, 1982; Klemm, 2004; Lecourtier and Kelly, 2007; Geisler and Trimble, 2008). In this article we focus on the functions of the LHb, particularly in relation to its efferent connection to dopamine (DA) neurons.
Connections between the LHb, basal ganglia, and limbic system
The LHb is comprised of multiple subnuclei that collectively form medial and lateral divisions (Andres et al., 1999). The medial division of the LHb receives afferents (Fig. 2) primarily from limbic brain regions that are directly or indirectly innervated by the cerebral cortex: the lateral hypothalamic and lateral preoptic areas, basal forebrain structures including the ventral pallidum, substantia innominata, and diagonal band, and parts of the extended amygdala, including the bed nucleus of the stria terminalis (Herkenham and Nauta, 1977; Hikosaka, 2007a; Geisler and Trimble, 2008). The lateral division of the LHb is mainly innervated by the basal ganglia, in particular the globus pallidus internal segment (rodent entopeduncular nucleus), which receives cortical input by way of the striatum. Through these parallel circuits, extensive information processed by the cerebral cortex ultimately funnels to the LHb, which serves as a convergence point for limbic and basal ganglia circuits. The efferents of the LHb (Fig. 2) primarily descend to brainstem structures but include less dense ascending projections to forebrain regions (Herkenham and Nauta, 1979; Araki et al., 1988; Hikosaka, 2007a; Geisler and Trimble, 2008). Within the brainstem, LHb efferents mainly target the nuclei containing monoamine neurons: the dopaminergic ventral tegmental area (VTA) and substantia nigra pars compacta (SNc), serotonergic dorsal and median raphe, and cholinergic laterodorsal tegmentum. Some DA neurons in the VTA project back to the LHb (Gruber et al., 2007). Hence, the LHb forms a node of connection between the cortex and brainstem monoamine neurons that operates in parallel to the medial forebrain bundle. LHb neurons are heterogeneous in their neurochemical expression patterns, although the majority appear to have a glutamatergic phenotype (Geisler and Trimble, 2008). In their physiological regulation of brainstem DA and serotonin neurons, LHb cells exert a relatively short latency and potent inhibitory influence (Wang and Aghajanian, 1977; Christoph et al., 1986; Park, 1987; Shepard et al., 2006; Ji and Shepard, 2007; Lecourtier and Kelly, 2007; Matsumoto and Hikosaka, 2007).
The results of physiological experiments indicate a probable disynaptic circuit involving LHb projections to intermediate, short-range GABA cells that ultimately inhibit monoamine cell firing. Hence, it can be hypothesized that LHb projections to the VTA synapse onto GABA cells and not DA neurons, thus providing an indirect inhibitory action through local GABA connections. However, preliminary ultrastructural analyses have so far failed to support this hypothesis (Bell et al., 2007). Specifically, LHb axons with excitatory morphology have been found to synapse directly onto DA neurons, and conversely, excitatory-type synapses onto GABA cells have not yet been observed (Bell et al., 2007). Additional investigation is necessary before a disynaptic inhibitory circuit involving VTA GABA neurons can be ruled out. Nevertheless, it must be considered that the dominant inhibition of DA neurons induced by the LHb may involve GABA cells that are extrinsic to the VTA. Similar circuit questions remain to be addressed in the raphe nuclei and other brainstem targets of the LHb.
Motor functions of the LHb
The inhibitory effects of the LHb on DA neurons are crucial for reward-based motor control. DA neurons carry signals that are essential for initiation and learning of body movements. First, they represent reward prediction error (a difference between the actual reward value and the expected reward value), which is thought to act as a teaching signal for motor behavior that would maximize the gain of reward (Schultz, 1998). Second, they encode novel environmental events that would cause immediate changes in motor behavior (Redgrave and Gurney, 2006). DA neurons in the SNc mainly project to the dorsal and ventral striatum and are thought to modulate the effectiveness of the inputs to the striatum, especially those from the cerebral cortex (Reynolds and Wickens, 2002). Indeed, the sensorimotor activity of many neurons in the striatum and other basal ganglia areas is modulated by the expected reward value of the impending action (for review, see Hikosaka et al., 2006). Several lines of evidence suggest that these reward modulations are caused, at least in part, by the DA inputs to the striatum (Nakamura and Hikosaka, 2006). Thus, the basal ganglia contribute to the selection of body movements (or actions) based on their reward outcomes.
A recent study suggested that the LHb plays an instructive role in reward-based action selection (Matsumoto and Hikosaka, 2007). Monkeys were trained to make a saccade to one of two targets, but the amount of reward after the saccade was determined by the position of the target. LHb neurons were inhibited by the target indicating a large reward and excited by the target indicating a small reward. They responded to the reward only when it was not predicted: inhibited by unexpectedly large rewards and excited by unexpectedly small rewards. These response patterns were exactly opposite to those of DA neurons. The excitation of LHb neurons in response to a small reward-predicting target started before the inhibition of DA neurons in the same condition. Further, weak electrical stimulation in the LHb induced clear inhibitions in DA neurons, consistent with other reports (Ji and Shepard, 2007). These results suggest that the LHb suppresses less rewarding motor behaviors by inhibiting DA neurons.
Cognitive functions of the LHb
In addition to its functions in motivational and motor control of behavior, a role for the LHb in cognition has recently been emphasized. In rats, studies of brain metabolic activity showed a selective activation of the LHb after the retrieval phase of an odor discrimination task for food reward (Tronel and Sara, 2002). Also, there was a decreased metabolism in the LHb during a water-maze task in aged memory-impaired rats compared with young unimpaired animals (Villarreal et al., 2002). Lesions of the habenula (including LHb and MHb) in rats induced impairments of spatial reference memory, a model of declarative memory (Lecourtier et al., 2004), and attention (Lecourtier and Kelly, 2005). In the latter study, using the five-choice serial reaction time task (5-CSRTT), two types of deficits were observed, i.e., an immediate occurrence of an impulsive mode of behavior, represented by a large increase in premature responding, and a late-occurring decline of accuracy. Finally, habenula lesions depress synaptic plasticity in the hippocampus–nucleus accumbens pathway (Lecourtier et al., 2006), involved in many cognitive processes.
Several lines of evidence point to alterations of subcortical DA transmissions as being one of the effectors of habenula lesion-induced cognitive deficits. In fact, the LHb tonically inhibits DA transmission, as evidenced by a marked and sustained increase in DA release in the striatum and nucleus accumbens (Lecourtier et al., 2008). The latter findings might explain the occurrence of impulsivity in the 5-CSRTT after habenula lesions, as well as the postlesion depressed plasticity of the hippocampus–nucleus accumbens pathway, given that similar plasticity changes occur with increased DA tone (Beurrier and Malenka, 2002; Hunt et al., 2005).
On the other hand, impaired spatial reference memory induced by habenula lesions might involve other neurotransmitter systems such as serotonin and acetylcholine. Indeed, the LHb participates in the regulation of the cholinergic medial septum (MS)–hippocampus pathway, which is involved in spatial reference memory. Also, the LHb modulates the activity of raphe serotonin neurons (Wang and Aghajanian, 1977) and serotonin transmission (Reisine et al., 1982), whereas serotonin regulates the neuronal activity of the MS and consequently of the MS-hippocampus pathway (Cassel and Jeltsch, 1995; Koenig et al., 2008). Finally, the fact that habenula lesions induced a hypersensitivity to stress could also account for some cognitive deficits (Thornton and Davies, 1991; Heldt and Ressler, 2006).
Clinical implications
Given the reciprocal connections between the LHb, DA and serotonin neurons, it is not surprising that these pathways have been linked to neuropsychiatric conditions in which monoamine-containing neurons have been implicated. Early studies showing that serotonin neurons in the dorsal raphe are inhibited by habenular stimulation (Wang and Aghajanian, 1977; Reisine et al., 1982; Park, 1987) led to the suggestion that increased activation of the LHb may contribute to major depressive disorders (for review, see Shumake and Gonzalez-Lima, 2003). In support of this notion, patients experiencing transient depressive episodes elicited by a reduction in dietary tryptophan show increased regional cerebral blood flow in the habenula that correlates directly with ratings of depressed mood and inversely with plasma tryptophan levels (Morris et al., 1999). Enhanced LHb metabolism and reduced brain serotonin levels have been observed in several animal models of depressed behavior and can be attenuated or blocked by antidepressant drugs (Caldecott-Hazard et al., 1988) or lesions of the LHb (Yang et al., 2008). Together, these data suggest that habenular efferents play a prominent role in reducing brain serotonin activity associated with some forms of depressive mood and behavior and have led to the proposition that functional inactivation of the LHb by deep brain stimulation may have utility in the treatment of major depressive illness in humans (Sartorius and Henn, 2007; Hauptman et al., 2008).
Habenular efferents also exert a powerful inhibitory influence over DA-containing neurons in the SN and VTA. Lesions of the LHb increase DA turnover in terminal fields (Lisoprawski et al., 1980; Nishikawa et al., 1986; Lecourtier et al., 2008), whereas local stimulation transiently inhibits the spontaneous firing of SN and VTA DA neurons (Christoph et al., 1986; Ji and Shepard, 2007). A functional magnetic resonance imaging (fMRI) study conducted in healthy human volunteers has provided evidence that the habenula participates in a circuit encoding the neural representation of negative outcomes (Ullsperger and von Cramon, 2003). Subsequent studies showing that neural activity in the LHb is transiently increased in primates experiencing the loss of an anticipated reward, and that these changes are both antecedent and opposite to those occurring in DA neurons, provide compelling support for the role of LHb in the generation of a teaching signal used to modify action selection (Matsumoto and Hikosaka, 2007). Many patients with DA-related disorders, including Parkinson's disease and schizophrenia, do not make optimal use of negative feedback to guide action selection and problem solving (Frank et al., 2004; Cools et al., 2006; Prentice et al., 2008), which could be attributable to disruption in the habenulomesencephalic reward circuit. Preliminary fMRI results indicate that in contrast to healthy volunteers, schizophrenics do not engage habenular activity in response to informative error (Shepard et al., 2006; Holcomb et al., 2008). These differences may partially account for the maladaptive behaviors (e.g., perseveration) exhibited by patients making inconsistent use of negative feedback.
Conclusions
Recent studies reviewed above have suggested that the LHb controls cognitive and motor behaviors based on the motivational values associated with the behaviors. Such functions are, at least partly, mediated by the inhibitory effects of the LHb on midbrain DA neurons. A hypothetical scheme in Figure 3 shows how the LHb might control motor and cognitive behaviors through its influences on DA neurons.
A robust effect occurs when LHb neurons are excited by a sensory event predicting a nonrewarding (or more accurately, less rewarding than predicted) or punishing outcome (Matsumoto and Hikosaka, 2007). This causes a transient inhibition of DA neurons (Ji and Shepard, 2007), and a reduction of DA levels, especially in the dorsal and ventral striatum. Although its functional significance is still under debate (Bayer and Glimcher, 2005), the reduction in DA transmission may then lead to suppression of motor and cognitive actions, presumably through D2 receptor-mediated LTD mechanisms (Lecourtier et al., 2006; Nakamura and Hikosaka, 2006; Hikosaka, 2007b; Kreitzer and Malenka, 2007). This mechanism thus could underlie a principle of behavior: if the outcome of an action is undesirable, avoid the action.
On the other hand, LHb neurons are inhibited by a sensory event predicting a rewarding (or more accurately, more rewarding than predicted) or nonpunishing outcome (Matsumoto and Hikosaka, 2007). This causes a disinhibition of DA neurons and a transient increase in DA concentration in the striatum (Lecourtier et al., 2008). This may lead to facilitation of motor and cognitive actions, presumably through D1-mediated LTP mechanisms (Reynolds and Wickens, 2002). This mechanism might underlie a second principle of behavior: if the outcome of an action is desirable, repeat the action.
If the LHb is damaged or does not work normally, these two functions would be deranged. Even if the outcome of an action is undesirable, it would be difficult to avoid the action. This may be expressed as impulsivity, attention deficits (Lecourtier and Kelly, 2005), stereotyped motor behaviors (Carvey et al., 1987), and increased bingeing behavior (Ellison, 2002). Or, even if the outcome of an action is desirable, it would be difficult to repeat the action. It may be expressed as deficits in motor or cognitive learning (Tronel and Sara, 2002; Lecourtier et al., 2004). These hypothetical deficits might have common grounds with some psychiatric symptoms (Sandyk, 1991; Shepard et al., 2006).
The LHb also exerts inhibitory control over serotonin neurons and dysfunctions of this mechanism may underlie mood disorders such as depression, as discussed above. However, it is unclear whether different kinds of information are processed between the LHb-DA pathway and the LHb-serotonin pathway. A hint on this question has been provided by a recent study showing that neurons in the monkey dorsal raphe, presumably including serotonin neurons, encode reward values, but in a manner different from DA neurons (Nakamura et al., 2008).
To summarize, the recent surge in interest in the LHb has opened up a new perspective for understanding the subcortical mechanisms of cognitive and emotive behaviors. Upcoming experimental results may be instrumental for determining how this structure might be targeted in the design of novel treatments for mental and neurological disorders.

Limbic systems for emotion and for memory, but no single limbic system.
Rolls ET1.
The concept of a (single) limbic system is shown to be outmoded. Instead, anatomical, neurophysiological, functional neuroimaging, and neuropsychological evidence is described that anterior limbic and related structures including the orbitofrontal cortex and amygdala are involved in emotion, reward valuation, and reward-related decision-making (but not memory), with the value representations transmitted to the anterior cingulate cortex for action-outcome learning. In this 'emotion limbic system' a computational principle is that feedforward pattern association networks learn associations from visual, olfactory and auditory stimuli, to primary reinforcers such as taste, touch, and pain. In primates including humans this learning can be very rapid and rule-based, with the orbitofrontal cortex overshadowing the amygdala in this learning important for social and emotional behaviour. Complementary evidence is described showing that the hippocampus and limbic structures to which it is connected including the posterior cingulate cortex and the fornix-mammillary body-anterior thalamus-posterior cingulate circuit are involved in episodic or event memory, but not emotion. This 'hippocampal system' receives information from neocortical areas about spatial location, and objects, and can rapidly associate this information together by the different computational principle of autoassociation in the CA3 region of the hippocampus involving feedback. The system can later recall the whole of this information in the CA3 region from any component, a feedback process, and can recall the information back to neocortical areas, again a feedback (to neocortex) recall process. Emotion can enter this memory system from the orbitofrontal cortex etc., and be recalled back to the orbitofrontal cortex etc. during memory recall, but the emotional and hippocampal networks or 'limbic systems' operate by different computational principles, and operate independently of each other except insofar as an emotional state or reward value attribute may be part of an episodic memory.The concept of the limbic system has a long history, and is a
concept that has endured to the present day (Catani,
Dell’acqua, & Thiebaut de Schotten, 2013; Mesulam, 2000).
In this paper I describe evidence that there are separate
systems in the brain for emotion and for memory, each
involving limbic structures, but that there is no single limbic
system.Wemight term the system for emotion the ‘emotional
limbic system’, and the system for memory the ‘memory
limbic system’, but there are non-limbic components to both
systems. The important concept I advance here is that the
systems for emotion and for episodic memory involve largely
different brain structures and connections, and different
computational principles of operation, which are described. I
argue here that of course some links from the emotional
system into the memory system are present, for often an
emotional state is part of an episodic memory, and when that
episodic memory is recalled, the emotional state must be
included in what is recalled. These concepts are important not
only within neuroscience, but also for neurology (Catani et al.,
2013; Mesulam, 2000), neuropsychology (Aggleton, 2012), and
psychiatry. The use of the term ‘limbic’ has changed over time, but the
concept of a limbic system is still in use (Catani et al., 2013).
The term ‘limbic’ was introduced by Thomas Willis (1664) to
designate a cortical border encircling the brainstem (limbus,
Latin for ‘border’). Paul Broca (1878) held the view that ‘le
grand lobe limbique’ was mainly an olfactory structure common
to all mammalian brains, although he argued that its
functions were not limited to olfaction. Limbic structures are
frequently taken to include cortical structures such as the
hippocampus and cingulate cortex, and structures to which
they are connected such as the mammillary bodies, septal
area, and amygdala (Isaacson, 1982). After Broca’s publication,
the accumulation of experimental evidence from ablation
studies in animals broadened the role of limbic structures to
include other aspects of behaviour such as controlling social
interactions and behaviour (Brown & Scha¨ fer, 1888), consolidating
memories (Bechterew, 1900), and forming emotions
(Cannon, 1927). Anatomical and physiological advances led
James Papez (1937) to describe a neural circuit for linking action
and perception to emotion. The Papez circuit consists of
the hippocampus connecting via the fornix to the mammillary
body, which connects via the mammillo-thalamic tract to the
anterior nuclei of the thalamus and thus back to the cingulate
cortex. According to Papez, emotion arises either from
cognition entering the circuit from the cortex through the
hippocampus, or from visceral and somatic perceptions
entering the circuit through the hypothalamus. Some of
Papez’ evidence on his circuit and emotion was that in rabies
where the disease appears to have a predilection for the hippocampus
and cerebellum, the patient is subject to anxiety,
apprehensiveness, and paroxysms of rage or terror. Papez held that ‘the cortex of the cingular gyrus may be looked on as
the receptive region for the experiencing of emotion as the
result of impulses coming from the hypothalamic region or
the hippocampal formation’ (Papez, 1937). A decade later, Paul
Yakovlev (1948) proposed that the orbitofrontal cortex, insula,
amygdala, and anterior temporal lobe form a network underlying
emotion and motivation. Paul MacLean crystallised
previous works by incorporating both Papez’ and Yakovlev’s
views into a model of the limbic system (MacLean, 1949, 1952).
MacLean concluded that the limbic cortex, together with the
limbic subcortical structures, is a functionally integrated
system involved especially in emotion. Robert Isaacson
assembled evidence on the functions of this system in
emotion and memory in a book entitled The limbic system
(Isaacson, 1982).
In the remainder of this paper I describe evidence that
there are separate systems in the brain for emotion and for
episodic memory, each involving limbic structures; introduce
a hypothesis about the nature of the links between these
systems; show that the computations in the two systems are
very different; and argue that there is no (single) limbic
system. A very useful working definition of emotions is that they are
states elicited by rewards and punishers, that is, by instrumental
reinforcers (Gray, 1975; Rolls, 2005, 2014; Weiskrantz,
1968). Instrumental reinforcers are rewards and punishers
that are obtained as a result of an action instrumental in
gaining the reward or avoiding the punisher. This approach is
supported by many considerations (Rolls, 2014), including the
following three. First, the definition is conceptually acceptable,
in that it is difficult to think of exceptions to the rule that
rewards and punishers are associated with emotional states,
and to the rule that emotional states are produced by rewards
and punishers (Rolls, 2014). Second, the definition is powerful
in an evolutionary and explanatory sense, in that the functions
of emotion can be conceived of as related to processes
involved in obtaining goals, and in states that are produced
when goals are received. Indeed, my evolutionary Darwinian
account states that the adaptive value of rewards and punishers
is that they are gene-specified goals for action, and that
it is much more effective for genes to specify rewards and
punishers, the goals for action, than to attempt to specify
actions (Rolls, 2014). Examples of such primary (i.e., unlearned
or gene-specified) reinforcers include the taste of food, pain,
stimuli that promote reproductive success, and face expression.
Other stimuli become secondary reinforcers by learned
associations with primary reinforcers in parts of the brain
involved in emotion such as the orbitofrontal cortex and
amygdala. An example is the sight of food, which by learned
association with a primary reinforcer, taste, becomes a secondary
reinforcer. Third, this approach provides a principled
way to analyse the brain mechanisms of emotion, by examination of where in the brain stimuli are represented by
their reward value (Rolls, 2014).
3.2. An anatomical and functional framework for
understanding the neural basis of emotion
I now provide a framework for understanding some of the
brain structures involved in emotion, and at the same time
contrast them with the structures that in terms of connectivity
and function precede them and succeed them in the
anatomical and functional hierarchy moving from left to right
in Fig. 1 (Rolls, 2014).
In Tier 1 (Fig. 1), information is processed to a level at which
the neurons represent ‘what’ the stimulus is, independently
of the reward or punishment value of the stimulus. Thus
neurons in the primary taste cortex represent what the taste
is, and its intensity, but not its reward value (Rolls, 2014). In
the inferior temporal visual cortex, the representation is of
objects, invariantly with respect to the exact position on the
retina, size, and even view. Forming invariant representations involves a great deal of cortical computation in the hierarchy
of visual cortical areas from the primary visual cortex V1 to
the inferior temporal visual cortex (Rolls, 2008c, 2012a). The
fundamental advantage of this separation of ‘what’ processing
in Tier 1 from reward value processing in Tier 2 is that any
learning in Tier 2 of the value of an object or face seen in one
location on the retina, size, and view will generalize to other
views etc. In rodents there is no such clear separation of
‘what’ from ‘value’ representations. For example in the taste
system, satiety influences taste processing at the first central
synapse in the taste system (Rolls & Scott, 2003), and this
property makes the processing in rodents not only different
from that in primates including humans, but also much more
difficult to analyse (Rolls, 2014).
There are brain mechanisms in Tier 2 in the orbitofrontal
cortex that are involved in computing the reward value of
primary (unlearned) reinforcers, as shown by devaluation
experiments in which for example a food is fed to satiety
(Critchley & Rolls, 1996a; Kringelbach, O’Doherty, Rolls, &
Andrews, 2003; Rolls & Grabenhorst, 2008; Rolls, Sienkiewicz, & Yaxley, 1989), and by neuroeconomics experiments which
show that the amount and quality of each commodity is
encoded by orbitofrontal cortex neurons (Grabenhorst & Rolls,
2011; Padoa-Schioppa, 2011; Padoa-Schioppa & Assad, 2008).
The primary reinforcers include taste, touch (both pleasant
touch and pain), and to some extent smell, and perhaps
certain visual stimuli such as face expression. There is evidence
that there is a representation of the (reward/punishment)
value of many primary reinforcers in the orbitofrontal
cortex, including taste, positive touch and pain, face expression,
face beauty, and auditory consonance/dissonance. In
neuroeconomics, these are termed ‘outcome value’ representations
(Rolls, 2014). Further evidence for value representations
is that orbitofrontal cortex activations in humans to
these stimuli are linearly related to the subjectively reported
pleasantness of stimuli (medially), or to their unpleasantness
(laterally) (Rolls, 2014).
Brain regions in Tier 2 are also concerned with learning
associations between previously neutral stimuli, such as the
sight of objects or of individuals’ faces, with primary reinforcers.
These brain regions include the amygdala and
orbitofrontal cortex, with the orbitofrontal cortex being
especially important in the rapid, one-trial, learning and
reversal of stimulus-reinforcer associations. In neuroeconomics,
these are termed ‘expected value’ representations.
Once the Tier 2 brain regions have determined whether
the input is reinforcing, whether primary or secondary, the
signal is passed directly to output regions of the brain, with no
need to produce and then feed back peripheral body or autonomic
responses to the brain.
In the orbitofrontal cortex in Tier 2, the representation is of
the value of stimuli, and actions are not represented. The
values of very many different types of stimuli, events or goals
are represented separately at the neuronal level, providing the
basis for choice between stimuli, and the selection at later
stages of processing of an appropriate action to obtain the
chosen goal.
Whereas the orbitofrontal cortex in Tier 2 represents the
value of stimuli (potential goals for action) on a continuous
scale, an area anterior to this, medial prefrontal cortex area 10
(in Tier 3), is implicated in decision-making between stimuli,
in which a selection or choice must be made, moving beyond a
representation of value on a continuous scale towards a decision
between goods based on their value (Grabenhorst, Rolls,
& Margot, 2011; Rolls, 2014; Rolls, Grabenhorst, & Parris, 2008).
The brain regions in which the reinforcing, and hence
emotional, value of stimuli are represented interface to three
main types of output system:
The first is the autonomic and endocrine system, for producing
such changes as increased heart rate and release of
adrenaline, which prepare the body for action. Structures
receiving from the orbitofrontal cortex, amygdala, and ACC
that provide a route for these autonomic effects include the
hypothalamus and parts of the anterior insula close to the
insular taste cortex (Critchley & Harrison, 2013; Rolls, 2014).
The second type of output is to brain systems
concerned with performing actions unconsciously or
implicitly, in order to obtain rewards or avoid punishers.
These brain systems include the basal ganglia for habit (‘stimuluseresponse’) behaviour, and the ACC for
actioneoutcome learning (The ‘outcome’ is the reward or
punisher that is or is not obtained when the action is performed.).
The ACC contains representations of reward and
punisher value, and thus of outcome, which are essential
for learning associations between actions and the outcomes
that follow actions. The midcingulate area contains
representations of actions.
The third type of output is to a system capable of planning
many steps ahead, and for example deferring short-term
rewards in order to execute a long-term plan. This system
may use syntactic processing to perform the planning,
and is therefore part of a linguistic system which performs
explicit (conscious) processing, as described more fully
elsewhere (Rolls, 2014).
It is notable that the orbitofrontal cortex and amygdala do
not receive inputs from the dorsal visual ‘where’ processing
areas such as the parietal cortex including the retrosplenial
cortex (which is part of the posterior cingulate cortex) that
provide inputs via parahippocampal areas TF/TH to the hippocampus
for its spatial (‘where’) functions in memory, which
are described in Section 4. In a complementary way, the hippocampus
and parahippocampal areas do not contain value
representations of stimuli, except insofar as value may be part
of a memory such as reward-place memory (Rolls, 2010b; Rolls
& Xiang, 2005, 2006). This is part of the evidence that the
emotional and episodic memory systems have different connections
and functions, as described in Section 4 and elsewhere
(Rolls, 2008c, 2010b; Rolls & Xiang, 2005, 2006), and thus
that there is no single and unified limbic system.
Because of the intended relevance to understanding
human emotion and its disorders, the focus of the research
described here is on humans and macaques. This is important,
for many of the brain systems that are involved in
emotion have undergone considerable development in primates
(e.g., monkeys and humans) (Rolls, 2014), as summarized
next.
First, the temporal lobe has undergone great development
in primates, and several systems in the temporal lobe are
either involved in emotion (e.g., the amygdala), or provide
some of the main sensory inputs to brain systems involved in
emotion and motivation. For example, the amygdala and the
orbitofrontal cortex, key brain structures in emotion, both
receive inputs from the highly developed primate temporal
lobe cortical areas, including those involved in invariant visual
object recognition, and face identity and expression
processing (Rolls, 2000a, 2008c, 2011a, 2012a).
Second, there are many topological, cytoarchitectural, and
probably connectional similarities between macaques and
humans with respect to the orbitofrontal cortex (see Fig. 1 and
Carmichael & Price, 1994; Kringelbach & Rolls, 2004; O¨ ngu¨r &
Price, 2000; Petrides & Pandya, 1995; Price, 2006, 2007).
Third, the prefrontal cortex has also undergone great
development in primates, and one part of it, the orbitofrontal
cortex, is very little developed in rodents, yet is one of the
major brain areas involved in emotion and motivation in
primates including humans. Indeed, it has been argued that
the granular prefrontal cortex is a primate innovation, and the
implication of the argument is that any areas that might be termed orbitofrontal cortex in rats (Schoenbaum, Roesch,
Stalnaker, & Takahashi, 2009) are homologous only to the
agranular parts of the primate orbitofrontal cortex (shaded
mid grey in Fig. 2), that is to areas 13a, 14c, and the agranular
insular areas labelled Ia in Fig. 2 (Passingham & Wise, 2012;
Wise, 2008). It follows from that argument that for most
areas of the orbitofrontal and medial prefrontal cortex in
humans and macaques (those shaded light grey in Fig. 2),
special consideration must be given to research in macaques
and humans. As shown in Fig. 2, there may be no cortical area
in rodents that is homologous to most of the primate
including human orbitofrontal cortex (Passingham & Wise,
2012; Preuss, 1995; Wise, 2008).
Fourth, even the taste system (which might have been
supposed to be phylogenetically old and preserved) of primates
and rodents may be different, with obligatory processing
from the nucleus of the solitary tract via the thalamus
to the cortex in primates, but a subcortical pathway in rodents
via a pontine taste area to the amygdala, and differences in
where satiety influences taste-responsive neurons in primates and rodents (Norgren, 1984; Rolls, 2014; Rolls & Scott,
2003; Small & Scott, 2009).
Fifth, with the great development of the orbitofrontal cortex
in primates, the amygdala may become relatively less
important in humans in emotion than in other vertebrates
(Rolls, 2014) (see Section 3.4).
To understand the functions of the orbitofrontal cortex and
connected areas in humans, the majority of the studies
described here were therefore performed with macaques or
with humans.
3.3. The orbitofrontal cortex
The first structure considered is the orbitofrontal cortex, and
although not a limbic structure such as the amygdala, has
similar connections to the amygdala, is connected to the
amygdala, and has considerably eclipsed in primates
including humans the functions of the amygdala (Rolls, 2014).
The amygdala has evolutionarily old origins and can be
identified, as can the hippocampus, in amphibia (Isaacson,
1982; Medina, Bupesh, & Abellan, 2011), whereas most of the
orbitofrontal cortex, the granular parts, is new to primates
(Passingham & Wise, 2012; Wise, 2008) (Fig. 2).
3.3.1. Anatomical and functional connectivity
Maps of the architectonic areas in the orbitofrontal cortex and
medial prefrontal cortex are shown in Fig. 2 for humans (left)
and monkeys (middle) (Carmichael & Price, 1994;O¨ ngu¨ r, Ferry,
& Price, 2003). The connections of the orbitofrontal cortex
(Barbas, 1995; Carmichael & Price, 1994, 1995b; O¨ ngu¨ r & Price,
2000; Pandya & Yeterian, 1996; Petrides & Pandya, 1995; Price,
2006, 2007) are summarized in Fig. 3. Conceptually, the orbitofrontal
cortex can be thought of as receiving from the ends
of each modality-specific ‘what’ cortical pathway as shown in
Fig. 1, and this functional connectivity is emphasized in the
following.
Rolls, Yaxley, and Sienkiewicz (1990) discovered a taste
area with taste-responsive neurons in the lateral part of the macaque orbitofrontal cortex, and showed anatomically with
horseradish peroxidase pathway tracing that this was the
secondary taste cortex in that it receives a major projection
from the neurophysiologically identified primary taste cortex
(Baylis, Rolls, & Baylis, 1995). This region projects on to more
anterior areas of the orbitofrontal cortex (Baylis et al., 1995).
Taste neurons are also found more medially (Critchley & Rolls,
1996c; Pritchard et al., 2005; Rolls, 2008b; Rolls & Baylis, 1994;
Rolls, Critchley, Wakeman, & Mason, 1996).
In the mid-orbitofrontal cortex, there is an area with olfactory
neurons (Rolls & Baylis, 1994), and anatomically there
are direct connections from the primary olfactory cortex,
pyriform cortex, to area 13a of the posterior orbitofrontal
cortex, which in turn has onward projections to a middle part
of the orbitofrontal cortex (area 13) (Barbas, 1993; Carmichael,
Clugnet, & Price, 1994; Morecraft, Geula, & Mesulam, 1992;
Price, 2007; Price et al., 1991) (see Fig. 1).
Thorpe, Rolls, and Maddison (1983) found neurons with
visual responses in the orbitofrontal cortex, and anatomically,
visual inputs reach the orbitofrontal cortex directly from the
inferior temporal cortex [where object and face identity are
represented (Rolls, 2007b, 2008c)], the cortex in the superior
temporal sulcus [where face expression and gesture are represented
(Hasselmo, Rolls, & Baylis, 1989)], and the temporal
pole cortex (see Barbas, 1988, 1993, 1995; Barbas & Pandya,
1989; Carmichael & Price, 1995b; Morecraft et al., 1992;
Seltzer & Pandya, 1989). There are corresponding auditory
inputs (Barbas, 1988, 1993; Rolls, Critchley, Browning, & Inoue,
2006; Romanski & Goldman-Rakic, 2001; Romanski et al.,
1999).
Some neurons in the orbitofrontal cortex respond to oral
somatosensory stimuli such as the texture of food (Rolls,
Critchley, Browning, Hernadi, & Lenard, 1999; Rolls,
Verhagen, & Kadohisa, 2003), and anatomically there are inputs
to the orbitofrontal cortex from somatosensory cortical
areas 1, 2 and SII in the frontal and pericentral operculum, and
from the insula (Barbas, 1988; Carmichael & Price, 1995b). The
caudal orbitofrontal cortex receives inputs from the amygdala
(Price et al., 1991). The orbitofrontal cortex also receives inputs
via the mediodorsal nucleus of the thalamus, pars magnocellularis,
which itself receives afferents from temporal lobe
structures such as the prepyriform (olfactory) cortex, amygdala,
and inferior temporal cortex (see O¨ ngu¨ r & Price, 2000).
These connections provide some routes via which the responses
of orbitofrontal cortex neurons can be produced.
Within the orbitofrontal cortex, there are many intrinsic
connections (O¨ ngu¨ r & Price, 2000), and these may be part of
what enables many orbitofrontal cortex neurons to have
multimodal responses, as described below and elsewhere
(Rolls, 2005, 2008b, 2008c, 2014; Rolls & Grabenhorst, 2008).
The orbitofrontal cortex projects back to temporal lobe
areas such as the amygdala via the uncinate fasciculus
(Barbas, 2007). The orbitofrontal cortex also has projections to
the ACC (Carmichael & Price, 1996; Price, 2006), the ventral
striatum (Ferry, Ongur, An, & Price, 2000) and head of the
caudate nucleus (Haber, Kim, Mailly, & Calzavara, 2006; Kemp
& Powell, 1970), medial prefrontal cortex area 10 (Price, 2007),
preoptic region and lateral hypothalamus [where neurons
respond to the sight and taste of food, and show sensoryspecific
satiety (Burton, Rolls, & Mora, 1976; Rolls, Burton, &
Mora, 1976)], and the ventral tegmental area (Johnson,
Rosvold, & Mishkin, 1968; Nauta, 1964; Price, 2006), and
these connections provide some routes via which the orbitofrontal
cortex can influence behaviour (Rolls, 2014). The orbitofrontal
cortex also has connections to the entorhinal and
perirhinal cortex (Barbas, 2007; Insausti, Amaral, & Cowan,
1987; Price, 2006) providing a route for reward information to
reach the hippocampus where it can become linked into
memories about for example where reward is located, though
not about which objects are associated with reward (Rolls,
2010b; Rolls & Xiang, 2005), which is an orbitofrontal/amygdala
function important in emotion. In turn, connections back
to the orbitofrontal cortex from the entorhinal cortex, and
even from CA1 and the subiculum (Price, 2006), provide a route
for the reward value and emotional state to be recalled to the
orbitofrontal cortex as part of the recall of an episodic
memory.
3.3.2. Effects of damage to the orbitofrontal cortex on emotion
and emotion-related learning
Part of the evidence on the functions of the orbitofrontal cortex
in emotion comes from the effects of lesions of the orbitofrontal
cortex. Macaques with lesions of the orbitofrontal
cortex are impaired at tasks that involve learning about which
stimuli are rewarding and which are not, and are especially
impaired at altering behaviour when reinforcement contingencies
change. The monkeys may respond when responses
are inappropriate, e.g., no longer rewarded, or may respond to
a non-rewarded stimulus. For example, monkeys with orbitofrontal
cortex damage are impaired on Go/NoGo task performance
in that they Go on the NoGo trials (Iversen & Mishkin,
1970); in an object reversal task in that they respond to the
object that was formerly rewarded with food; and in extinction
in that they continue to respond to an object which is no longer
rewarded (Butter, 1969; Izquierdo & Murray, 2004; Izquierdo,
Suda, & Murray, 2004; Jones & Mishkin, 1972; Murray &
Izquierdo, 2007; Rudebeck & Murray, 2011). Rapid associations
between visual stimuli and reinforcers such as taste, and
the rapid reversal of these associations, is an important function
of the orbitofrontal cortex as shown by neurons with onetrial
object-reward reversal learning (Rolls, 2014; Rolls,
Critchley, Mason, & Wakeman, 1996; Thorpe et al., 1983).
Consistent with this, in humans rapid reversal is impaired by
orbitofrontal cortex damage (Fellows & Farah, 2003; Hornak
et al., 2004; Rolls, Hornak, Wade, & McGrath, 1994).
Orbitofrontal cortex damage affects reward value as also
shown by devaluation investigations. Sensory-specific satiety
(a method of reward devaluation in which a food is fed to
satiety), which is implemented neuronally in the orbitofrontal
cortex (Rolls, Sienkiewicz, et al., 1989), is impaired by orbitofrontal
cortex lesions (but perhaps less by amygdala lesions)
(Murray & Izquierdo, 2007; Rudebeck & Murray, 2011). In
relation to neuroeconomics, the estimation of expected
reward value as influenced by reward size, and delay to
reward, or both, is impaired by orbitofrontal cortex lesions in
macaques (Simmons, Minamimoto, Murray, & Richmond,
2010).
It is suggested that difficulty in processing reinforcers, and
especially in rapid visual discrimination reversal learning,
underlies some of the impairments in emotion produced by damage to the orbitofrontal cortex (Rolls, 2014). In humans,
euphoria, irresponsibility, lack of affect, and impulsiveness
can follow frontal lobe damage (Damasio, 1994; Kolb &
Whishaw, 2003; Rolls, 1999a; Zald & Rauch, 2006), particularly
orbitofrontal cortex damage (Berlin, Rolls, & Iversen,
2005; Berlin, Rolls, & Kischka, 2004; Hornak et al., 2003;
Hornak, Rolls, & Wade, 1996; Rolls, 1999a, 2014; Rolls et al.,
1994). These emotional changes may be related at least in
part to a failure to rapidly update the reinforcement associations
of stimuli when the contingencies are changed as in a
visual discrimination reversal task (Berlin et al., 2004; Fellows,
2007, 2011; Fellows & Farah, 2003; Hornak et al., 2004; Rolls,
1999b, 2014; Rolls et al., 1994). Similar mechanisms may
contribute at least in part to the poor performance of humans
with ventromedial prefrontal cortex damage on the Iowa
Gambling Task (Bechara, Damasio, & Damasio, 2000; Maia &
McClelland, 2004). It is of interest that the patients with
bilateral orbitofrontal cortex damage who were impaired at
the visual discrimination reversal task had high scores on
parts of a Social Behaviour Questionnaire in which the patients
were rated on behaviours such as emotion recognition
in others (e.g., their sad, angry, or disgusted mood); in interpersonal
relationships (such as not caring what others think,
and not being close to the family); emotional empathy (e.g.,
when others are happy, is not happy for them); interpersonal
relationships (e.g., does not care what others think, and is not
close to his family); public behaviour (is uncooperative);
antisocial behaviour (is critical of and impatient with others);
impulsivity (does things without thinking); and sociability (is
not sociable, and has difficulty making or maintaining close
relationships) (Hornak et al., 2003, 2004), all of which could
reflect less behavioural sensitivity to different types of punishment
and reward. Further, in a Subjective Emotional
Change Questionnaire in which the patients reported on any
changes in the intensity and/or frequency of their own experience
of emotions, the bilateral orbitofrontal cortex lesion
patients with deficits in the visual discrimination reversal task
reported a number of changes, including changes in sadness,
anger, fear and happiness (Hornak et al., 2003).
3.3.3. Reward outcome value for taste, olfaction, flavour, oral
texture, and oral temperature in the orbitofrontal cortex
3.3.3.1. TASTE AND ORAL TEXTURE. One of the discoveries that
have helped us to understand the functions of the orbitofrontal
cortex in behaviour is that it contains a major cortical representation
of taste (see Kadohisa, Rolls, & Verhagen, 2005a;
Rolls, 1995a, 1997, 2014; Rolls & Scott, 2003; Rolls et al., 1990)
(cf. Fig. 1). Given that taste can act as a primary reinforcer, that
is without learning as a reward or punisher, we now have the
start for a fundamental understanding of the function of the
orbitofrontal cortex in stimulus-reinforcer association
learning (Rolls, 1999a, 2004, 2008c, 2014). We know how one
class of primary reinforcers reaches and is represented in the
orbitofrontal cortex. A representation of primary reinforcers is
essential for a system that is involved in learning associations
between previously neutral stimuli and primary reinforcers,
e.g., between the sight of an object and its taste.
The representation in the orbitofrontal cortex (shown by
analysing the responses of single neurons in macaques) is for
the majority of neurons the reward value of taste (Baylis &
Rolls, 1991; Kadohisa et al., 2005a; Rolls, 1995a, 1997, 2000c;
Rolls, Critchley, Browning, & Hernadi, 1998; Rolls, Critchley,
Wakeman, et al., 1996; Rolls & Scott, 2003; Rolls et al., 1990)
and oral texture including viscosity (Rolls, Verhagen, et al.,
2003), fat texture (Rolls et al., 1999; Verhagen, Rolls, &
Kadohisa, 2003), and astringency as exemplified by tannic
acid (Critchley & Rolls, 1996c). The evidence for this is that the
responses of orbitofrontal cortex taste neurons are modulated
by hunger (as is the reward value or palatability of a taste). In
particular, it has been shown that orbitofrontal cortex taste
neurons gradually stop responding to the taste of a food as the
monkey is fed to satiety, but not to the taste of other foods,
revealing a mechanism for sensory-specific satiety and
reward devaluation (Rolls, Critchley, Wakeman, et al., 1996;
Rolls, Sienkiewicz, et al., 1989). In contrast, the representation
of taste in the primary taste cortex (Scott, Yaxley,
Sienkiewicz, & Rolls, 1986; Yaxley, Rolls, & Sienkiewicz,
1990) is not modulated by hunger (Rolls, Scott, Sienkiewicz,
& Yaxley, 1988; Yaxley, Rolls, & Sienkiewicz, 1988). Thus in
the primate including human primary taste cortex, the reward
value of taste is not represented, and instead the identity and
intensity of the taste are represented (Grabenhorst & Rolls,
2008; Grabenhorst, Rolls, & Bilderbeck, 2008; Rolls, 2008c,
2014).
Additional evidence that the reward value of food is represented
in the orbitofrontal cortex is that monkeys work for
electrical stimulation of this brain region if they are hungry,
but not if they are satiated (Mora, Avrith, Phillips, & Rolls,
1979; Rolls, 2005). Further, neurons in the orbitofrontal cortex
are activated from many brain-stimulation reward sites
(Mora, Avrith, & Rolls, 1980; Rolls, Burton, & Mora, 1980). Thus
there is clear evidence that it is the reward value of taste that
is represented in the orbitofrontal cortex (see further Rolls,
1999a, 2000d, 2014), and this is further supported by the
finding that feeding to satiety decreases the activation of the
human orbitofrontal cortex to the food eaten to satiety in a
sensory-specific way (Kringelbach et al., 2003). Some orbitofrontal
cortex neurons respond to the ‘taste’ of water in the
mouth (Rolls et al., 1990), and their responses occur only when
thirsty and not when satiated (Rolls, Sienkiewicz, et al., 1989);
and correspondingly in humans the subjective pleasantness
or affective value of the taste of water in the mouth is represented
in the orbitofrontal cortex (de Araujo, Kringelbach,
Rolls, & McGlone, 2003). This is part of the evidence for the
separation of a ‘what’ tier of processing, which in this case is
the primary taste cortex, from a reward and affect-related
representation in the orbitofrontal cortex tier of processing,
as shown in Fig. 1.
Functional neuroimaging studies in humans have shown
that the most medial part of the human orbitofrontal cortex is
activated by taste, oral texture, and olfactory stimuli (de
Araujo, Kringelbach, Rolls, & Hobden, 2003; de Araujo &
Rolls, 2004; de Araujo, Rolls, Kringelbach, McGlone, &
Phillips, 2003; de Araujo, Rolls, Velazco, Margot, & Cayeux,
2005; Francis et al., 1999; Gottfried, Small, & Zald, 2006;
McCabe & Rolls, 2007; O’Doherty et al., 2000; Rolls,
Kringelbach, & de Araujo, 2003; Rolls & McCabe, 2007; Small,
Gerber, Mak, & Hummel, 2005; Small, Zatorre, Dagher,
Evans, & Jones-Gotman, 2001), and that the activations
correlate with ratings of subjective pleasantness and so are in the domain of affective representations (Kringelbach & Rolls,
2004; Rolls, 2014). This most medial part of the human orbitofrontal
cortex may have moved medially when compared
with the representation in macaques, probably because of the
extensive development of the dorsolateral prefrontal cortex in
humans (Rolls, 2008b; Rolls & Grabenhorst, 2008). Affectively
pleasant stimuli are often represented medially, and unpleasant
or aversive stimuli laterally, in the human orbitofrontal
cortex. Evidence consistent with this has been found
for taste (de Araujo, Kringelbach, Rolls, & Hobden, 2003;
O’Doherty, Rolls, Francis, Bowtell, & McGlone, 2001),
pleasant touch (Francis et al., 1999; Rolls, O’Doherty, et al.,
2003), and pleasant versus aversive olfactory stimuli (Francis
et al., 1999; O’Doherty et al., 2000; Rolls, 2000d; Rolls,
Kringelbach, et al., 2003) (see further Grabenhorst & Rolls,
2011; Kringelbach & Rolls, 2004). An important point for
those seeking to understand the hedonic topology of the
human orbitofrontal cortex is that it should not be assumed to
be the same as that in macaques.
3.3.3.2. AN OLFACTORY REWARD REPRESENTATION IN THE ORBITOFRONTAL
CORTEX. For 35% of orbitofrontal cortex olfactory neurons, the
odours to which a neuron responded were influenced by the
taste value (glucose or saline) with which the odour was
associated (Critchley & Rolls, 1996b). Thus the odour representation
for 35% of orbitofrontal neurons appeared to be built
by olfactory-to-taste association learning. This possibility was
confirmed by reversing the taste with which an odour was
associated in the reversal of an olfactory discrimination task.
It was found that 68% of the sample of neurons analysed
altered the way in which they responded to odour when the
taste reinforcement association of the odour was reversed
(Rolls, Critchley, Mason, et al., 1996). The olfactory-to-taste
reversal was quite slow, both neurophysiologically and
behaviourally, often requiring 20e80 trials, consistent with
the need for some stability of flavour representations formed
by a combination of odour and taste inputs.
To analyse the nature of the olfactory representation in the
orbitofrontal cortex, Critchley and Rolls (1996a) measured the
responses of olfactory neurons that responded to food while
they fed the monkey to satiety. They found that the majority
of orbitofrontal olfactory neurons decreased their responses
to the odour of the food with which the monkey was fed to
satiety. Thus for these neurons, the reward value of the odour
is what is represented in the orbitofrontal cortex (cf. Rolls &
Rolls, 1997), and this parallels the changes in the relative
pleasantness of different foods after a food is eaten to satiety
(Rolls, 1997; Rolls, Rolls, Rowe, & Sweeney, 1981a, 1981b; see
Rolls, 1999a, 2000d, 2014). The subjective pleasantness or
reward or affective value of odour is represented in the orbitofrontal
cortex, in that feeding humans to satiety decreases
the activation found to the odour of that food, and this effect is
relatively specific to the food eaten in the meal (O’Doherty et
al., 2000; Francis et al., 1999; cf. Morris & Dolan, 2001).
Further, the human medial orbitofrontal cortex has activation
that is related to the subjective pleasantness of a set of odours,
and a more lateral area has activation that is related to the
degree of subjective unpleasantness of odours (Rolls,
Kringelbach, et al., 2003). A functional magnetic resonance
imaging (fMRI) investigation in humans showed that whereas
in the orbitofrontal cortex the pleasantness versus unpleasantness
of odours is represented, this was not the case in
primary olfactory cortical areas, where instead the activations
reflected the intensity of the odours (Rolls, Kringelbach, et al.,
2003), providing a further example of the hierarchy of ‘what’
followed by reward processing shown in Fig. 1.
3.3.3.3. CONVERGENCE OF TASTE AND OLFACTORY INPUTS IN THE ORBITOFRONTAL
CORTEX: THE REPRESENTATION OF FLAVOUR. In the orbitofrontal
cortex, not only unimodal taste neurons, but also
unimodal olfactory neurons are found. In addition some single
neurons respond to both gustatory and olfactory stimuli,
often with correspondence between the two modalities (Rolls
& Baylis, 1994). It is probably here in the orbitofrontal cortex of
primates including humans that these two modalities
converge to produce the representation of flavour (de Araujo,
Rolls, et al., 2003; Rolls & Baylis, 1994), for neurons in the primary
taste cortex in the insular/frontal opercular cortex do
not respond to olfactory (or visual) stimuli (Verhagen,
Kadohisa, & Rolls, 2004).
The importance of the combination of taste and smell for
producing affectively pleasant and rewarding representations
of sensory stimuli is exemplified by findings with umami, the
delicious taste or flavour that is associated with combinations
of components that include meat, fish, milk, tomatoes, and
mushrooms, all ofwhich are rich in umami-related substances
such as glutamate or inosine 50 monophosphate. Umami taste
is produced by glutamate acting on a fifth taste system
(Chaudhari, Landin, & Roper, 2000; Maruyama, Pereira,
Margolskee, Chaudhari, & Roper, 2006; Zhao et al., 2003).
However, glutamate presented alone as a taste stimulus is not
highly pleasant, and does not act synergistically with other
tastes (sweet, salt, bitter and sour). However, when glutamate
is given in combination with a consonant, savoury, odour
(vegetable), the resulting flavour can be much more pleasant
(McCabe & Rolls, 2007). We showed using functional brain imaging
with fMRI that this glutamate taste and savoury odour
combination produced much greater activation of the medial
orbitofrontal cortex and pregenual cingulate cortex than the
sum of the activations by the taste and olfactory components
presented separately (McCabe & Rolls, 2007). Supra-linear effectsweremuch
less (and significantly less) evident for sodium
chloride and vegetable odour. Further, activations in these
brain regions were correlated with the subjective pleasantness
and fullness of the flavour, and with the consonance of the
taste and olfactory components. Supra-linear effects of glutamate
taste and savoury odour were not found in the insular
primary taste cortex. We thus proposed that glutamate acts by
the non-linear effects it can produce when combined with a
consonant odour in multimodal cortical taste-olfactory
convergence regions. We suggested that umami can be
thought of as a rich and delicious flavour that is produced by a
combination of glutamate taste and a consonant savoury odour
(Rolls, 2009c). Glutamate is thus a flavour enhancer because of
the way that it can combine supra-linearly with consonant
odours in cortical areaswhere the taste and olfactory pathways
converge far beyond the receptors (McCabe & Rolls, 2007).
3.3.3.4. ORAL TEXTURE AND TEMPERATURE. A population of orbitofrontal
cortex neurons responds when a fatty food such as the domain of affective representations (Kringelbach & Rolls,
2004; Rolls, 2014). This most medial part of the human orbitofrontal
cortex may have moved medially when compared
with the representation in macaques, probably because of the
extensive development of the dorsolateral prefrontal cortex in
humans (Rolls, 2008b; Rolls & Grabenhorst, 2008). Affectively
pleasant stimuli are often represented medially, and unpleasant
or aversive stimuli laterally, in the human orbitofrontal
cortex. Evidence consistent with this has been found
for taste (de Araujo, Kringelbach, Rolls, & Hobden, 2003;
O’Doherty, Rolls, Francis, Bowtell, & McGlone, 2001),
pleasant touch (Francis et al., 1999; Rolls, O’Doherty, et al.,
2003), and pleasant versus aversive olfactory stimuli (Francis
et al., 1999; O’Doherty et al., 2000; Rolls, 2000d; Rolls,
Kringelbach, et al., 2003) (see further Grabenhorst & Rolls,
2011; Kringelbach & Rolls, 2004). An important point for
those seeking to understand the hedonic topology of the
human orbitofrontal cortex is that it should not be assumed to
be the same as that in macaques.
3.3.3.2. AN OLFACTORY REWARD REPRESENTATION IN THE ORBITOFRONTAL
CORTEX. For 35% of orbitofrontal cortex olfactory neurons, the
odours to which a neuron responded were influenced by the
taste value (glucose or saline) with which the odour was
associated (Critchley & Rolls, 1996b). Thus the odour representation
for 35% of orbitofrontal neurons appeared to be built
by olfactory-to-taste association learning. This possibility was
confirmed by reversing the taste with which an odour was
associated in the reversal of an olfactory discrimination task.
It was found that 68% of the sample of neurons analysed
altered the way in which they responded to odour when the
taste reinforcement association of the odour was reversed
(Rolls, Critchley, Mason, et al., 1996). The olfactory-to-taste
reversal was quite slow, both neurophysiologically and
behaviourally, often requiring 20e80 trials, consistent with
the need for some stability of flavour representations formed
by a combination of odour and taste inputs.
To analyse the nature of the olfactory representation in the
orbitofrontal cortex, Critchley and Rolls (1996a) measured the
responses of olfactory neurons that responded to food while
they fed the monkey to satiety. They found that the majority
of orbitofrontal olfactory neurons decreased their responses
to the odour of the food with which the monkey was fed to
satiety. Thus for these neurons, the reward value of the odour
is what is represented in the orbitofrontal cortex (cf. Rolls &
Rolls, 1997), and this parallels the changes in the relative
pleasantness of different foods after a food is eaten to satiety
(Rolls, 1997; Rolls, Rolls, Rowe, & Sweeney, 1981a, 1981b; see
Rolls, 1999a, 2000d, 2014). The subjective pleasantness or
reward or affective value of odour is represented in the orbitofrontal
cortex, in that feeding humans to satiety decreases
the activation found to the odour of that food, and this effect is
relatively specific to the food eaten in the meal (O’Doherty et
al., 2000; Francis et al., 1999; cf. Morris & Dolan, 2001).
Further, the human medial orbitofrontal cortex has activation
that is related to the subjective pleasantness of a set of odours,
and a more lateral area has activation that is related to the
degree of subjective unpleasantness of odours (Rolls,
Kringelbach, et al., 2003). A functional magnetic resonance
imaging (fMRI) investigation in humans showed that whereas
in the orbitofrontal cortex the pleasantness versus unpleasantness
of odours is represented, this was not the case in
primary olfactory cortical areas, where instead the activations
reflected the intensity of the odours (Rolls, Kringelbach, et al.,
2003), providing a further example of the hierarchy of ‘what’
followed by reward processing shown in Fig. 1.
3.3.3.3. CONVERGENCE OF TASTE AND OLFACTORY INPUTS IN THE ORBITOFRONTAL
CORTEX: THE REPRESENTATION OF FLAVOUR. In the orbitofrontal
cortex, not only unimodal taste neurons, but also
unimodal olfactory neurons are found. In addition some single
neurons respond to both gustatory and olfactory stimuli,
often with correspondence between the two modalities (Rolls
& Baylis, 1994). It is probably here in the orbitofrontal cortex of
primates including humans that these two modalities
converge to produce the representation of flavour (de Araujo,
Rolls, et al., 2003; Rolls & Baylis, 1994), for neurons in the primary
taste cortex in the insular/frontal opercular cortex do
not respond to olfactory (or visual) stimuli (Verhagen,
Kadohisa, & Rolls, 2004).
The importance of the combination of taste and smell for
producing affectively pleasant and rewarding representations
of sensory stimuli is exemplified by findings with umami, the
delicious taste or flavour that is associated with combinations
of components that include meat, fish, milk, tomatoes, and
mushrooms, all ofwhich are rich in umami-related substances
such as glutamate or inosine 50 monophosphate. Umami taste
is produced by glutamate acting on a fifth taste system
(Chaudhari, Landin, & Roper, 2000; Maruyama, Pereira,
Margolskee, Chaudhari, & Roper, 2006; Zhao et al., 2003).
However, glutamate presented alone as a taste stimulus is not
highly pleasant, and does not act synergistically with other
tastes (sweet, salt, bitter and sour). However, when glutamate
is given in combination with a consonant, savoury, odour
(vegetable), the resulting flavour can be much more pleasant
(McCabe & Rolls, 2007). We showed using functional brain imaging
with fMRI that this glutamate taste and savoury odour
combination produced much greater activation of the medial
orbitofrontal cortex and pregenual cingulate cortex than the
sum of the activations by the taste and olfactory components
presented separately (McCabe & Rolls, 2007). Supra-linear effectsweremuch
less (and significantly less) evident for sodium
chloride and vegetable odour. Further, activations in these
brain regions were correlated with the subjective pleasantness
and fullness of the flavour, and with the consonance of the
taste and olfactory components. Supra-linear effects of glutamate
taste and savoury odour were not found in the insular
primary taste cortex. We thus proposed that glutamate acts by
the non-linear effects it can produce when combined with a
consonant odour in multimodal cortical taste-olfactory
convergence regions. We suggested that umami can be
thought of as a rich and delicious flavour that is produced by a
combination of glutamate taste and a consonant savoury odour
(Rolls, 2009c). Glutamate is thus a flavour enhancer because of
the way that it can combine supra-linearly with consonant
odours in cortical areaswhere the taste and olfactory pathways
converge far beyond the receptors (McCabe & Rolls, 2007).
3.3.3.4. ORAL TEXTURE AND TEMPERATURE. A population of orbitofrontal
cortex neurons responds when a fatty food such as cream is in the mouth. These neurons can also be activated by
pure fat such as glyceryl trioleate, and by non-fat substances
with a fat-like texture such as paraffin oil (hydrocarbon) and
silicone oil [(Si(CH3)2O)n]. These neurons thus provide information
by somatosensory pathways that a fatty food is in the
mouth (Rolls et al., 1999). These inputs are perceived as
pleasant when hungry, because of the utility of ingestion of
foods that are likely to contain essential fatty acids and to
have a high calorific value (Rolls, 2000d, 2014). Satiety produced
by eating a fatty food, cream, can decrease the responses
of orbitofrontal cortex neurons to the texture of fat in
the mouth (Rolls et al., 1999).
We have shown that the orbitofrontal cortex receives inputs
from a number of different oral texture channels, which
together provide a rich sensory representation of what is in
the mouth (Rolls, 2011b, 2012b). Using a set of stimuli in which
viscosity was systematically altered (carboxymethylcellulose
with viscosity in the range 10e10,000 cP), we have shown that
some orbitofrontal cortex neurons encode fat texture independently
of viscosity (by a physical parameter that varies
with the slickness of fat) (Verhagen et al., 2003); that other
orbitofrontal cortex neurons encode the viscosity of the
texture in the mouth (with some neurons tuned to viscosity,
and others showing increasing or decrease firing rates as
viscosity increases) (Rolls, Verhagen, et al., 2003); and that
other neurons have responses that indicate the presence of
texture stimuli (such as grittiness and capsaicin) in the mouth
independently of viscosity and slickness (Rolls, Verhagen,
et al., 2003). Ensemble (i.e., population, distributed) encoding
of all these variables is found (Rolls, Critchley, Verhagen, &
Kadohisa, 2010; Rolls & Treves, 2011). In a complementary
human functional neuroimaging study, it has been shown
that activations of parts of the orbitofrontal cortex, primary
taste cortex, and mid-insular somatosensory region posterior
to the insular taste cortex have activations that are related to
the viscosity of what is in the mouth, and that there is in
addition a medial prefrontal/cingulate area where the mouth
feel of fat is represented (de Araujo & Rolls, 2004). Moreover,
the subjective pleasantness of fat is represented in the orbitofrontal
cortex and a region to which it projects the pregenual
cingulate cortex (Grabenhorst, Rolls, Parris, & D’Souza, 2010).
An overlapping population of orbitofrontal cortex neurons
represents the temperature of what is in the mouth (Kadohisa,
Rolls, & Verhagen, 2004), and this is supported by a human
fMRI study (Guest et al., 2007).
3.3.4. Outcome value and somatosensory and temperature
inputs to the orbitofrontal cortex
In addition to these oral somatosensory inputs to the orbitofrontal
cortex, there are also somatosensory inputs from other
parts of the body (Rolls, 2010a), and indeed an fMRI investigation
we have performed in humans indicates that pleasant
and painful touch stimuli to the hand produce greater activation
of the orbitofrontal cortex relative to the somatosensory
cortex than do affectively neutral stimuli (Francis et al.,
1999; Rolls, O’Doherty, et al., 2003). In an fMRI investigation
in humans, it was found that the mid-orbitofrontal and pregenual
cingulate cortex and a region to which they project, the
ventral striatum, have activations that are correlated with the
subjective pleasantness ratingsmade to warm (41 C) and cold
(12 C) stimuli, and combinations of warm and cold stimuli,
applied to the hand (Rolls, Grabenhorst, & Parris, 2008). Activations
in the lateral and some more anterior parts of the
orbitofrontal cortex were correlated with the unpleasantness
of the stimuli. In contrast, activations in the somatosensory
cortex and ventral posterior insula were correlated with the
intensity but not the pleasantness of the thermal stimuli.
Further, cognitive modulators of affective value such as the
description of cream being rubbed on the arm as ‘rich and
moisturizing’ increase activations to the sight of rubbing of
the arm in the orbitofrontal and pregenual cingulate cortex,
and increased correlations there with the subjectively rated
pleasantness of the touch (McCabe, Rolls, Bilderbeck, &
McGlone, 2008).
A principle thus appears to be that processing related to the
affective value and associated subjective emotional experience
of thermal stimuli that are important for survival is
performed in different brain areas to those where activations
are related to sensory properties of the stimuli such as their
intensity. This conclusion appears to be the case for processing
in a number of sensory modalities, including taste
(Grabenhorst & Rolls, 2008; Grabenhorst, Rolls, & Bilderbeck,
2008) and olfaction (Anderson et al., 2003; Grabenhorst,
Rolls, Margot, da Silva, & Velazco, 2007; Rolls, Kringelbach,
et al., 2003), and the finding with such prototypical stimuli
as warm and cold (Rolls, Grabenhorst, & Parris, 2008) provides
strong support for this principle (see Fig. 1).
Non-glabrous skin such as that on the forearm contains C
fibre tactile afferents that respond to light moving touch
(Olausson et al., 2002). The orbitofrontal cortex is implicated in
some of the affectively pleasant aspects of touch that may be
mediated through C fibre tactile afferents, in that it is activated
more by light touch to the forearm than by light touch to
the glabrous skin (palm) of the hand (McCabe et al., 2008).
3.3.5. Expected value visual inputs to the orbitofrontal cortex,
visual stimulus-reinforcement association learning and
reversal, and negative reward prediction error neurons
We have been able to show that there is a major visual input to
many neurons in the orbitofrontal cortex, and that what is
represented by these neurons is in many cases the reinforcement
association of visual stimuli. The visual input is
from the ventral, temporal lobe, visual stream concerned with
‘what’ object is being seen (see Rolls, 2000a, 2012a). Many
neurons in these temporal cortex visual areas have responses
to objects or faces that are invariant with respect to size, position
on the retina, and even view (Rolls, 2000a, 2007b, 2008a,
2008c, 2009d, 2012a), making these neurons ideal as an input
to a system that may learn about the reinforcement association
properties of objects and faces, for after a single learning
trial, the learning then generalizes correctly to other views etc.
(see Rolls, 2000a, 2008c, 2012a, 2014). Using this object-related
information, orbitofrontal cortex visual neurons frequently
respond differentially to objects or images depending on their
reward association (Rolls, Critchley, Mason, et al., 1996;
Thorpe et al., 1983). The primary reinforcer that has been
used is taste, and correlates of visual to taste association
learning have been demonstrated in the human orbitofrontal
cortex with fMRI (O’Doherty, Deichmann, Critchley, & Dolan,
2002). Many of these neurons show visual-taste reversal in one or a very few trials [In a visual discrimination task, they
will reverse the stimulus to which they respond, from e.g., a
triangle to a square, in one trial when the taste delivered for a
behavioural response to that stimulus is reversed (Thorpe
et al., 1983).]. This reversal learning probably occurs in the
orbitofrontal cortex, for it does not occur one synapse earlier
in the visual inferior temporal cortex (Rolls, Judge, &
Sanghera, 1977), and it is in the orbitofrontal cortex that
there is convergence of visual and taste pathways onto the
same single neurons (Rolls & Baylis, 1994; Rolls, Critchley,
Mason et al., 1996; Thorpe et al., 1983).
The probable mechanism for this learning is an associative
modification of synapses conveying visual input onto tasteresponsive
neurons, implementing a pattern association
network (Rolls, 2008c, 2014; Rolls & Deco, 2002; Rolls & Treves,
1998) (see Fig. 8), with the reversal facilitated by a rule for
which stimulus is currently rewarded held in short-term
memory (Deco & Rolls, 2005c).
The visual and olfactory neurons in primates that respond
to the sight or smell of stimuli that are primary reinforcers
such as taste clearly signal an expectation of reward that is
based on previous stimulus-reinforcement associations (Rolls,
Critchley, Mason, et al., 1996; Thorpe et al., 1983). So do the
conditional reward neurons which reflect the reward value
only for one of a pair of stimuli (Rolls, Critchley, Mason, et al.,
1996; Rolls & Grabenhorst, 2008; Thorpe et al., 1983). With
visual-taste association learning and reversal in primates, in
which the orbitofrontal cortex neurons and the behaviour can
change in one trial (Rolls, Critchley, Mason, et al., 1996; Thorpe
et al., 1983), the changing responses of the orbitofrontal cortex
neurons can contribute to the reversed behaviour, a view of
course supported by the impaired reversal learning produced
in primates including humans by orbitofrontal cortex damage
(e.g., Berlin et al., 2004; Fellows & Farah, 2003; Hornak et al.,
2004; Murray & Izquierdo, 2007; Rolls et al., 1994).
To analyse the nature of the visual representation of foodrelated
stimuli in the orbitofrontal cortex, Critchley and Rolls
(1996a) measured the responses of neurons that responded to
the sight of food while they fed the monkey to satiety in a
devaluation investigation. They found that themajority of orbitofrontal
visual food-related neurons decreased their responses
to the sight of the foodwithwhich themonkeywas fed to satiety.
Thus for these neurons, the expected reward value of the sight of
food is what is represented in the orbitofrontal cortex.
In addition to these neurons that encode the reward association
of visual stimuli, other, ‘error’, neurons in the orbitofrontal
cortex detect non-reward, in that they respond for
example when an expected reward is not obtained when a
visual discrimination task is reversed (Thorpe et al., 1983), or
when reward is no longer made available in a visual
discrimination task. These may be called “negative reward
prediction error neurons” (Rolls, 2014; Rolls & Grabenhorst,
2008). Evidence that there may be similar error neurons in
the human orbitofrontal cortex is that in a model of social
learning, orbitofrontal cortex activation occurred in a visual
discrimination reversal task at the time when the face of one
person no longer was associated with a smile, but became
associated with an angry expression, indicating on such error
trials that reversal of choice to the other individual’s face
should occur (Kringelbach & Rolls, 2003).
The orbitofrontal cortex negative reward prediction error
neurons respond to a mismatch between the reward expected
and the reward that is obtained. Both signals are represented
in the orbitofrontal cortex, in the form of for example neurons
that respond to the sight of a learned reinforcer such as the
sight of a stimulus paired with taste, and neurons that
respond to the primary reinforcer, the taste (or texture or
temperature). The orbitofrontal cortex is the probable brain
region for this computation, because both the signals required
to compute negative reward prediction error are present in the
orbitofrontal cortex, so are the negative reward prediction
error neurons, and lesions of the orbitofrontal cortex impair
tasks such as visual discrimination reversal in which this type
of negative reward prediction error is needed [see above and
Rolls (2014)].
3.3.6. Orbitofrontal cortex neurons compared to dopamine
neurons
The dopamine neurons in the midbrain that respond to positive
reward prediction error (a greater reward than expected)
may not be able to provide a good representation of negative
reward prediction error, because their spontaneous firing
rates are so low (Schultz, 2004) that much further reduction
would provide only a small signal. In any case, the dopamine
neurons would not appear to be in a position to compute a
negative reward prediction error, as they are not known to
receive inputs that signal expected reward, and the actual
reward (outcome) that is obtained, and indeed do not represent
the reward obtained (or ‘outcome’), in that they stop
responding to a taste reward outcome if it is predictable.
Although some dopamine neurons do appear to represent a
positive reward prediction error signal (responding if a greater
than expected reward is obtained) (Schultz, 2004, 2006, 2013),
they do not appear to have the signals required to compute
this, that is, the expected reward, and the reward outcome
obtained, so even a positive reward prediction error must be
computed elsewhere. The orbitofrontal cortex does contain
representations of these two signals, the expected reward and
the reward outcome, and has projections to the ventral
striatum, which in turn projects to the region of the midbrain
dopamine neurons, and so this is one possible pathway along
which the firing of positive reward prediction error might be
computed (see Fig. 1) (Rolls, 2014). Consistent with this, activations
in parts of the human ventral striatum are related to
positive reward prediction error (Hare, O’Doherty, Camerer,
Schultz, & Rangel, 2008; Rolls, McCabe, & Redoute, 2008c).
Thus the dopamine projections to the prefrontal cortex and
other areas are not likely to convey information about reward
to the prefrontal cortex, which instead is likely to be decoded
by the neurons in the orbitofrontal cortex that represent primary
reinforcers, and the orbitofrontal cortex neurons that
learn associations of other stimuli to the primary reinforcers
to represent expected value (Rolls, 2008c; Rolls, Critchley,
Mason, et al., 1996; Rolls, McCabe, et al., 2008; Thorpe et al.,
1983). Although it has been suggested that the firing of dopamine
neurons may reflect the earliest signal in a task that
indicates reward and could be used as a reward prediction
error signal during learning (see Schultz, 2006; Schultz,
Tremblay, & Hollerman, 2000), it is likely, partly on the basis
of the above evidence, though an interesting topic for future investigation, that any error information to which dopamine
neurons fire originates from representations in the orbitofrontal
cortex that encode expected value and reward
outcome, and which connect to the ventral striatum (Rolls,
2008c, 2009b, 2014). A further problem is that some dopamine
neurons respond to aversive or salient stimuli
(Bromberg-Martin, Matsumoto, & Hikosaka, 2010; Matsumoto
& Hikosaka, 2009), and overall the population may not code a
reward prediction error (Rolls, 2014).
3.3.7. Face-selective processing in the orbitofrontal cortex
Another type of visual information represented in the orbitofrontal
cortex is information about faces. There is a population
of orbitofrontal cortex neurons that respond in many
ways similarly to those in the temporal cortical visual areas
(Rolls, 1984, 1992a, 1996a, 2000a, 2007b, 2008a, 2008c, 2011a,
2012a; Rolls & Deco, 2002). The orbitofrontal cortex faceresponsive
neurons, first observed by Thorpe et al. (1983),
then by Rolls, Critchley, et al. (2006), tend to respond with
longer latencies than temporal lobe neurons (140e200 msec
typically, compared to 80e100 msec); also convey information
about which face is being seen, by having different responses
to different faces; and are typically rather harder to activate
strongly than temporal cortical face-selective neurons, in that
many of them respond much better to real faces than to twodimensional
images of faces on a video monitor (Rolls, 2011a;
Rolls, Critchley, et al., 2006) (cf. Rolls & Baylis, 1986). Some of
the orbitofrontal cortex face-selective neurons are responsive
to face expression, gesture or movement (Rolls, Critchley,
et al., 2006). The findings are consistent with the likelihood
that these neurons are activated via the inputs from the
temporal cortical visual areas in which face-selective neurons
are found (see Fig. 1). The significance of the neurons is likely
to be related to the fact that faces convey information that is
important in social reinforcement in at least two ways that
could be implemented by these neurons. The first is that some
may encode face expression (Rolls, Critchley, et al., 2006) (cf.
Hasselmo et al., 1989), which can indicate reinforcement. The
second way is that they encode information about which individual
is present (Rolls, Critchley, et al., 2006), which by
stimulus-reinforcement association learning is important in
evaluating and utilising learned reinforcing inputs in social
situations, e.g., about the current reinforcement value as
decoded by stimulus-reinforcement association, to a particular
individual. Between them, these neurons represent
whose face has a particular expression, and this is important
in social situations.
This system has also been shown to be present in humans.
For example, Kringelbach and Rolls (2003) showed that activation
of a part of the human orbitofrontal cortex occurs
during a face discrimination reversal task. In the task, the
faces of two different individuals are shown, and when the
correct face is selected, the expression turns into a smile (The
expression turns to angry if the wrong face is selected.). After
a period of correct performance, the contingencies reverse,
and the other face must be selected to obtain a smile
expression as a reinforcer. It was found that activation of a
part of the orbitofrontal cortex occurred specifically in relation
to the reversal, that is when a formerly correct face was
chosen, but an angry face expression was obtained. In a
control task, it was shown that the activations were not
related just to showing an angry face expression. Thus in
humans, there is a part of the orbitofrontal cortex that responds
selectively in relation to face expression specifically
when it indicates that behaviour should change, and this
activation is error-related (Kringelbach & Rolls, 2003) and occurs
when the error neurons in the orbitofrontal cortex
become active (Thorpe et al., 1983).
Also prompted by the neuronal recording evidence of face
and auditory neurons in the orbitofrontal cortex (Rolls,
Critchley, et al., 2006), it has further been shown that there
are impairments in the identification of facial and vocal
emotional expression in a group of patients with ventral
frontal lobe damage who had socially inappropriate behaviour
(Hornak et al., 1996). The expression identification impairments
could occur independently of perceptual impairments
in facial recognition, voice discrimination, or environmental
sound recognition. Poor performance on both expression tests
was correlated with the degree of alteration of emotional
experience reported by the patients. There was also a strong
positive correlation between the degree of altered emotional
experience and the severity of the behavioural problems (e.g.,
disinhibition) found in these patients (Hornak et al., 1996). A
comparison group of patients with brain damage outside the
ventral frontal lobe region, without these behavioural problems,
was unimpaired on the face expression identification
test, was significantly less impaired at vocal expression
identification, and reported little subjective emotional change
(Hornak et al., 1996). It has further been shown that patients
with discrete surgical lesions of restricted parts of the orbitofrontal
cortex may have face and/or voice expression identification
impairments, and these are likely to contribute to
their difficulties in social situations (Hornak et al., 2003).
3.3.8. Topedown effects of cognition and attention on taste,
olfactory, flavour, somatosensory, and visual processing:
cognitive enhancement of the value of affective stimuli
How does cognition influence affective value? How does
cognition influence the way that we feel emotionally? Do
cognition and emotion interact in regions that are high in the
brain’s hierarchy of processing, for example in areas where
language processing occurs, or do cognitive influences
descend down anatomically to influence the first regions that
represent the affective value of stimuli?
An fMRI study to address these fundamental issues in
brain design has shown that cognitive effects can reach down
into the human orbitofrontal cortex and influence activations
produced by odours (de Araujo et al., 2005). In this study, a
standard test odour, isovaleric acid with a small amount of
cheese flavour, was delivered through an olfactometer (The
odour alone, like the odour of brie, might have been interpreted
as pleasant, or perhaps as unpleasant.). On some trials
the test odour was accompanied with the visually presented
word label “cheddar cheese”, and on other trials with the word
label “body odour”. It was found that the activation in the
medial orbitofrontal cortex to the standard test odour was
much greater when the word label was cheddar cheese than
when it was body odour (Controls with clean air were run to
show that the effect could not be accounted for by the word
label alone.). Moreover, the word labels influenced the subjective pleasantness ratings to the test odour, and the
changing pleasantness ratings were correlated with the activations
in the human medial orbitofrontal cortex. Part of the
interest and importance of this finding is that it shows that
cognitive influences, originating here purely at the word level,
can reach down and modulate activations in the first stage of
cortical processing that represents the affective value of sensory
stimuli (de Araujo et al., 2005; Rolls, 2014).
Also important is how cognition influences the affective
brain representations of the taste and flavour of a food. This is
important not only for understanding topedown influences in
the brain, but also in relation to the topical issues of appetite
control and obesity (Rolls, 2007c, 2007d, 2010c, 2011c, 2012b).
In an fMRI study it was shown that activations related to the
affective value of umami taste and flavour (as shown by correlations
with pleasantness ratings) in the orbitofrontal cortex
were modulated by word-level descriptors (e.g., “rich and delicious
flavour”) (Grabenhorst, Rolls, & Bilderbeck, 2008).
Affect-related activations to taste were modulated in a region
that receives from the orbitofrontal cortex, the pregenual
cingulate cortex, and to taste and flavour in another region
that receives from the orbitofrontal cortex, the ventral striatum.
Affect-related cognitive modulations were not found in
the insular taste cortex, where the intensity but not the
pleasantness of the taste was represented. Thus the topedown
language-level cognitive effects reach far down into the
earliest cortical areas that represent the appetitive value of
taste and flavour. This is an important way anatomically in
which cognition influences the neural mechanisms that control
appetite and emotion.
When we see a person being touched, we may empathize
the feelings being produced by the touch. Interestingly,
cognitive modulation of this effect can be produced. When
subjects were informed by word labels that a cream seen
being rubbed onto the forearm was a “Rich moisturising
cream” versus “Basic cream”, these cognitive labels influenced
activations in the orbitofrontal/pregenual cingulate cortex
and ventral striatum to the sight of touch and their correlations
with the pleasantness ratings (McCabe et al., 2008). Some
evidence for topedown cognitive modulation of the somatosensory
effects produced by the subject being rubbed with the
cream was found in brain regions such as the orbitofrontal
and pregenual cingulate cortex and ventral striatum, but
some effects were found in other brain regions, perhaps
reflecting backprojections from the orbitofrontal cortex
(McCabe et al., 2008; Rolls, 2010a).
What may be a fundamental principle of how topedown
attention can influence affective versus non-affective processing
has recently been discovered. For an identical taste
stimulus, paying attention to pleasantness activated some
brain systems (including emotion-related limbic structures),
and paying attention to intensity, which reflected the physical
and not the affective properties of the stimulus, activated
other brain systems (Grabenhorst & Rolls, 2008). In an fMRI
investigation, when subjects were instructed to remember
and rate the pleasantness of a taste stimulus, .1 M monosodium
glutamate, activations were greater in the medial
orbitofrontal and pregenual cingulate cortex than when subjects
were instructed to remember and rate the intensity of
the taste. When the subjects were instructed to remember and
rate the intensity, activations were greater in the insular taste
cortex. Thus, depending on the context in which tastes are
presented and whether affect is relevant, the brain responds
to a taste differently. These findings show that when attention
is paid to affective value, the brain systems engaged to
represent the sensory stimulus of taste are different from
those engaged when attention is directed to the physical
properties of a stimulus such as its intensity. This differential
biasing of brain regions engaged in processing a sensory
stimulus depending on whether the attentional demand is for
affect-related versus more sensory-related processing may be
an important aspect of cognition and attention. This has
many implications for understanding attentional effects to
affective value not only on taste, but also on other sensory
stimuli (Ge, Feng, Grabenhorst, & Rolls, 2012; Luo, Ge,
Grabenhorst, Feng, & Rolls, 2013; Rolls, 2013a, 2014).
Indeed, the concept has been validated in the olfactory
system too. In an fMRI investigation, when subjects were
instructed to remember and rate the pleasantness of a jasmin
odour, activations were greater in the medial orbitofrontal and
pregenual cingulate cortex than when subjectswere instructed
to remember and rate the intensity of the odour (Rolls,
Grabenhorst, Margot, da Silva, & Velazco, 2008). When the
subjects were instructed to remember and rate the intensity,
activations were greater in the inferior frontal gyrus. These
topedown effects occurred not only during odour delivery, but
started in a preparation period after the instruction before
odour delivery, and continued after termination of the odour in
a short-term memory period. Thus, depending on the context
in which odours are presented and whether affect is relevant,
the brain prepares itself, responds to, and remembers an odour
differently. These findings show that when attention is paid to
affective value, the brain systems engaged to prepare for,
represent, and remember a sensory stimulus are different
from those engaged when attention is directed to the physical
properties of a stimulus such as its intensity. This differential
biasing of brain regions engaged in processing a sensory
stimulus depending on whether the cognitive/attentional demand
is for affect-related versus more sensory-related processing
may be important for understanding how the context
can influence how we process stimuli that may have affective
properties, how different people may respond differently to
stimuli if they process the stimuli in different ways, and more
generally, how attentional set can influence the processing of
affective stimuli by influencing processing in for example the
orbitofrontal cortex and related areas (Rolls, 2013a, 2014).
The principle thus appears to be that topedown attentional
and cognitive effects on affective value influence representations
selectively in cortical areas that process the affective
value and associated subjective emotional experience of taste
(Grabenhorst & Rolls, 2008; Grabenhorst, Rolls, & Bilderbeck,
2008) and olfactory (Anderson et al., 2003; Grabenhorst et al.,
2007; Rolls, Kringelbach, et al., 2003) stimuli in brain regions
such as the orbitofrontal cortex; whereas topedown attentional
and cognitive effects on intensity influence representations
in brain areas that process the intensity and identity of
the stimulus such as the primary taste and olfactory cortical
areas (Anderson et al., 2003; Grabenhorst & Rolls, 2008;
Grabenhorst, Rolls, & Bilderbeck, 2008; Grabenhorst et al.,
2007; Rolls, Kringelbach, et al., 2003). This is computationally appropriate in topedown models of attention (Deco & Rolls,
2005a; Rolls, 2008c, 2013a; Rolls & Deco, 2002).
To investigate the anatomical source of the topedown
modulatory effects on attentional processing,we utilised fMRI
psychophysiological interaction connectivity analyses
(Friston et al., 1997) with taste stimuli when attention was
being paid to the pleasantness or to the intensity (Grabenhorst
& Rolls, 2010). We showed that in the anterior lateral prefrontal
cortex at Y ¼ 53 mm the correlation with activity in
orbitofrontal cortex and pregenual cingulate cortex seed regions
was greater when attention was to pleasantness
compared to when attention was to intensity. Conversely, we
showed that in a more posterior region of lateral prefrontal
cortex at Y ¼ 34 the correlation with activity in the anterior
insula seed region was greater when attention was to intensity
compared to when attention was to pleasantness (Ge
et al., 2012; Grabenhorst & Rolls, 2010; Luo et al., 2013). We
proposed a biased activation theory of selective attention to
account for the findings (Grabenhorst & Rolls, 2010; Rolls,
2013a), and contrasted this with a biased competition (Deco
& Rolls, 2005b; Desimone & Duncan, 1995; Rolls, 2008c,
2008d; Rolls & Deco, 2002) theory of selective attention.
Individual differences in these reward and topedown
attentional effects, and their relation to some psychiatric
symptoms, are described elsewhere (Rolls, 2014; Rolls &
Grabenhorst, 2008).
3.3.9. Representations of specific reward value on a common
scale but with no common currency, and emotion
In my book Emotion and decision-making explained (Rolls, 2014) I
developed a unified approach to emotion, neuroeconomics,
and decision-making. This showed how emotion could be
considered as states elicited by rewards and punishers, which
are the gene-specified goals for action in an evolutionary
approach to how genes specify rewards and punishers in their
(the genes’ own) interests. The genes specify effectively the
value of many different stimuli, together with mechanisms
for devaluing the stimuli such as decreasing the reward value
of a food as it is eaten to satiety, and for ensuring that the
value of different specific rewards is on a common scale that
ensures that each specific reward is chosen as frequently as it
is advantageous to the collection of genes in an individual to
enable that individual’s genes to operate with high fitness,
that is to be passed into the next generation by sexual reproduction.
The value defined in neuroeconomics operates according
to heuristics that help to meet these requirements, for
example valuing an immediate reward more than a deferred
reward, down-valuing risky choices (those with probabilistic
outcomes), and trading of the quality of a commodity with the
quantity (Rolls, 2014). Moreover these factors are all reflected
in the responses of orbitofrontal cortex neurons, in which
different neurons represent the value of different rewards on
a continuous scale. After this, there must then be a system for
making choices between these goods, which requires now a
highly non-linear choice process, which we have suggested is
implemented anterior to the orbitofrontal cortex, in or close to
medial prefrontal cortex area 10 with attractor decisionmaking
neuronal networks (Grabenhorst & Rolls, 2011; Rolls,
2014; Rolls & Grabenhorst, 2008; Rolls, Grabenhorst, & Deco,
2010b, 2010c; Rolls, Grabenhorst, & Parris, 2010).
For this system to operate, different neurons must represent
by their firing different rewards on a continuous scale,
and much evidence for this by orbitofrontal cortex neurons
(Grabenhorst & Rolls, 2011; Rolls, 2005, 2014; Rolls &
Grabenhorst, 2008) and from activations in fMRI studies
(Grabenhorst, Rolls, & Parris, 2008; Rolls, et al., 2010b, 2010c;
Rolls, Grabenhorst, & Parris, 2010) has been presented. The
implication is that choices can be made between different
neuronal populations each specifying the value of a particular
good, so that there is no conversion to a common currency
(Rolls, 2014).
However, a classical view of economic decision theory
(Bernoulli, 1738/1954) implies that decision-makers convert
the value of different goods into a common scale of utility.
Ecological (McFarland & Sibly, 1975), psychological (Cabanac,
1992), and neuroeconomic approaches (Glimcher, 2011;
Glimcher & Fehr, 2013; Montague & Berns, 2002) similarly
suggest that the values of different kinds of rewards are
converted into a common currency. Rolls and Grabenhorst
(Grabenhorst & Rolls, 2011; Rolls, 2005, 2008c, 2014; Rolls &
Grabenhorst, 2008) have argued that different specific rewards
must be represented on the same scale, but not converted
into a common currency, as the specific goal selected
must be the output of the decision process so that the
appropriate action for that particular goal can then be chosen
(Rolls, 2014; Rolls & Grabenhorst, 2008). The key difference
between the two concepts of common currency and common
scaling lies in the specificity with which rewards are represented
at the level of single neurons. While a common currency
view implies convergence of different types of rewards
onto the same neurons (a process in which information about
reward identity is lost), a common scaling view implies that
different rewards are represented by different neurons
(thereby retaining reward identity in information processing),
with the activity of the different neurons scaled to be in the
same value range (Rolls, 2014).
An fMRI study demonstrated the existence of a region in
the human orbitofrontal cortex where activations are scaled
to the same range as a function of pleasantness for even
fundamentally different primary rewards, taste in the mouth
and warmth on the hand (Grabenhorst, D’Souza, Parris, Rolls,
& Passingham, 2010). A different study found that the decision
value for different categories of goods (food, non-food consumables,
and monetary gambles) during purchasing decisions
correlated with activity in the adjacent ventromedial
prefrontal cortex (Chib, Rangel, Shimojo, & O’Doherty, 2009).
Importantly, because of the limited spatial resolution of fMRI,
these studies do not answer whether it is the same or different
neurons in these areas that encode the value of different rewards.
However, as shown most clearly by single neuron
recording studies, the representations in the orbitofrontal
cortex provide evidence about the exact nature of each reward
(Rolls, 2009b, 2014; Rolls & Grabenhorst, 2008). Moreover, in
economic decision-making, neurons in the macaque orbitofrontal
cortex encode the economic value of the specific
choice options on offer, for example different juice rewards
(Padoa-Schioppa & Assad, 2006). For many of these “offer
value” neurons, the relationship between neuronal firing rate
and value was invariant with respect to the different types of
juice that were available (Padoa-Schioppa & Assad, 2008), suggesting that different types of juice are evaluated on a
common value scale.
With our current computational understanding of how
decisions are made in attractor neural networks (Deco & Rolls,
2006; Deco, Rolls, Albantakis, & Romo, 2013; Rolls, 2008c, 2014;
Rolls & Deco, 2010; Wang, 2002, 2008) it is important that
different rewards are expressed on a similar scale for
decision-making networks to operate correctly but retain information
about the identity of the specific reward. The
computational reason is that one type of reward (e.g., food
reward) should not dominate all other types of reward and
always win in the competition, as this would be maladaptive.
Making different rewards approximately equally rewarding
makes it likely that a range of different rewards will be
selected over time (and depending on factors such as motivational
state), which is adaptive and essential for survival
(Rolls, 2014). The exact scaling into a decision-making
attractor network will be set by the number of inputs from
each source, their firing rates, and the strengths of the synapses
that introduce the different inputs into the decisionmaking
network (Deco & Rolls, 2006; Deco, Rolls, & Romo,
2009; Rolls, 2008c; Rolls & Deco, 2010). Importantly, common
scaling need not imply conversion into a new representation
that is of a common currency of general reward (Grabenhorst
& Rolls, 2011; Rolls, 2014; Rolls & Grabenhorst, 2008). In the
decision process itself it is important to know which reward
has won, and the mechanism is likely to involve competition
between different rewards represented close together in the
cerebral cortex, with one of the types of reward winning the
competition, rather than convergence of different rewards
onto the same neuron (Deco & Rolls, 2006; Deco et al., 2009;
Rolls, 2008c, 2014; Rolls & Deco, 2010).
3.3.10. Absolute value and relative value are both represented
in the orbitofrontal cortex
For economic decision-making both absolute and relative
valuation signals have to be neurally represented. A representation
of the absolute value of rewards is important for
stable long-term preferences and consistent economic
choices (Glimcher, Camerer, Fehr, & Poldrack, 2009; Padoa-
Schioppa & Assad, 2008). Such a representation should not
be influenced by the value of other available rewards. In
contrast, to select the option with the highest subjective value
in a specific choice situation, the relative value of each option
could be represented. There is evidence for absolute value
coding in orbitofrontal cortex, in that neuronal responses that
encoded the value of a specific stimulus did not depend on
what other stimuli were available at the same time (Padoa-
Schioppa & Assad, 2008). It was suggested that transitivity, a
fundamental trait of economic choice, is reflected by the
neuronal activity in the orbitofrontal cortex (Padoa-Schioppa
& Assad, 2008). This type of encoding contrasts with valuerelated
signals found in the parietal cortex, where neurons
encode the subjective value associated with specific eye
movements in a way that is relative to the value of the other
options that are available (Kable & Glimcher, 2009). The
apparent difference in value coding between orbitofrontal
cortex and parietal cortex has led to the suggestion that absolute
value signals encoded in orbitofrontal cortex are subsequently
rescaled in the parietal cortex to encode relative
value in order to maximize the difference between the choice
options for action selection (Kable & Glimcher, 2009). However,
there is also evidence for relative encoding of value in
the orbitofrontal cortex, in that neuronal responses to a food
reward can depend on the value of the other reward that is
available in a block of trials (Tremblay & Schultz, 1999). Two
studies demonstrated that neurons in the orbitofrontal cortex
adapt the sensitivity with which reward value is encoded to
the range of values that are available at a given time
(Kobayashi, Pinto de Carvalho, & Schultz, 2010; Padoa-
Schioppa, 2009). This reflects an adaptive scaling of reward
value, evident also in positive and negative contrast effects,
that makes the system optimally sensitive to the local reward
gradient, by dynamically altering the sensitivity of the reward
system so that small changes can be detected (Rolls, 2014).
The same underlying mechanism may contribute to the
adjustment of different types of reward to the same scale
described in the preceding section.
Given that representations of both absolute value and
relative value are needed for economic decision-making,
Grabenhorst and Rolls (2009) tested explicitly whether both
types of representation are present simultaneously in the
human orbitofrontal cortex. In a task in which two odours
were successively delivered on each trial, they found that
blood oxygenation-level dependent signal (BOLD) activations
to the second odour in the antero-lateral orbitofrontal cortex
tracked the relative subjective pleasantness, whereas in the
medial and mid-orbitofrontal cortex activations tracked the
absolute pleasantness of the odour. Thus, both relative and
absolute subjective value signals, both of which provide
important inputs to decision-making processes, are separately
and simultaneously represented in the human orbitofrontal
cortex (Grabenhorst & Rolls, 2009).
3.3.11. Abstract monetary reward value, social value, and
attractiveness are represented in the orbitofrontal cortex
Many different types of reward are represented in the orbitofrontal
cortex. They include quite abstract representations,
such as monetary value. For example, the monetary outcome
reward value is represented in the medial orbitofrontal cortex,
and the monetary outcome loss in the lateral orbitofrontal
cortex (O’Doherty, Kringelbach, Rolls, Hornak, & Andrews,
2001). All these signals are reflected in activations found for
expected value and for reward outcome in the human medial
orbitofrontal cortex. Moreover, the expected value of monetary
reward as well as the outcome value of monetary reward
is represented in the medial orbitofrontal cortex (Rolls,
McCabe, et al., 2008). The beauty in a face is represented in
the orbitofrontal cortex (O’Doherty et al., 2003). Many further
types of value are represented in the orbitofrontal cortex
(Grabenhorst & Rolls, 2011; Rolls, 2014).
3.3.12. A representation of novel visual stimuli in the
orbitofrontal cortex
A population of neurons has been discovered in the primate
orbitofrontal cortex that responds to novel but not familiar
visual stimuli, and takes typically a few trials to habituate
(Rolls, Browning, Inoue, & Hernadi, 2005). The memories
implemented by these neurons last for at least 24 h. Exactly
what role these neurons have is not yet known, though this input might be part of a process whereby novel stimuli can be
rewarding (Rolls, 2014), but there are connections from the
area in which these neurons are recorded to the temporal lobe,
and activations in a corresponding orbitofrontal cortex area in
humans are found when new visual stimuli must be encoded
in memory (Frey & Petrides, 2002, 2003; Petrides, 2007).
3.4. The amygdala
The amygdala is a limbic structure that appears early in evolution,
before the orbitofrontal cortex, and although important
in emotion in rodents, may be less important in primates
including humans, in which it is in many ways overshadowed
by the orbitofrontal cortex. Part of the anatomical basis for
this may be that the orbitofrontal cortex, as a cortical structure,
naturally finds its place in the cortical hierarchy, and can
perform a number of computational functions better,
including holding items in short-term memory, and reward
reversal learning, because of its highly developed neocortical
recurrent collateral design (Rolls, 2008c, 2014).
3.4.1. Connections
The connections of the amygdala are summarized in Figs. 1
and 4, are similar in many respects to those of the orbitofrontal
cortex, and are described in more detail elsewhere
(Amaral, Price, Pitkanen, & Carmichael, 1992; Freese &
Amaral, 2009; Ghashghaei & Barbas, 2002; Rolls, 2014). The
amygdala receives massive projections in the primate from
the overlying temporal lobe cortex. These come in the monkey
to overlapping but partly separate regions of the lateral and
basal amygdala from the inferior temporal visual cortex, the
superior temporal auditory cortex, the cortex of the temporal
pole, and the cortex in the superior temporal sulcus. These
inputs thus come from the higher stages of sensory processing
in the visual and auditory modalities, and not from early
cortical processing areas. Via these inputs, the amygdala receives
inputs about objects that could become secondary reinforcers,
as a result of pattern association in the amygdala
with primary reinforcers. The amygdala also receives inputs
that are potentially about primary reinforcers, e.g., taste inputs
(from the insula, and from the secondary taste cortex in the orbitofrontal cortex), and somatosensory inputs, potentially
about the rewarding or painful aspects of touch (from
the somatosensory cortex via the insula). The amygdala receives
strong projections from the posterior orbitofrontal
cortex where there are value representations, and from the
ACC (Carmichael & Price, 1995a; Freese & Amaral, 2009;
Ghashghaei & Barbas, 2002). It is notable that the amygdala
is connected with only anterior areas of the cingulate cortex
(pregenual cingulate cortex areas 24 and 32, and subgenual
cortex area 25) (Van Hoesen, 1981; Vogt & Pandya, 1987; Yukie
& Shibata, 2009). This helps to make it clear that the emotional
and memory/spatial limbic ‘systems’ are separate. The
emotional parts of the limbic system such as the amygdala
have connections with the ACC; whereas the cingulate connections
to the hippocampus (via parahippocampal areas TF
and TH, entorhinal cortex etc.) include strong connections
from the posterior cingulate cortex (Yukie & Shibata, 2009),
which is strongly connected to visual parietal cortex areas
involved in spatial functions (Section 4).
3.4.2. Effects of amygdala lesions
Bilateral removal of the amygdala in monkeys produces
striking behavioural changes which include tameness, a lack
of emotional responsiveness, excessive examination of objects,
often with the mouth, and eating of previously rejected
items such as meat (Weiskrantz, 1956). These behavioural
changes comprise much of the KluvereBucy syndrome which
is produced in monkeys by bilateral anterior temporal lobectomy
(Kluver & Bucy, 1939). In analyses of the bases of these
behavioural changes, it has been observed that there are
deficits in some types of learning. For example, bilateral
ablation of the amygdala in the monkey produced a deficit on
learning an active avoidance task (Weiskrantz, 1956). Evidence
soon became available that associations between stimuli and
positive reinforcers (reward) were also impaired in, for
example, serial reversals of a visual discrimination made to
obtain food (Jones & Mishkin, 1972). However, when selective
lesions are made with a neurotoxin, to damage neurons but
not fibres of passage, the effects are more subtle. Using such
lesions (made with ibotenic acid) in monkeys, impairments in
the processing of food reward value were found, in that when
the reward value of one set of foods was devalued by feeding it
to satiety [i.e., sensory-specific satiety, a reward devaluation
procedure (Rolls, Sienkiewicz, et al., 1989)], the monkeys still
chose the visual stimuli associated with the foods with which
they had been satiated (Murray & Izquierdo, 2007), so there
was some impairment of reward valuation. Consistently, such
monkeys showed abnormal patterns of food choice, picking
up and eating foods not normally eaten such as meat, and
picking up and placing in their mouths inedible objects. In
addition, neurotoxic amygdala lesions (as well as orbitofrontal
cortex lesions) impaired emotional responses to snakes and
human intruders (Murray & Izquierdo, 2007). However, macaques
with neurotoxic lesions of the amygdala reveal only
mild deficits in social behaviour (Amaral, 2003; Amaral et al.,
2003), and this is consistent with the trend for the orbitofrontal
cortex to become relatively more important in emotion
and social behaviour in primates including humans.
A difference between the effects of selective amygdala lesions
and orbitofrontal cortex lesions in monkeys is that
selective amygdala lesions have no effect on object reversal
learning, whereas orbitofrontal cortex lesions do impair object
reversal learning (Murray & Izquierdo, 2007). Further, and
consistently, orbitofrontal but not selective amygdala lesions
impair instrumental extinction (i.e., macaques with orbitofrontal
cortex lesions showed a large number of choices of the
previously rewarded object when it was no longer rewarded)
(Murray & Izquierdo, 2007). This is consistent with the evidence
described in Section 3.3 that the orbitofrontal cortex is
important in rapid, one-trial, learning and reversal between
visual stimuli and primary reinforcers using both associative
and rule-based mechanisms, and its representations of
outcome value, expected value, and negative reward prediction
error. These contributions of the orbitofrontal cortex are
facilitated by its neocortical architecture, which can operate
using attractors that are important in many functions
including short-term memory, attention, rule-based operation
with switching, long-term memory, and decision-making
which may help it to compute and utilize non-reward to reset
value representations in the orbitofrontal cortex (Deco & Rolls,
2005c; Rolls, 2008c, 2014). The ability to learn very rapidly to
alter behaviour when rewards being obtained change is
important in emotional and social behaviour, and may be a
key computation made possible by the development of the
orbitofrontal cortex in primates including humans (Rolls,
2014).
In rats, there is also evidence that the amygdala is involved
in behaviour to stimuli learned as being associated with at
least classically conditioned reinforcers. We may summarize
these investigations in the rat as follows. The central nuclei of
the amygdala encode or express Pavlovian SeR (stimuluseresponse,
CSeUR where CS is the conditioned stimulus
and UR is the unconditioned response) associations (including
conditioned suppression, conditioned orienting, conditioned
autonomic and endocrine responses, and Pavlovianinstrumental
transfer); and modulate perhaps by arousal the
associability of representations stored elsewhere in the brain
(Gallagher & Holland, 1994; Holland & Gallagher, 1999). In
contrast, the basolateral amygdala (BLA) encodes or retrieves
the affective value of the predicted unconditioned stimulus
(US), and can use this to influence actioneoutcome learning
via pathways to brain regions such as the nucleus accumbens
and prefrontal cortex including the orbitofrontal cortex
(Cardinal, Parkinson, Hall, & Everitt, 2002). The nucleus
accumbens is not involved in actioneoutcome learning itself,
but does allow the affective states retrieved by the BLA to
conditioned stimuli to influence instrumental behaviour by
for example Pavlovian-instrumental transfer, and facilitating
locomotor approach to food which appears to be in rats a
Pavlovian process (Cardinal et al., 2002; Everitt, Cardinal,
Parkinson, & Robbins, 2003; Everitt & Robbins, 2013). This
leaves parts of the prefrontal and cingulate cortices as strong
candidates for actioneoutcome learning. Consistent with
these findings, the acquisition of fear-conditioning in the rat,
measured using the fear-potentiated startle test, is impaired
by local infusion of the NMDA(N-methyl-D-aspartate) receptor
antagonist AP5 (which blocks long-term potentiation, an
index of synaptic plasticity) (Davis, 1994, 2006). These investigations
have now been extended to primates, in which
similar effects are found, with ibotenic acid-induced lesions of the amygdala preventing the acquisition of fear-potentiated
startle (Antoniadis, Winslow, Davis, & Amaral, 2009).
3.4.3. Amygdala neuronal activity
In the rat, in classical (Pavlovian) conditioning of fear, for
some classes of stimulus such as pure tones, the association
between the tone and an aversive US (a footshock) is reflected
in the responses of neurons in the amygdala (LeDoux, 1995,
2000b). The auditory inputs reach the amygdala both from
the subcortical, thalamic, auditory nucleus, the medial
geniculate (medial part), and from the auditory cortex. These
auditory inputs project to the lateral nucleus of the amygdala
(LA), which in turn projects to the central nucleus of the
amygdala (Ce) both directly and via the basal (B) and accessory
basal nuclei of the amygdala. LeDoux has emphasized the role
of the subcortical inputs to the amygdala in this type of conditioning,
based on the observations that the conditioning to
pure tones can take place without the cortex, and that the
shortest latencies of the auditory responses in the amygdala
are too short to be mediated via the auditory cortex (LeDoux,
1995, 2000b). This ‘low road’ from subcortical structures
bypassing cortical processing is unlikely to be a route for most
emotions in primates including humans (Rolls, 2014), for
stimuli requiring complex analysis (e.g., of face identity and
expression and gaze) and view, translation, and size invariance
require massive cortical computation which is now
starting to become understood (Rolls, 2008c, 2012a); and the
response latencies of amygdala neurons to such stimuli are
longer than those of neurons in the inferior temporal cortex
(Rolls, 1984).
There are separate output pathways in the rat from the
amygdala for different fear-related classically conditioned
responses. Lesions of the lateral hypothalamus (which receives
from the central nucleus of the amygdala) blocked
conditioned heart rate (autonomic) responses. Lesions of the
central gray of the midbrain (which also receives from the
central nucleus of the amygdala) blocked the conditioned
freezing but not the conditioned autonomic response, and
lesions of the stria terminalis blocked the neuroendocrine
responses (LeDoux, 2000a).
In primates, the amygdala contains neurons that respond
to taste, and oral texture including viscosity, fat texture, and
capsaicin, and to somatosensory stimuli (Kadohisa et al.,
2005a; Kadohisa, Rolls, & Verhagen, 2005b; Rolls, 1992b,
2000b; Sanghera, Rolls, & Roper-Hall, 1979). These are all
potentially primary reinforcers, and these neurons could
represent outcome value. Neurons in the primate amygdala
also respond to the sight of instrumental reinforcers in a visual
discrimination task (Sanghera et al., 1979; Wilson & Rolls,
2005), and to odours. These neurons could signal expected
value. However, in reward reversal learning, many amygdala
neurons do not reverse their responses (Sanghera et al., 1979;
Wilson & Rolls, 2005), and if any reversal is found, it is slow,
taking many trials (Paton, Belova, Morrison, & Salzman, 2006;
Sanghera et al., 1979; Wilson & Rolls, 2005), whereas orbitofrontal
neurons show rule-based reversal in one trial (Thorpe
et al., 1983). Further, devaluation by feeding to satiety produced
only a partial reduction in responses to taste (Yan &
Scott, 1996), and little reduction in responses to visual stimuli
associated with food (Sanghera et al., 1979), compared to
the complete reduction found for orbitofrontal cortex taste
(Rolls, Sienkiewicz, et al., 1989) and visual food (Critchley &
Rolls, 1996a) neurons. Thus by both reversal learning and devaluation
measures of neuronal responses, the amygdala in primates
appears to make a less important contribution to reward value
representations than the orbitofrontal cortex.
Other amygdala neurons respond to faces (Leonard, Rolls,
Wilson, & Baylis, 1985; Rolls, 1992b, 2000b, 2011a; Sanghera
et al., 1979), including face expression and identity (Gothard,
Battaglia, Erickson, Spitler, & Amaral, 2007), both of which
are represented in the inferior temporal cortex areas that
project into the amygdala (Hasselmo et al., 1989). The presence
of these neurons in the primate amygdala does emphasize
that as the amygdala has evolved in primates, it does
represent information that becomes highly developed in the
primate inferior temporal cortical areas, and that is important
for social and emotional responses to individuals, though the
orbitofrontal cortex of course also represents similar information
(Rolls, Critchley, et al., 2006).
3.4.4. Amygdala damage in humans
The greater importance of the orbitofrontal cortex in emotion
in humans is emphasized by a comparison with the effects of
bilateral amygdala damage in humans, which although producing
demonstrable deficits in face processing (Adolphs
et al., 2005; Spezio, Huang, Castelli, & Adolphs, 2007),
decision-making with linked autonomic deficits (Bechara,
Damasio, Damasio, & Lee, 1999; Brand, Grabenhorst, Starcke,
Vandekerckhove, & Markowitsch, 2007), and autonomic conditioning
(Phelps & LeDoux, 2005), may not (in contrast with
the orbitofrontal cortex) produce major changes in emotion
that are readily apparent in everyday behaviour (Phelps &
LeDoux, 2005; Rolls, 2008c; Seymour & Dolan, 2008; Whalen
& Phelps, 2009).
3.5. The anterior cingulate cortex (ACG)
The ACC is a limbic structure involved in emotion, with major
inputs from structures such as the amygdala and orbitofrontal
cortex, and activations that are correlated with the pleasantness
or unpleasantness of stimuli. The ACC uses these value
(including outcome value) representations in its function with
the midcingulate cortex in actioneoutcome learning. The ACC
can be conceived as a system that in primates links the orbitofrontal
cortex representations of the value of stimuli
(including reward and punisher outcomes) with actions (Rolls,
2014).
3.5.1. Connections
The connections of the ACC (Vogt, 2009; Yukie & Shibata, 2009)
are illustrated in Fig. 5. The ACC includes area 32, area 25 the
subgenual cingulate cortex, and part of area 24. The cortex
anterior to the genu (knee, at the front) of the corpus callosum
is referred to as the pregenual cingulate cortex. The caudal
orbitofrontal cortex and amygdala project to the ACC
(Carmichael & Price, 1996; Price, 2006), and especially the
orbitofrontal cortex appears to influence the ACC strongly, for
activations related to the pleasantness or unpleasantness of
many stimuli are present in both (Grabenhorst & Rolls, 2011;
Rolls, 2014). It is of interest that the anterior but not posterior cingulate cortex receives projections from the
amygdala (with a similar trend for the caudal orbitofrontal
cortex) (Morecraft & Tanji, 2009; Yukie & Shibata, 2009),
consistent with the evidence that there are separate limbic
structures including different parts of the cingulate cortex for
emotion and memory/spatial function. Having said this, the
entorhinal cortex (area 28), the perirhinal cortex area 35/36,
and the parahippocampal cortex (areas TF and TH) (areas
illustrated in Fig. 6, and providing the gateway to the hippocampus)
do have reciprocal connections with the anterior,
mid, and posterior cingulate cortex, and the interpretation
provided in this paper is that emotional/reward/value information
from the anterior cingulate and orbitofrontal cortex
may need to be stored in the hippocampus as part of an
episodic memory involving spatial and also usually object
information; and then subsequently recalled from the hippocampus
via the backprojections to the anterior cingulate and
orbitofrontal cortex. The ACC also has connections with areas
through which it can influence autonomic function, including
the anterior insula, hypothalamus, and brainstem autonomic
nuclei (Critchley & Harrison, 2013; Vogt & Derbyshire, 2009).
3.5.2. Activations and neuronal activity
The orbitofrontal cortex projects to the pregenual cingulate
cortex (Carmichael & Price, 1996; Price, 2006), and both these
areas have reward and punishment value representations
that correlate on a continuous scale with the subjective
pleasantness/unpleasantness ratings of olfactory (Anderson
et al., 2003; Grabenhorst et al., 2007; Rolls, Critchley, Mason,
et al., 1996; Rolls, Kringelbach, et al., 2003), taste
(Grabenhorst, Rolls, & Bilderbeck, 2008; Rolls, 2008b; Rolls,
Sienkiewicz, et al., 1989; Small et al., 2003), somatosensory
(Rolls, O’Doherty, et al., 2003), temperature (Guest et al., 2007),
visual (O’Doherty et al., 2003), monetary (Knutson, Rick,
Wimmer, Prelec, & Loewenstein, 2007; O’Doherty,
Kringelbach, et al., 2001), and social stimuli (Hornak et al.,

Fig. 5 e Connections of the anterior cingulate (perigenual) and midcingulate cortical areas (shown on views of the primate
brain). The cingulate sulcus (cgs) has been opened to reveal the cortex in the sulcus, with the dashed line indicating the
depths (fundus) of the sulcus. The cingulate cortex is in the lower bank of this sulcus, and in the cingulate gyrus which
hooks above the corpus callosum and around the corpus callosum at the front and the back. The ACC extends from
cingulate areas 32, 24a and 24b to subgenual cingulate area 25 [The cortex is called subgenual because it is below the genu
(knee) formed by the anterior end of the corpus callosum, cc.]. The perigenual cingulate cortex tends to have connections
with the amygdala and orbitofrontal cortex, whereas area 24c tends to have connections with the somatosensory insula
(INS), the auditory association cortex (22, TA), and with the temporal pole cortex (38). The midcingulate areas include area
24d, which is part of the cingulate motor area. Abbreviations: as, arcuate sulcus; cc, corpus callosum; cf, calcarine fissure;
cgs, cingulate sulcus; cs, central sulcus; ls, lunate sulcus; ios, inferior occipital sulcus; mos, medial orbital sulcus; os, orbital
sulcus; ps, principal sulcus; sts, superior temporal sulcus; lf, lateral (or Sylvian) fissure (which has been opened to reveal the
insula); A, amygdala; INS, insula; NTS, autonomic areas in the medulla, including the nucleus of the solitary tract and the
dorsal motor nucleus of the vagus; TE (21), inferior temporal visual cortex; TA (22), superior temporal auditory association
cortex; TF and TH, parahippocampal cortex; TPO, multimodal cortical area in the superior temporal sulcus; 10, medial
prefrontal cortex area 10; 12, 13, 11, orbitofrontal cortex; 23, 31, posterior cingulate cortex areas; 28, entorhinal cortex; 38,
TG, temporal pole cortex; 51, olfactory (prepyriform and periamygdaloid) cortex.
posterior cingulate cortex receives projections from the
amygdala (with a similar trend for the caudal orbitofrontal
cortex) (Morecraft & Tanji, 2009; Yukie & Shibata, 2009),
consistent with the evidence that there are separate limbic
structures including different parts of the cingulate cortex for
emotion and memory/spatial function. Having said this, the
entorhinal cortex (area 28), the perirhinal cortex area 35/36,
and the parahippocampal cortex (areas TF and TH) (areas
illustrated in Fig. 6, and providing the gateway to the hippocampus)
do have reciprocal connections with the anterior,
mid, and posterior cingulate cortex, and the interpretation
provided in this paper is that emotional/reward/value information
from the anterior cingulate and orbitofrontal cortex
may need to be stored in the hippocampus as part of an
episodic memory involving spatial and also usually object
information; and then subsequently recalled from the hippocampus
via the backprojections to the anterior cingulate and
orbitofrontal cortex. The ACC also has connections with areas
through which it can influence autonomic function, including
the anterior insula, hypothalamus, and brainstem autonomic
nuclei (Critchley & Harrison, 2013; Vogt & Derbyshire, 2009).
3.5.2. Activations and neuronal activity
The orbitofrontal cortex projects to the pregenual cingulate
cortex (Carmichael & Price, 1996; Price, 2006), and both these
areas have reward and punishment value representations
that correlate on a continuous scale with the subjective
pleasantness/unpleasantness ratings of olfactory (Anderson
et al., 2003; Grabenhorst et al., 2007; Rolls, Critchley, Mason,
et al., 1996; Rolls, Kringelbach, et al., 2003), taste
(Grabenhorst, Rolls, & Bilderbeck, 2008; Rolls, 2008b; Rolls,
Sienkiewicz, et al., 1989; Small et al., 2003), somatosensory
(Rolls, O’Doherty, et al., 2003), temperature (Guest et al., 2007),
visual (O’Doherty et al., 2003), monetary (Knutson, Rick,
Wimmer, Prelec, & Loewenstein, 2007; O’Doherty,
Kringelbach, et al., 2001), and social stimuli (Hornak et al., 2003; Kringelbach & Rolls, 2003; Moll et al., 2006; Spitzer,
Fischbacher, Herrnberger, Gron, & Fehr, 2007) (see further
Bush, Luu, & Posner, 2000; Grabenhorst & Rolls, 2011; Rolls,
2009a, 2014). Indeed, the pregenual cingulate cortex may be
identified inter alia as a tertiary cortical taste area, with single
neurons responding to the taste and texture of food (Rolls,
2008b). Moreover, there is very interesting topology, with the
activations that are correlated with the pleasantness of
stimuli in the pregenual cingulate cortex, and the activations
that are correlated with the unpleasantness of stimuli just
dorsal and posterior to this, extending back above the genu of
the corpus callosum (Grabenhorst & Rolls, 2011; Rolls, 2014).
We may ask why, if the activations in the orbitofrontal
cortex and the pregenual cingulate cortex are somewhat
similar in their continuous and typically linear representations
of reward or affective value (pleasantness ratings), are
there these two different areas? A suggestion I make (Rolls,
2014) is that the orbitofrontal cortex is the region that computes
the rewards, expected rewards etc., and updates these
rapidly when the reinforcement contingencies change, based
on its inputs about primary reinforcers from the primary taste
cortex (Baylis et al., 1995), the primary olfactory cortex
(Carmichael et al., 1994), the somatosensory cortex (Morecraft
et al., 1992), etc. The orbitofrontal cortex makes explicit in its
representations the reward value, based on these inputs, and
in a situation where reward value is not represented at the
previous tier, but instead where the representation is about
the physical properties of the stimuli, their intensity, etc.
(Grabenhorst & Rolls, 2008; Grabenhorst, Rolls, & Bilderbeck,
2008; Grabenhorst et al., 2007; Rolls, 2014; Rolls, Grabenhorst,
& Parris, 2008; Rolls, O’Doherty, et al., 2003; Small et al.,
2003) (see Fig. 1). The orbitofrontal cortex computes the expected
value of previously neutral stimuli, and updates these
representations rapidly when the reinforcement contingencies
change, as described here. Thus the orbitofrontal
cortex is the computer of reward magnitude and expected
reward value. It can thus represent outcomes, and expected
outcomes, but it does not represent actions such as motor
responses or movements (Rolls, 2014). It is suggested that the
representations of outcomes, and expected outcomes, are
projected from the orbitofrontal cortex to the pregenual
cingulate cortex, as the cingulate cortex has longitudinal
connections which allow this outcome information to be
linked to the information about actions that is represented in
the midcingulate cortex, and that the outcome information
derived from the orbitofrontal cortex can contribute to
actioneoutcome learning implemented in the cingulate cortex
(Rolls, 2008c, 2014; Rushworth, Behrens, Rudebeck, &
Walton, 2007; Rushworth, Buckley, Behrens, Walton, &
Bannerman, 2007; Rushworth, Noonan, Boorman, Walton, &
Behrens, 2011). Some of this evidence on actioneoutcome
learning has been obtained in rat lesion studies, and indicates
that the costs of actions, as well as the rewards produced by
actions, involve the ACC. Although the ACC is activated in
relation to autonomic function (Critchley, Wiens, Rotshtein,
Ohman, & Dolan, 2004), its functions clearly extend much beyond this, as shown also for example by the emotional
changes that follow damage to the ACC and related areas in
humans (Hornak et al., 2003).
In responding when the reward obtained is less than that
expected, the orbitofrontal cortex negative reward prediction
error neurons are working in a domain that is related to the
sensory inputs being received (expected reward and reward
obtained). There are also error neurons in the ACC that
respond when errors are made (Niki & Watanabe, 1979), or
when rewards are reduced (Shima & Tanji, 1998) (and in
similar imaging studies, Bush et al., 2002). Some of these
neurons may be influenced by the projections from the orbitofrontal
cortex, and reflect a mismatch between the reward
expected and the reward that is obtained. However, some
error neurons in the ACC may reflect errors that arise when
particular behavioural responses or actions are in error, and
this type of error may be important in helping an action system
to correct itself, rather than, as in the orbitofrontal cortex,
when a reward prediction system about stimuli needs to be
corrected. Consistent with this, many studies provide evidence
that errors made in many tasks activate the anterior/
midcingulate cortex, whereas tasks with response conflict
activate the superior frontal gyrus (Matsumoto, Matsumoto,
Abe, & Tanaka, 2007; Rushworth & Behrens, 2008;
Rushworth, Walton, Kennerley, & Bannerman, 2004; Vogt,
2009).
A neuroimaging study in humans illustrates how nonreward/
error signals relevant to emotion and social behaviour
are found in the ACC. Kringelbach et al. (2003) used the
faces of two different people, and if one face was selected then
that face smiled, and if the other was selected, the face
showed an angry expression. After good performance was
acquired, there were repeated reversals of the visual
discrimination task. Kringelbach et al. (2003) found that activation
of a lateral part of the orbitofrontal cortex and in a
dorsal part of the ACC in the fMRI study was produced on the
error trials, that is when the human chose a face, and did not
obtain the expected reward. An interesting aspect of this
study that makes it relevant to human social behaviour is that
the conditioned stimuli were faces of different individuals,
and the USs were face expressions. Thus the association that
was being reversed in this study was between a representation
of face identity and a representation of face expression.
Moreover, the study reveals that the human orbitofrontal
cortex and ACC are very sensitive to social feedback when it
must be used to change behaviour (Kringelbach & Rolls, 2003,
2004).
3.5.3. Effects of ACC lesions in humans
Cardinal et al. (2002) and Devinsky, Morrell, and Vogt (1995)
review evidence that anterior cingulate lesions in humans
produce apathy, autonomic dysregulation, and emotional
instability.
An investigation in patients with selective surgical lesions
has shown that patients with unilateral lesions of the anteroventral
part of the ACC and/or medial area 9 were in some
cases impaired on voice and face expression identification,
had some change in social behaviour (such as inappropriateness,
being less likely to notice when other people were angry,
not being close to his or her family, and doing things without
thinking), and had significant changes in their subjective
emotional state (Hornak et al., 2003). Unilateral lesions were
sufficient to produce these effects, and there were no strong
laterality effects.
The results of Hornak et al. (2003) also confirmed that
damage restricted to the orbitofrontal cortex can produce
impairments in face and voice expression identification,
which may be primary reinforcers. The system is sensitive, in
that even patients with unilateral orbitofrontal cortex lesions
may be impaired. The impairment is not a generic impairment
of the ability to recognize any emotions in others, in that
frequently voice but not face expression identification was
impaired, and vice versa. This implies some functional
specialization for visual versus auditory emotion-related
processing in the human orbitofrontal cortex. The results
also show that the changes in social behaviour can be produced
by damage restricted to the orbitofrontal cortex. The
patients were particularly likely to be impaired on emotion
recognition (they were less likely to notice when others were
sad, or happy, or disgusted); on emotional empathy (they were
less likely to comfort those who are sad, or afraid, or to feel
happy for others who are happy); on interpersonal relationships
(not caring what others think, and not being close to his/
her family); and were less likely to cooperate with others; were
impatient and impulsive; and had difficulty in making and
keeping close relationships. The results also show that
changes in subjective emotional state (including frequently
sadness, anger and happiness) can be produced by damage
restricted to the orbitofrontal cortex (Hornak et al., 2003). In
addition, the patients with bilateral orbitofrontal cortex lesions
were impaired on the probabilistic reversal learning task
(Hornak et al., 2004). The findings overall thus make clear the
types of deficit found in humans with orbitofrontal cortex
damage, and can be easily related to underlying fundamental
processes in which the orbitofrontal cortex is involved as
described by Rolls (1999a, 2005), including decoding and representing
primary reinforcers, being sensitive to changes in
reinforcers, and rapidly readjusting behaviour to stimuli when
the reinforcers available change. The implication is that some
of the inputs to the ACC which produce similar deficits may
come from the orbitofrontal cortex. Consistent with this,
unilateral lesions of the ACC (including some of medial area 9)
can produce voice and/or face expression identification deficits,
changes in social behaviour, and marked changes in
subjective emotional state (Hornak et al., 2003).
In summary, the primate including human ACC has connections
with other limbic structures such as the amygdala
and limbic-related structures such as the caudal orbitofrontal
cortex, receives reward and punishment-related information
from these brain regions, and appears to be involved in using
this reward and punishment information to learn about actions
to obtain goals, taking into account the costs of the actions
(Grabenhorst & Rolls, 2011; Rolls, 2009a, 2014; Rushworth
et al., 2011). Appropriate actions to emotion-provoking stimuli
may not be made after ACC damage. The ACC can thus be seen
as a limbic structure involved in emotion that forms part of a
system with other limbic structures involved in emotion, the
amygdala and orbitofrontal cortex. Structures that receive
input from these three structures are involved in linking to
habit responses (striatum including ventral striatum and nucleus accumbens), autonomic and endocrine output
(insula, hypothalamus, brainstem autonomic nuclei), etc.
(Rolls, 2014).
These three limbic and related structures, the amygdala,
orbitofrontal cortex, and ACC, could be thought of as an
emotion limbic system, but not the limbic system, as different
limbic structures related to the hippocampus are involved
primarily in episodic memory and related spatial function,
and not in emotion. Damage to the emotion limbic system or
structures is not described as impairing episodic memory or
spatial function.
3.6. The insular cortex
The insula has sometimes been lumped in with limbic structures
(Catani et al., 2013; Yakovlev, 1948), and parts of it are
involved in autonomic/visceral function (Critchley &
Harrison, 2013), so it is considered briefly here, and in more
detail elsewhere (Rolls, 2014).
3.6.1. The insular primary taste cortex
The primary taste cortex is in the dorsal part of the anterior
insula and adjoining frontal operculum (Pritchard, Hamilton,
Morse, & Norgren, 1986), and this region projects to the orbitofrontal
cortex (Baylis et al., 1995). Neurons in the primary
taste cortex represent what the taste is (including sweet, salt,
sour, bitter, and umami) (Baylis & Rolls, 1991; Rolls, 2009c;
Scott et al., 1986; Yaxley et al., 1990), but do not represent
reward value in that their responses are not decreased by
feeding to satiety (Rolls et al., 1988; Yaxley et al., 1988). Neurons
in the insular primary taste cortex do represent oral
texture including fat texture (de Araujo & Rolls, 2004;
Kadohisa et al., 2005a; Verhagen et al., 2004) and oral temperature
(Guest et al., 2007; Kadohisa et al., 2005a; Verhagen
et al., 2004). Neurons in the macaque primary taste cortex do
not have olfactory responses (Verhagen et al., 2004), and
consistently in a human fMRI investigation of olfactory and
taste convergence in the brain, it was shown that an agranular
more anterior past of the insula does show convergence between
taste and odour to represent flavour (de Araujo, Rolls,
et al., 2003).
3.6.2. The visceral/autonomic insular cortex
A region of the anterior insular cortex just ventral to the primary
taste cortex has strong projections to the orbitofrontal
cortex (Baylis et al., 1995), and is putatively the visceral/autonomic
region of the insula (Critchley & Harrison, 2013). This
anterior insular region probably receives inputs from the
orbitofrontal cortex and ACC (Price, 2006, 2007), which decode
and represent the reward and punishment-related signals that
can produce autonomic/visceral responses (Rolls, 2014). It is
suggested (Rolls, 2014) that when the anterior insula is activated
by emotion-related stimuli and events such as face expressions
of disgust (Phillips et al., 1998, 2004) and unfair offers
(which are aversive) in an ultimatum game (Sanfey, Rilling,
Aronson, Nystrom, & Cohen, 2003), the activations reflect the
effects produced by regions such as the orbitofrontal and
anterior cingulate cortices that can produce emotional and
autonomic responses, rather than face expression decoding
[which is performed elsewhere, in the cortex in the superior
temporal sulcus and in the orbitofrontal cortex (Hasselmo
et al., 1989; Rolls, 2007b, 2012a, 2011a)] or economic computations.
Consistent with this, in a neuroeconomics study with
monetary reward, it was found that expected value was
negatively correlated with activations in the anterior insula
[38 24 16] in a region that has been implicated in disgust, and
interestingly, the activations here were also correlated with
the uncertainty of the magnitude of the reward that would be
obtained (Rolls, McCabe, et al., 2008). Effectively there was
more insula activation in situations that might be described as
aversive. This part of the insula has activations that are related
to visceral/autonomic function, for example to heart and
stomach responses during disgust-associated nausea
(Critchley & Harrison, 2013). Further, electrical stimulation in
the antero-ventral insula produced feelings related to disgust,
including viscero-autonomic feelings (Krolak-Salmon et al.,
2003). Moreover, it is of course to be expected, and is the
case, that the autonomic output and the corresponding
visceral insular activity will be different for different emotional
states, e.g., when eating a food versus when reacting to the
disgusting bitter taste of quinine or to pain or the sight of
aversive or unpleasant stimuli. Menon and Uddin (2010) have
suggested that the insula is part of a ‘saliency’ network. They
postulate that the insula is sensitive to salient events, and that
its core function is to mark such events for additional processing
and initiate appropriate control signals. However,
given the inputs to the anterior insula from the orbitofrontal
and anterior cingulate cortices, which decode stimuli and
events such as rewards, punishers, non-reward, and novel
stimuli, it is suggested (Rolls, 2014) that the anterior insulamay
respond to though not compute such “salient” stimuli and
events, and its activation may reflect autonomic and related
responses which are of course elicited by these “salient”
stimuli. A related point (Rolls, 2014) is relevant to Damasio’s
somatic marker hypothesis (Damasio, 1996).
3.6.3. The somatosensory insula
The mid- and posterior insula has somatosensory representations
of the body (Mufson & Mesulam, 1982). A property that
may be special about these somatosensory cortical representations
is that activations are produced by touch to the body
but, in contrast to many other somatosensory cortical areas,
not by the sight of touch (McCabe et al., 2008). It was therefore
suggested that insular cortex activation thus allows an individual
to know that it is touch to the person’s body, and not
that someone else’s body is about to be touched (McCabe et al.,
2008). The insular somatosensory cortex may thus provide
evidence about what is happening to one’s own body (Rolls,
2010a). The same might be said of the insular primary taste
cortex, which when activated leaves no doubt that one is
tasting, and not seeing someone else tasting. So, in a sense,
feelings associated with activations of the insular cortex [and
they are: the subjective intensity of taste is linearly related to
the activation of the primary taste cortex (Grabenhorst &
Rolls, 2008)] do inform one about the state of one’s own
body, and this relates to Craig’s suggestions (Craig, 2009, 2011)
about the importance of the insula in interoceptive feelings.
However, this does not mean that the insular cortex is
necessary for body feelings, and indeed that seems to be ruled
out by the finding that a patient with extensive bilateral damage to the insular cortex reported normal body/emotional
feelings (Damasio, Damasio, & Tranel, 2013).
4. A hippocampal limbic system for memory
and spatial function
Evidence will be described that the hippocampus and its
connected structures are involved in episodic memory, and
not in emotion. This is thus a separate system from the
amygdala/orbitofrontal cortex/ACC emotional system. Moreover,
the computational principles of operation of these two
systems are very different.
4.1. Connections
The primate hippocampus receives inputs via the entorhinal
cortex (area 28) and the highly developed parahippocampal
gyrus (areas TF and TH) as well as the perirhinal cortex (area
35/36) from the ends of many processing streams of the cerebral
association cortex, including the visual and auditory
temporal lobe association cortical areas, the prefrontal cortex,
and the parietal cortex (Aggleton, 2012; Amaral, 1987; Amaral
et al., 1992; Lavenex, Suzuki, & Amaral, 2004; Rolls, 2008c; Rolls
& Kesner, 2006; Suzuki & Amaral, 1994b; Van Hoesen, 1982;
Witter, Wouterlood, Naber, & Van Haeften, 2000; Yukie &
Shibata, 2009) (see Figs. 6 and 7). The hippocampus is thus
by its connections potentially able to associate together object
representations (from the temporal lobe visual and auditory
cortical areas via entorhinal and perirhinal cortex), and spatial
representations (from the parietal cortical areas including the
posterior cingulate cortex via parahippocampal areas TF and
TH). In addition, the entorhinal cortex receives inputs from
the amygdala and the orbitofrontal cortex (Carmichael &
Price, 1995a; Pitkanen, Kelly, & Amaral, 2002; Price, 2006;
Rolls, 2010b; Stefanacci, Suzuki, & Amaral, 1996; Suzuki &
Amaral, 1994a), which thus provide reward-related information
to the hippocampus. The primary output from the hippocampus to neocortex
originates in CA1 and projects to subiculum, entorhinal cortex,
and parahippocampal structures (areas TFeTH) as well as
prefrontal cortex (Delatour &Witter, 2002; van Haeften, Bakste-
Bulte, Goede, Wouterlood, & Witter, 2003; Van Hoesen,
1982; Witter, 1993) (see Figs. 6 and 7). These are the pathways
that are likely to be involved in the recall of information from
the hippocampus. There are other outputs (Rolls & Kesner,
2006), including subicular complex connections to the orbitofrontal
cortex and ACC, and even direct connections from
CA1 to the orbitofrontal cortex and ACC (Price, 2006; Vogt &
Pandya, 1987; Yukie & Shibata, 2009), which are likely it is
suggested to be involved in the recall of emotional and
reward-related components of episodic memory; and subicular
complex projections to the posterior cingulate cortex
(Vogt & Pandya, 1987; Yukie & Shibata, 2009), which are likely
it is suggested to be involved in the recall of spatial components
of episodic memory.
In addition, there are subcortical connections that form
Papez’ circuit (Papez, 1937). The subiculum (subicular complex)
projects via the fornix to the mammillary bodies (Fig. 7),
which then project via the mammillo-thalamic tract to the
anterior thalamic nuclei, which project most strongly to the
posterior cingulate cortex (Shibata & Yukie, 2009), which in
turn via the cingulum projects back towards the hippocampus
via the parahippocampal cortex (Yukie & Shibata, 2009). The
fornix also conveys the cholinergic input from the septal
nuclei, classed as limbic structures, to the hippocampus, and
this must be taken into account when considering the effects
on memory of damage to the hippocampus, for acetylcholine
facilitates synaptic modification and regulates recurrent
collateral efficacy in the CA3 system (Giocomo & Hasselmo,
2007; Rolls, 2010b).
4.2. Effects of hippocampal system damage in primates
including humans
In humans, episodic memory, the memory of a particular
episode, requires the ability to remember particular events,
and to distinguish them from other events, and is impaired by
damage to the hippocampal system. An event consists of a set
of items that occur together, such as seeing a particular object
or person’s face in a particular place. An everyday example
might be remembering where one was for dinner, who was
present, what was eaten, what was discussed, and the time at
which it occurred. The spatial context is almost always an
important part of an episodic memory (Dere, Easton, Nadel, &
Huston, 2008), and it may be partly for this reason that
episodic memory is linked to the functions of the hippocampal
system, which is involved in spatial processing and
memory. A famous case, is that of H.M., who after surgery for
epilepsy that removed bilaterally parts of the temporal lobe
including parts of the hippocampus could no longer form
memories of events that occurred after the damage (anterograde
amnesia), but could recall memories of events prior to
the hippocampal damage (Corkin, 2002), and similar impairments
are found in other patients with damage to the hippocampus
and connected structures (Squire & Wixted, 2011).
Indeed, section of the fornix in humans, at one time a side
effect of surgery on a third ventricle cyst, produces similar
amnesia (Gaffan & Gaffan, 1991), as can damage to the
mammillary body/mammillo-thalamic tract/anterior nucleus
of the thalamus pathway (see Aggleton, 2012). In humans,
functional neuroimaging shows that the hippocampal system
is activated by allocentric spatial processing and episodic
memory (Burgess, 2008; Burgess, Maguire, & O’Keefe, 2002;
Chadwick, Hassabis, Weiskopf, & Maguire, 2010; Hassabis
et al., 2009).
Damage to the hippocampus or to some of its connections
such as the fornix in monkeys produces deficits in learning
about the places of objects and about the places where responses
should be made (Buckley & Gaffan, 2000). For
example, macaques and humans with damage to the hippocampal
system or fornix are impaired in object-place memory
tasks in which not only the objects seen, but where they were
seen, must be remembered (Burgess et al., 2002; Crane &
Milner, 2005; Gaffan, 1994; Gaffan & Saunders, 1985;
Parkinson, Murray, & Mishkin, 1988; Smith & Milner, 1981).
Posterior parahippocampal lesions in macaques impair even a
simple type of object-place learning in which the memory
load is just one pair of trial-unique stimuli (Malkova &
Mishkin, 2003) (It is further predicted that a more difficult
object-place learning task with non-trial-unique stimuli and
with many object-place pairs would be impaired by neurotoxic
hippocampal lesions.). Further, neurotoxic lesions that
selectively damage the primate hippocampus impair spatial
scene memory, tested by the ability to remember where in a
scene to touch to obtain reward (Murray, Baxter, & Gaffan,
1998). Also, fornix lesions impair conditional lefteright
discrimination learning, in which the visual appearance of an
object specifies whether a response is to be made to the left or
the right (Rupniak & Gaffan, 1987). A comparable deficit is
found in humans (Petrides, 1985). Fornix sectioned monkeys
are also impaired in learning on the basis of a spatial cue
which object to choose (e.g., if two objects are on the left,
choose object A, but if the two objects are on the right, choose
object B) (Gaffan & Harrison, 1989a). Monkeys with fornix
damage are also impaired in using information about their
place in an environment. For example, there are learning
impairments when which of two or more objects the monkey
had to choose depended on the position of the monkey in the
room (Gaffan & Harrison, 1989b).
More recently, Lavenex et al. have described deficits produced
by hippocampal damage in monkeys performing allocentric
spatial memory tasks (Banta Lavenex & Lavenex,
2009). One such task involved freely moving in an environment
using allocentric spatial room cues to remember the
locations of inverted cups that contained food. This is a food
reward e allocentric place association task. In contrast, the
perirhinal cortex, area 35, with its connections to inferior
temporal cortex areas involved in object perception (Rolls,
2008c, 2012a), is involved in recognition memory (Buckley,
2005), and indeed contains neurons related to long-term familiarity
memory (Ho¨ lscher, Rolls, & Xiang, 2003; Rolls, 2008c;
Rolls, Franco, & Stringer, 2005).
Rats with hippocampal lesions are also impaired in using
environmental spatial cues to remember particular places
(Cassaday & Rawlins, 1997; Jarrard, 1993; Kesner, Lee, &
Gilbert, 2004; Kesner, Morris, & Weeden, 2012; Martin,
Grimwood, & Morris, 2000; O’Keefe & Nadel, 1978), to utilize spatial cues or to bridge delays (Kesner, 1998; Kesner et al.,
2004; Kesner & Rolls, 2001; Rawlins, 1985; Rolls & Kesner,
2006), to perform object-place memory (Kesner et al., 2012;
Rolls & Kesner, 2006), or to perform relational operations on
remembered material (Eichenbaum, 1997).
It is notable that emotional changes, reward-related
learning and reversal impairments, and devaluation impairments
of stimuli are not produced by damage to this hippocampal
system. This is further evidence for one of the theses
of this paper, that we should no longer be considering the
operation of limbic and related structures as being part of a
single limbic system. There is a double dissociation of functions,
with the orbitofrontal/amygdala/ACC limbic system
being involved in emotion but not episodic memory, and the
hippocampal system being involved in memory but not in
emotion.
4.3. Neuronal representations in the primate
hippocampus
The systems-level neurophysiology of the hippocampus
shows what information could be stored or processed by the
hippocampus. To understand how the hippocampus works it
is not sufficient to state just that it can store information e one
needs to know what information. The systems-level neurophysiology
of the primate hippocampus has been reviewed by
Rolls and Xiang (2006), and a summary is provided here
because it provides a perspective relevant to understanding
the function of the human hippocampus that is somewhat
different from that provided by the properties of place cells in
rodents, which have been reviewed elsewhere (Jeffery,
Anderson, Hayman, & Chakraborty, 2004; Jeffery & Hayman,
2004; McNaughton, Barnes, & O’Keefe, 1983; Muller, Kubie,
Bostock, Taube, & Quirk, 1991; O’Keefe, 1984).
4.3.1. Spatial view neurons in the primate hippocampus
We have shown that the primate hippocampus contains
spatial cells that respond when the monkey looks at a certain
part of space, for example at one quadrant of a video monitor
while the monkey is performing an object-place memory task
in which he must remember where on the monitor he has
seen particular images (Rolls, 1999c; Rolls, Miyashita, et al.,
1989). Approximately 9% of hippocampal neurons have such
spatial view fields, and approximately 2.4% combine information
about the position in space with information about the
object that is in that position in space (Rolls, Miyashita, et al.,
1989). The representation of space is for the majority of hippocampal
neurons in allocentric not egocentric coordinates
(Feigenbaum & Rolls, 1991). These spatial view cells can be
recorded while monkeys move themselves round the test
environment by walking (or running) on all fours (Georges-
Franc¸ois, Rolls, & Robertson, 1999; Robertson, Rolls, &
Georges-Franc¸ois, 1998; Rolls, Robertson, & Georges-
Franc¸ois, 1997; Rolls, Treves, Robertson, Georges-Franc¸ois, &
Panzeri, 1998). These hippocampal ‘spatial view neurons’
respond significantly differently for different allocentric
spatial views and have information about spatial view in their
firing rate, but do not respond differently just on the basis of
eye position, head direction, or place (Georges-Franc¸ois et al.,
1999). If the view details are obscured by curtains and
darkness, then some spatial view neurons (especially those in
CA1 and less those in CA3) continue to respond when the
monkey looks towards the spatial view field, showing that
these neurons can be updated for at least short periods by
idiothetic (self-motion) cues including eye position and head
direction signals (Robertson et al., 1998; Rolls, Treves, Foster, &
Perez-Vicente, 1997).
4.3.2. Object-place neurons in the primate hippocampus
A fundamental question about the function of the primate
including human hippocampus in relation to episodic memory
is whether object as well as allocentric spatial information
is represented. To investigate this, Rolls, Xiang, and Franco
(2005) made recordings from single hippocampal formation
neurons while macaques performed an object-place memory
task that required the monkeys to learn associations between
objects, and where they were shown in a room. Some neurons
(10%) responded differently to different objects independently
of location; other neurons (13%) responded to the spatial view
independently of which object was present at the location;
and some neurons (12%) responded to a combination of a
particular object and the place where it was shown in the
room. These results show that there are separate as well as
combined representations of objects and their locations in
space in the primate hippocampus. This is a property required
in an episodic memory system, for which associations between
objects and the places where they are seen, are prototypical.
The results thus show that a requirement for a human
episodic memory system, separate and combined neuronal
representations of objects and where they are seen “out there”
in the environment, are present in the primate hippocampus
(Rolls, Xiang, et al., 2005).
What may be a corresponding finding in rats is that some
rat hippocampal neurons respond on the basis of the
conjunction of location and odour (Wood, Dudchenko, &
Eichenbaum, 1999). Results consistent with our object-place
neurons in primates are that Diamond et al. have now
shown using the vibrissa somatosensory input for the ‘object’
system, that rat hippocampal neurons respond to object-place
combinations, objects, or places, and there is even a rewardplace
association system in rats (Itskov, Vinnik, & Diamond,
2011) similar to that in primates described below. This
brings the evidence from rats closely into line with the evidence
from primates of hippocampal neurons useful for
object-place episodic associative memory.
Spatial view cells and object-place cells, are also present in
the parahippocampal areas (Georges-Franc¸ois et al., 1999;
Robertson et al., 1998; Rolls, Robertson, et al., 1997; Rolls,
Treves, et al., 1998; Rolls, Xiang, et al., 2005). There are backprojections
from the hippocampus to the entorhinal cortex
and thus to parahippocampal areas, and these backprojections
could enable the hippocampus to influence the
spatial representations found in the entorhinal cortex and
parahippocampal gyrus. On the other hand, some of the
spatial functions may be provided for in these parahippocampal
areas, which will in turn influence the hippocampus.
However, it is argued below that the hippocampus
may be able to make a special contribution to event or episodic
memory, by enabling in the CA3 network with its very widespread
recurrent collateral connections an association between any one item with any other item to form an arbitrary
association to represent an event.
4.3.3. Recall-related neurons in the primate hippocampus
It has been possible to investigate directly, neurophysiologically,
the hippocampal recall process in primates (Rolls &
Xiang, 2006). We used a visual object-place memory task
because this is prototypical of episodic memory. It has been
shown that a one-trial odour-place recall memory task is
hippocampal-dependent in rodents (Day, Langston, & Morris,
2003). We designed a one-trial object-place recall task, in
which the whole memory was recalled from a part of it. Images
of new objects were used each day, and within a day the
same objects were used, so that with non-trial unique objects
within a day, the recall task is quite difficult.
Recordings were made from 347 neurons in the hippocampus
of a macaque performing the object-place recall task.
The following types of neurons were found in the task (Rolls &
Xiang, 2006).
One type of neuron had responses that occurred to one of
the objects used in the task. A number of these neurons had
activity that was related to the recall process. For example,
one type of single neuron had activity that was greater to
object one when it was shown, but also when the object was
no longer visible, and the macaque was touching the recalled
location of that object. Thus while the location was being
recalled from the object, this type of neuron continued to
respond as if the object was present, that is it kept the representation
of the object active after the object was no longer
visible, and the place to touch was being recalled. Sixteen of
the neurons responded in this way (Rolls & Xiang, 2006). None
of these neurons had differential responses for the different
places used in the object-place recall task.
A second type of neuron had responses related to the place
(left or right) in which an object was shown. This type of
neuron responded more for example when an object was
shown in the left position (P1) than in the right position (P2) on
the screen. Interestingly, when the recall object was shown
later in the trial in the top centre of the screen, the neuron also
responded as if the left position (P1) was being processed on
trials on which the left position had to be recalled. Thus this
type of neuron appeared to reflect the recall of the position on
the screen at which the object had been represented. Thirteen
neurons had differential responses to the different places P1
and P2, and continued to show place-related activity in the
recall part of the task (Rolls & Xiang, 2006). The new finding is
that 13 of the neurons had place-related responses when a
place was being recalled by an object cue. The recording sites
of the object and of the place neurons were within the hippocampus
proper (Rolls & Xiang, 2006). The mean firing rate of
the population of responsive neurons to the most effective
object or place was 7.2  .6 spikes/sec (sem), and their mean
spontaneous rate was 3.2  .6 spikes/sec.
These findings (Rolls & Xiang, 2006) are the first we know in
the primate hippocampus of neuronal activity that is related to
recall. It is particularly interesting that the neurons with
continuing activity to the object after it had disappeared in the
recall phase of the task could reflect the operation of the objectplace
recall process that ishypothesized totake place in theCA3
cells. By continuing to respond to the object while the place is
being recalled in the task, the object-related neurons could be
part of the completion of the whole object-place combination
memory from an autoassociation or attractor process in CA3
(Rolls & Kesner, 2006). Consistent with these findings, and with
thecomputational theory, ithasnowbeenreportedthathuman
hippocampal neurons are activated during recall (Gelbard-
Sagiv, Mukamel, Harel, Malach, & Fried, 2008).
The neurons with recall-related activity in the object-place
recall task also provide neurophysiological evidence on the
speed of association learning in the hippocampal formation.
Given that this is a one-trial object-place recall task, with the
association between the object and its place being made in
stages 1 and 2 of each trial, it is clear that it takes just one trial
for the object-place associations to be formed that are relevant
to the later recall on that trial (Rolls & Xiang, 2006). This is
the speed of learning that is required for episodic memory,
and this neurophysiological evidence shows that this type of
rapid, one-trial, object-place learning is represented in the
primate hippocampus (Rolls, 2010b).
4.3.4. Reward-place neurons in the primate hippocampus
The primate anterior hippocampus (which corresponds to the
rodent ventral hippocampus) receives inputs from brain regions
involved in reward processing such as the amygdala and
orbitofrontal cortex (Aggleton, 2012; Pitkanen et al., 2002). To
investigate how this affective input may be incorporated into
primate hippocampal function, Rolls and Xiang (2005) recorded
neuronal activity while macaques performed a rewardplace
association task in which each spatial scene shown on
a video monitor had one location which if touched yielded a
preferred fruit juice reward, and a second location which
yielded a less preferred juice reward. Each scene had different
locations for the different rewards. Of 312 hippocampal neurons
analysed, 18% responded more to the location of the
preferred reward in different scenes, and 5% to the location of
the less preferred reward (Rolls & Xiang, 2005). When the locations
of the preferred rewards in the scenes were reversed,
60% of 44 neurons tested reversed the location to which they
responded, showing that the reward-place associations could
be altered by new learning in a few trials. The majority (82%) of
these 44 hippocampal reward-place neurons tested did not
respond to object-reward associations in a visual discrimination
object-reward association task. Thus the primate hippocampus
contains a representation of the reward associations
of places “out there” being viewed, and this is a way in which
affective information can be stored as part of an episodic
memory, and how the current mood state may influence the
retrieval of episodic memories. There is consistent evidence
that rewards available in a spatial environment can influence
the responsiveness of rodent place neurons (Ho¨ lscher, Jacob,
& Mallot, 2003; Tabuchi, Mulder, & Wiener, 2003).
4.3.5. Grid cells in the entorhinal cortex
The entorhinal cortex contains grid cells, which have high
firing in the rat in a two-dimensional spatial grid as a rat
traverses an environment, with larger grid spacings in the
ventral entorhinal cortex (Fyhn, Molden, Witter, Moser, &
Moser, 2004; Hafting, Fyhn, Molden, Moser, & Moser, 2005).
This may be a system optimized for path integration
(McNaughton, Battaglia, Jensen, Moser, & Moser, 2006) which may self-organize during locomotion with longer time constants
producing more widely spaced grids in the ventral entorhinal
cortex (Kropff & Treves, 2008). How are the grid cell
representations, which would not be suitable for association
of an object or reward with a place to form an episodic
memory, transformed into a place representation that would
be appropriate for this type of episodic memory? We have
demonstrated how this could be implemented by a competitive
network (Rolls, 2008c) in the dentate gyrus which operates
to form place cells, implemented by each dentate granule cell
learning to respond to particular combinations of entorhinal
cortex cells firing, where each combination effectively specifies
a place, and this has been shown to be feasible computationally
(Rolls, Stringer, & Elliot, 2006).
In primates, there is now evidence that there is a grid cell
like representation in the entorhinal cortex, with neurons
having grid-like firing as the monkey moves the eyes across a
spatial scene (Killian, Jutras, & Buffalo, 2012). Similar
competitive learning processes may transform these entorhinal
cortex ‘spatial view grid cells’ into hippocampal spatial
view cells, and may help with the idiothetic (produced in this
case by movements of the eyes) update of spatial view cells
(Robertson et al., 1998). The presence of spatial view grid cells
in the entorhinal cortex of primates (Killian et al., 2012) is of
course predicted from the presence of spatial view cells in the
primate CA3 and CA1 regions (Georges-Franc¸ois et al., 1999;
Robertson et al., 1998; Rolls, 2008c; Rolls, Robertson, et al.,
1997; Rolls, Treves, et al., 1998; Rolls & Xiang, 2006). Further
support of this type of representation of space being viewed
‘out there’ rather than where one is located as for rat place
cells is that neurons in the human entorhinal cortex with
spatial view grid-like properties have now been described
(Jacobs et al., 2013).
4.3.6. Neuronal representations of space ‘out there’ for
episodic memory, and parietal inputs to the hippocampus
These discoveries showthat theprimate hippocampus contains
neurons that represent space ‘out there’ being viewed, and
reflect during learning associations of these viewed locations
with objects and with rewards. This is fundamental to human
episodicmemory.Humans canin one trial rememberwhere ina
roomor locality theyhave seenanobject or reward, eventhough
they may never have visited and been at the location. This
functionality could not be implemented by rat place cells,which
respond to the location where the rat is located.
The representations of space ‘out there’ which are very
important in primate hippocampal function show how
important the primate parietal cortical areas and the posterior
cingulate cortex are in providing inputs to the hippocampus,
for the parietal cortex receives inputs from the dorsal (‘where’)
visual system, and in areas such as the retrosplenial cortex,
contains representations of landmarks and spatial scenes
(Auger & Maguire, 2013). These areas connect to the hippocampus
via parahippocampal areas such as TF and TH (Yukie
& Shibata, 2009), in which, consistently, spatial view neurons
are found (Georges-Franc¸ois et al., 1999; Robertson et al., 1998;
Rolls, 2008c; Rolls, Robertson, et al., 1997; Rolls, Treves, et al.,
1998; Rolls & Xiang, 2006).
Macaque hippocampal neurons have little response to
objects or faces or rewards such as food or punishers such as
saline (Rolls & Xiang, 2006), even when the monkey is performing
an object-reward task (Rolls & Xiang, 2005). Thus the
hippocampus does not appear to be involved in emotion. It is
only when rewards must be associated with their spatial
context that primate hippocampal neurons become involved
(Rolls & Xiang, 2005). The hippocampus is thus seen as a
limbic structure in which space is very important as a typical
component of episodic memory, and as a structure in which is
a reward or emotional state as part of the episodic/single
event memory, can then be stored as part of the episodic
memory, and later recalled when the episodic memory is
recalled. Episodic memory must be able to incorporate
emotional states and rewards, and the way that this occurs in
terms of connectivity, neuronal responses, and learning in
hippocampal networks is proposed clearly in this paper.
4.4. Neural network computations in the hippocampus
for episodic memory
A computational theory of how the hippocampus implements
episodic memory has been developed in stages and described
elsewhere (Rolls, 1987, 1989a, 1989b, 1989c, 1990a, 1990b, 1991,
1995b, 1996b, 2007a, 2008c, 2010b, 2013b, 2013c; Rolls & Deco,
2010; Rolls & Kesner, 2006; Rolls & Treves, 1998; Rolls,
Tromans, & Stringer, 2008; Treves & Rolls, 1991, 1992, 1994).
Here I wish to show that hippocampal operation is computationally
very different from that involved in emotion,
providing further support for the thesis that there is no single
limbic system. There are at least two separate systems, performing
very distinct types of computation.
In outline, the theory describes quantitatively how the
hippocampal system illustrated in Fig. 7 operates to implement
episodic memory, and the later recall of a whole episodic
memory from any part. The CA3 recurrent collateral system
operates as a single attractor or autoassociation memory
network to enable rapid, one-trial, associations between any
spatial location (place in rodents, or spatial view in primates)
and an object or reward, and to provide for completion of the
whole memory during recall from any part. The theory is
extended to associations between time and object or reward
to implement temporal order memory, also important in
episodic memory. The dentate gyrus performs pattern separation
by competitive learning to produce sparse representations,
producing for example neurons with place-like fields
from entorhinal cortex grid cells. The dentate granule cells
produce by the very small number of mossy fibre connections
to CA3 a randomizing pattern separation effect important
during learning but not recall that separates out the patterns
represented by CA3 firing to be very different from each other,
which is optimal for an unstructured episodic memory system
in which each memory must be kept distinct from other
memories. The direct perforant path input to CA3 is quantitatively
appropriate to provide the cue for recall in CA3, but
not for learning. The CA1 recodes information from CA3 to set
up associatively learned backprojections to neocortex to allow
subsequent retrieval of information to neocortex, providing a
quantitative account of the large number of hippocamponeocortical
and neocorticaleneocortical backprojections.
The theory shows how object information could be recalled
in the inferior temporal visual cortex, spatial information in parietal allocentric spatial areas such as the retrosplenial
cortex, and reward and emotional information in areas that
receive as well as send inputs to the hippocampus, the orbitofrontal
cortex, amygdala, and ACC. Fundamental in this
computation is the autoassociation in the CA3 network, for
this enables different components of an episodic memory
(typically one being spatial, and others including as the case
may be object, face, and reward/emotional value information)
to be associated together, and then for the whole of the
memory to be recalled in CA3 from any one of the components.
[The operation of autoassociation or attractor networks
is described elsewhere (Hertz, Krogh, & Palmer, 1991; Hopfield,
1982; Rolls, 2008c; Rolls & Treves, 1998).]. After completion in
the CA3 autoassociation network, the backprojections to the
neocortex allow for recall of the neuronal activity in each
cortical area that was active during the original storage of the
episodic memory (Rolls, 2008c; Treves & Rolls, 1994).
This functionality, of storing a multicomponent memory
rapidly in an unstructured way, recalling the whole memory
from any part, and then recalling the activity that was present
in each high-order cortical area providing inputs to the hippocampus
(Rolls, 2008c, 2010b, 2013b) (see Fig. 7), is completely
different from the operations of other limbic structures
involved in emotion (the “emotion limbic system”), as
described next. The difference also helps to account for the
different connectivities of the two systems, with the emotion
system being primarily feedforward (Fig. 1), while the hippocampal
limbic system provides for feedback, both for recall
within the CA3 autoassociation network itself, and from the
hippocampus back to high-order cortical areas (Fig. 7).
5. Different computations for the ‘emotional
limbic system’ from those in the ‘memory limbic
system’
The circuitry involved in emotion in primates is shown
schematically in Fig. 1.
It is crucial that the representation of primary (unlearned)
reinforcers becomes explicit, i.e., in terms of reward value, in
the orbitofrontal cortex. In the case of taste, my hypothesis is
that genetic encoding of pathways is used to implement this
(Rolls, 2014), with molecular specification of the synapses in
the pathways from the genetically encoded taste receptors
(Chandrashekar, Hoon, Ryba, & Zuker, 2006; Chaudhari &
Roper, 2010), continuing through to the orbitofrontal cortex,
so that sweet in a receptor is encoded as the identity of sweet
taste in the insular primary taste cortex, and as the reward
value of a sweet taste in the orbitofrontal cortex.
The main type of learning required in the emotion system
is then pattern association between a previously neutral
stimulus such as a round object, and a primary reinforcer such
as taste, pleasant touch, or pain. The architecture and operation
of a pattern associator are shown in Fig. 8, with details
provided elsewhere (Rolls, 2008c, 2014; Rolls & Treves, 1998).
This is a feedforward operation. The conditioned stimulus,
e.g., the visual stimulus, becomes associated with the US (e.g.,
the taste). However, the opposite can not occur, the visual
stimulus can not be retrieved from the taste. This is a feedforward
operation. It is completely different from the
completion of a memory enabled by the autoassociation
network in the hippocampus (Rolls, 2013b). The pattern association
operation enables expected value to be computed
from outcome value (Rolls, 2008c, 2014).
In computing emotion-related value representations, inputs
about the properties of primary reinforcers and objects
are required, not inputs about space. That is why the inputs to
the emotional system are from sensors such as those involved
in taste, touch and pain, and are from the ventral visual system
where objects that can have these properties are represented.
This dominance of the ventral (‘what’) visual system
as providing the inputs for emotion and value decoding is
strongly to be contrasted with the episodic memory system, in
which where, and when, events happened is important, with
input therefore required from the parietal spatial/dorsal visual
systems. The orbitofrontal cortex is in a sense one end of
the ‘what’ processing systems, and does not know about
space or actions (Rolls, 2014). As we have seen the amygdala is
in primates to some extent a secondary player to the orbitofrontal
cortex, though involved in at least some classically
conditioned responses to stimuli that produce emotions.
The orbitofrontal cortex (but not the amygdala) goes even
beyond though the pattern associator computation illustrated
in Fig. 8, and can involve a one-trial, rule-based, change of
reward-related behaviour. For example, if one visual stimulus
suddenly becomes no longer associated with reward, the
orbitofrontal cortex system can immediately switch to
another visual stimulus, even though that stimulus has previously
been associated with punishment (Thorpe et al., 1983).
An integrate-and-fire neuronal network mechanism that utilises
the positive reward prediction error neurons found in the
orbitofrontal cortex has been described and simulated (Deco &
Rolls, 2005c). An attractor network keeps the current rule
active, and biases appropriately the conditional reward neurons
found in the orbitofrontal cortex (Rolls, 2014). The positive
reward prediction error neurons, themselves showing
continuous firing in an attractor, then quench the rule
attractor, so that another rule attractor population of neurons
can emerge because it is less adapted than the recently firing
rule neurons. The whole network operates well, and simulates
and provides an account for several types of neuron found in
the orbitofrontal cortex (Deco & Rolls, 2005c; Rolls, 2014).
The value representations computed in the orbitofrontal
cortex are then transmitted to the ACC so that with outcomes
represented now in the ACC, and with connections moving
back towards the midcingulate cortex, actioneoutcome
learning can be implemented. For actioneoutcome learning, a
representation of an action that has just been made is
required, and this is present in midcingulate cortex areas. The
action must be remembered, perhaps for several seconds,
until the outcome is received. The memory of the action, I
hypothesize, is provided for by an attractor network in the
mid/ACC. Associations can then be learned by pattern association
learning in the mid/ACC between the actions and the
outcomes. Often the whole process may be facilitated by the
learning of associations between the conditioned stimulus,
the expected value representation, and the actioneoutcome
pairing represented in the mid/ACC. The special role in this
scenario for the anterior and midcingulate cortex is then to
bring actions into the processing, and the costs of performing actions, so that appropriate instrumental, goal-directed, actions
can be performed to emotion-provoking stimuli represented
in terms of reward value (Rolls, 2014).
6. Conclusions: separate limbic structures or
systems for emotion and for memory, but no
single limbic system
The concept of a (single) limbic system has been shown to be
outmoded, in that anterior limbic and related structures
involved in emotion can operate independently, and by
different computational principles, from the hippocampal
memory system. Instead, the anatomical, neurophysiological,
functional neuroimaging, and neuropsychological evidence
described shows that anterior limbic and related structures
including the orbitofrontal cortex and amygdala are involved
in emotion, reward valuation, and reward-related decisionmaking
(but not memory), with the value representations
transmitted to the ACC for actioneoutcome learning. In this
‘emotion limbic system’ feedforward pattern association networks
learn associations between visual, olfactory and
auditory stimuli with primary reinforcers such as taste, touch,
and pain. In primates including humans this learning can be
very rapid and rule-based, with the orbitofrontal cortex overshadowing
the amygdala in this learning important for social
and emotional behaviour. The cortical inputs to this limbic
system come in primates from areas that represent ‘what’
object is present, including inferior temporal cortical areas
towards the end of the ventral visual system, and the anterior
insular taste cortex and somatosensory cortical areas.
The complementary anatomical, neurophysiological,
functional neuroimaging, and neuropsychological evidence
described shows that the hippocampus and limbic structures
to which it is connected including the posterior cingulate
cortex and the fornix-mammillary body-anterior thalamusposterior
cingulate circuit are involved in episodic or event
memory, but not emotion. This ‘hippocampal system’ receives
information from neocortical areas about spatial location
including parietal cortex areas towards the end of the
dorsal visual system, and about objects and faces from the
temporal cortical visual areas towards the end of the ventral
visual system, and can associate this information together by
autoassociation in the CA3 region of the hippocampus which involves feedback in the recurrent collateral system. The
hippocampal memory system can later recall the whole of this
information in the CA3 region from any component, a feedback
process, and can recall that information back to
neocortical areas, again a feedback (to neocortex) recall process.
The nature of the computation in this hippocampal
system is thus very different from the feedforward pattern
association involved in stimulus-reward association learning
in the emotion system (Rolls, 2013b). Emotion and reward
signals can enter the hippocampal memory system from the
orbitofrontal cortex, amygdala and ACC (Rolls, 2014; Rolls &
Xiang, 2005), and can be recalled back to the orbitofrontal
cortex, amygdala and ACC during memory recall (Smith,
Stephan, Rugg, & Dolan, 2006), as emotion can provide a
component of episodic memory.
Thus, the emotional and hippocampal networks or ‘limbic
systems’ operate by different principles, and operate independently
of each other except insofar as emotional state may
be part of an episodic memory. The concept of a single limbic
system may no longer appropriately reflect how the brain
operates. Instead, separate systems, which might be designated
the emotional limbic system and the episodic memory
hippocampal system, are present in the brain, and the separate
and different principles of their operation have been described
here and elsewhere (Rolls, 2008c, 2010b, 2013b, 2014).

Posner molecules: from atomic structure to nuclear spins
Michael W. Swift, a Chris G. Van de Walle *b and Matthew P. A. Fishera
We investigate ‘‘Posner molecules’’, calcium phosphate clusters with chemical formula Ca9(PO4)6. Originally identified in hydroxyapatite, Posner molecules have also been observed as free-floating molecules in vitro. The formation and aggregation of Posner molecules have important implications for bone growth, and may also play a role in other biological processes such as the modulation of calcium and phosphate ion concentrations within the mitochondrial matrix. In this work, we use a first-principles computational methodology to study the structure of Posner molecules, their vibrational spectra, their interactions with other cations, and the process of pairwise bonding. Additionally, we show that the Posner molecule provides an ideal environment for the six constituent 31P nuclear spins to obtain very long spin coherence times. In vitro, the spins could provide a platform for liquid-state nuclear magnetic resonance quantum computation. In vivo, the spins may have medical imaging applications. The spins have also been suggested as ‘‘neural qubits’’ in a proposed mechanism for quantum processing in the brain.
I Introduction
In1975BettsandPosner,whileexaminingtheX-raycrystalstructure of bone mineral—hydroxyapatite, Ca10(PO4)6(OH)2—noticed that within each unit cell there were two calcium–phosphate ‘‘structural clusters’’ with atomic constituents Ca9(PO4)6.1 It was subsequently argued that these so-called ‘‘Posner clusters’’ played an important role in the formation of amorphous calcium phosphate, which can be viewed as a ‘‘glass’’ of Posner clusters. It was over 20 years later that Onuma and Ito, while performing intensity-enhanced dynamic light scattering on simulated body fluids, identified evidence for free-floating calcium phosphate clusters of size roughly 1 nm.2 They suggested that these clusters, apparently stable in solution for months or longer were, in fact, Posner clusters. No spontaneous precipitation wasobservedeveninasupersaturatedsolution,butitwassuggested that these Posner clusters play a role in bone (hydroxyapatite) nucleation when presented with a seed crystal.3,4 Additional evidence was provided in 2010 when cryogenic transmission electronmicroscopyexperimentsonbonenucleationinsimulated body fluids imaged free-floating nanometer-sized molecules which coalesced near a seed, forming amorphous calcium phosphate before undergoing a dramatic transition into the crystalline form of hydroxyapatite.5 Further evidence for the structural integrity of these clusters in solution was found in AFM images
of bone growth on calcite surfaces.6 Taken together, these remarkable experiments provide evidence that Posner clusters are stable in solution and, as such, should perhaps be called ‘‘Posner molecules’’—the name we will henceforth adopt. Soon after the Onuma andIto experiments, several quantum chemistry calculations examined the putative arrangement of the ions in these Posner molecules, Ca9(PO4)6, which were indeed found to be stable in vacuum.7,8 Their basic form consisted of eight calcium ions on the corners of a cube with the ninth calcium in the center, while the six phosphate ions were situated on the six faces of the cube. Due to the symmetry incompatibility of the tetrahedral phosphate ions (PO43) with the cube faces, the cubic symmetry was broken in the low-energy molecular configurations. One of the most stable configurations was found to have S6 symmetry, with a threefold rotational symmetry axis that is aligned along one of the cube diagonals, as depicted in Fig. 1. A more detailed exploration of Posner molecules is worthwhile for multiple reasons: 1. Role in biological processes. Since experiments have provided strong evidence that Posner molecules are stable in simulated body fluids, it is likely that they are present in vivo, particularly in the extracellular fluid where the free calcium concentration is appreciable. These extracelluar Posner molecules could act as a reservoir for the previously suggested Posnermolecule-mediated mechanism for bone growth.3,4 The molecules may also be present within cells. Though cytoplasm is unlikely to contain Posner molecules due to its low calcium concentration, the mitochondiral matrix is known to contain stable calcium– phosphate complexes with a calcium:phosphate ratio suggestively close to that of Posner molecules (3:2).9 Posner molecules may form a dynamic aggregate within the mitochondiral matrix, with formation, aggregation, and dis-aggregation being finely modulated by the changing pH.10 The Posner molecule is also central to the proposed ‘‘quantum brain’’ concept set forth in ref. 11. In this proposal, clouds of entangled Posner molecules in the brain act as ‘‘neural qubits’’, serving as a platform for quantum computation in cognitive processes. In ref. 12, Halpern and Crosson give a thorough discussion of quantum information processing in the Posner molecule. Many of the properties we discuss here are key to the ongoing exploration of this concept. 2. Long-lived phosphorus nuclear spin states. Molecules or ions with isolated nuclear spins that exhibit macroscopic coherence times are of great physical interest, both theoretically and practically. Nuclear spins in liquid-state nuclear magnetic resonance (NMR) have some of the longest known coherence times of any system in physics, especially nuclei with spin 1/2 that do not couple to electric fields and interact with the environment only through magnetic fields (e.g., dipole fields from other nuclear spins) and intra-molecular electronmediated exchange interactions.13 Motional narrowing due to the rapid rotation of the molecules in solution averages out dipole–dipole magnetic field interactions, giving spin-1/2 nuclei long coherence times.14 Since the most abundant isotopes of both calcium and oxygen have zero nuclear spin, we predict that the six 31P nuclear spins with S = 1/2 in a Posner molecule are especially long lived, making them an intriguing platform for liquid-state NMR quantum computation.15 Ifthe nuclearspinscan behyperpolarized, the spins could have medical imaging applications,
since the 31P NMR signal is quite robust. The Posner molecule’s nuclear spins are also central to the quantum brain concept,11,12 in which long coherence times are key to their role as neural qubits. 3. Doping with other cations. We shall present evidence that replacing the calcium ion at the center of a Posner molecule with either another divalent cation or a pair of monovalent cations can further stabilize the Posner molecule. This provides apossible mechanismforthe knownimpactofelementssuchas lithium, magnesium, and iron on bone health.16–18 Impurities are also relevant to the quantum brain hypothesis,11,12 which suggests that the cognitive effects of lithium (and the isotope dependence of these effects) may arise from the interaction of the lithium nuclear spin with the neural qubits provided by the 31P nuclear spins. 4.Aggregation of Posnermolecules.The presence of two Posner clusters within the unit cell of hydroxyapatite suggests the possible importance of aggregation of Posner molecules in bone growth. Ourquantumchemistrycalculationsprovideevidencethatapairof Posner molecules can chemically bind together (in vacuum) with a substantial binding energy of roughly 5 eV. This pairwise bonding is the first step towards larger-scale aggregation, which may lead to the formation of amorphous calcium phosphate and eventually hydroxyapatite.3–6 5. Quantum dynamics of the six phosphorus nuclear spins in a Posner molecule. Provided the Posner molecules do have an S6 symmetry, the 26 = 64 nuclear spin eigenstates in each molecule can be chosen also as eigenstates under a 3-fold rotation about the symmetry axis, with eigenvalues of the form ei2pt/3 with the ‘‘pseudospin’’ t taking one of 3 allowed values, t = 0,1. Understanding the dynamics of these spins is important for any application of Posner molecules to medical imaging or quantum computation as discussed above. The spin dynamics are also key to the quantum brain concept.11,12 Our paper isorganized asfollows. Afterabrief description of the computational methods (Section II), we treat the structural properties of the Posner molecule: symmetry (Section IIIA), vibrational spectra (Section IIIB), impurity incorporation (Section IIIC), and pairwise bonding (Section IIID), discussing the results and implications of each in turn. We then move on to the spin properties, using first-principles methods to calculate the indirect spin–spin coupling between 31P nuclear spins in a Posner molecule.Thesevalues becomeinput toan effectiveHamiltonian for the spins. We study this effective model, paying special attention to the implications for coherence of encoded quantum information (Section IIIE).
II Computational methods
Ourfirst-principlescalculationsarebasedondensityfunctional theory (DFT). Molecular symmetry, impurity incorporation, and pairwise bonding were studied with the Projector-Augmented Wave (PAW) method19 as implemented in the Vienna Ab initio Simulation Package (VASP).20 The hybrid exchange–correlation functional of Heyd, Scuseria, and Ernzerhof21 was employed with the standard 25% mixing and screening parameter o = 0.20 Å1 (a combination of parameters commonly referred to as HSE06).22 The use of a plane-wave basis set avoids complications due to localized basis sets (such as the Basis Set Superposition Error), and allows systematic improvement through the plane-wave energy cutoff. We use a cutoff of 400 eV; tests at 450 eV found thatenergydifferencesbetweenstructuresareconvergedtowithin 4 meV. Single Posner molecule calculations were performed in a vacuum supercell 16 Å on a side. These cells were verified to be sufficiently large to avoid interactions between periodic images; tests with cell side-length of 20 Å found that energy differences are converged to within 10 meV. Calculations exploring the chemical bonding between two Posner molecules used a 16 Å  16 Å  32 Å cell. The vibrational spectrum of was calculated via density functional perturbation theory (DFPT) with the Quantum Espresso package23 using ultrasoft pseudopotentials24 and the Perdew–Burke–Ernzerhof exchange–correlation functional.25 The acoustic sum rule was applied for the diagonalization of the dynamical matrix in order to account for the molecule’s translational and rotational degrees of freedom. Calculations of the pairwise nuclear spin–spin interactions between 31P nuclear spins inside a Posner molecule were performed in Dalton201526,27 with the B3LYP hybrid functional28 using the method of ref. 29 and 30. The electronic states around oxygen and calcium were expanded in the popular 6-31G** basis,31,32 while those around phosphorus were expanded in the pcJ-4 basis, which is optimized for the calculation of indirect spin–spin couplings.33 Molecular visualizations were produced using VESTA.34
III Results and discussion A. Symmetry Previous computational work7,8 used DFT to study the possible symmetries of a Posner molecule. The authors found various candidates for the lowest-energy molecular structure, all within the numerical accuracy of the technique. They concluded that the S6 structure, the candidate ground state with the highest symmetry, is the prototypical Posner molecule. Our calculations agree with these results. Recent theoretical results using an implicit solvation method suggest lowersymmetry configurations with high dipole moments may be stabilized by interactions with the solvent,35 but these interactions are not considered here. Several structures relaxed in vacuum are illustrated in Fig. 2. The lowest-energy structures have C1 (no) symmetry but are only slightly distorted from S6 symmetry. The S6 structure is higher in energy by only 0.06 eV, or less than 2 meV per atom, while the Th structure is a full 1.98 eV higher in energy than the S6. This is consistent with earlier work, which identified S6 as the prototypical structure.8 To further test this identification, five C1 structures were generated by random 0.1 Å perturbations of an S6 structure. After relaxation, the energies of these structures had a spread
of only 15 meV, suggesting the existence of many nearlydegenerate C1 configurations. Atomic positions of these structures are found to deviate from S6 symmetry by at most 0.008 Å, while the average positions across all five structures deviate from S6 symmetry by at most 0.002 Å. This demonstrates that the S6 structure is indeed correct on average, and we expect thermal fluctuations will wash out any symmetry breaking. We therefore assume the S6 symmetry for the Posner molecule in the remainder of this work.
B. Vibrational spectra Experiments find evidence for calcium phosphate clusters roughly 1 nm in size in simulated body fluid.6,36 While it is suspected that these clusters are Posner molecules,3 this has not been shown conclusively. Spectroscopic probes such as infrared absorption spectroscopy, Raman scattering, or wideangle X-ray scattering could identify the molecules definitively. IR spectroscopy is based on the absorption of incoming photons resonant with a dipole-inducing vibrational mode of a molecule.37 A calculation of vibrational modes and their associated dipole moments indicates the wavelengths of light that could be absorbed by the molecule. The IR activity of the associated vibrational mode, which is proportional to the square of the induced dipole moment, corresponds to the strength of the peak in an absorption experiment. Practically speaking, IR spectroscopy in solution is challenging, since the H2O peak at 1575 cm1 with an IR activity of 1.659 (D Å1)2 amu1 38tends to wash out the IR absorption signal of any species in aqueous solution. Nevertheless, spectroscopic methods remain one of the best ways to conclusively identify Posner molecules, and calculations of vibrational spectra are an essential step in this process.35 In addition to their spectroscopic relevance, the vibrational modes of the molecule may also couple to the interactions between the six phosphorus nuclear spins. These interactions will be discussed in Section IIIE. The vibrational spectra of both the S6 structure and a symmetry-broken C1 structure are shown in Fig. 3. Note that the S6 symmetry guarantees that a number of the modes will induce no dipole moment, and thus have zero IR activity. The corresponding vibrational modes of the C1 structure are shifted slightly and have a small IR activity, but the overall shape of the spectrum remains the same. In addition to the plotted modes, we also found modes with small imaginary frequencies. The S6 structure has two such modes, andthe C1 has one. The imaginary frequencyinthe C1 structureisnotanindicationofanincomplete relaxation; it persists even after full relaxation of the atomic positions (forces are less than 103 Ry Bohr1). Indeed, this imaginary frequency is to be expected given the existence of many diﬀerent C1 states (see Section III A); we identify it as a soft vibrational mode moving between diﬀerent low-energy states.
C. Impurity substitution During theformation ofPosnermolecules, ionsotherthanCa2+ and PO43 will typically be present in solution, and thus could
be substituted for one of the native ions. We refer to these ionic substitutions as ‘‘impurities’’. Here we consider the energetics of substituting the central calcium ion with either another divalent cation or with two monovalent cations. In making any comparisons in energies, it is important to take into account the hydration energies of both the ions to be substituted and of the central calcium ion once removed from the molecule. In principle the hydration energy of a Posner molecule in solution also needs to be taken into account. However, it is reasonable to assume that any changes inthishydration energyupon impurity substitution of the central calcium ion (which is encased inside themolecule)willbesmall,andhencetheyareignoredhere.The enthalpy of hydration for a single ion, DHhyd, is defined as the energy change upon taking an ion from a gaseous state to a dilute solution in water. It is always negative since polar water molecules can considerably reduce their energy by rearranging around the ionic point charge.39 It is notoriously diﬃcult to calculate hydration enthalpies from first principles;40,41 in this work we take these values from experiment.42 We will denote the enthalpy change due to the substitution of the central calcium ion with either a single divalent cation or of two monovalent cations as DHA, where A specifies the cation substituted. DHA can be expressed as DHA ¼ DH f AnCa8 PO4ðÞ 6  þDHhyd Ca2þ   DH f Ca9 PO4ðÞ 6  þnDHhyd Að3nÞþhi  (1) where n = 1 corresponds to a single substituted divalent cation, and n = 2 corresponds to a pair of substituted monovalent cations. Here DH f is the enthalpy of formation of the Posner molecule with or without the impurity. Our results for several selected impurities are presented in Table 1. We find a significant diﬀerence in the favorability of various ionic substitutions. This diﬀerence is large enough to outweigh the hydration enthalpy of cations such as Li+ and Fe2+, making them highly favored as impurities. Indeed, the trend is that ions with a stronger tendency to hydrate have an even stronger tendency to substitute for Ca2+ in the Posner molecule. Increased hydration enthalpy is outweighed by increased stability on the central site of the Posner molecule. These results suggest that if significant concentrations of lithium, iron, or magnesium are present when Posner molecules are formed, they are likely to incorporate into the Posner molecule structure. This could have a variety of implications. In the context of calcium phosphate biomineralization, the presence of impurity ions and the nature of their interactions with Posner molecules will have important impacts on Posnermolecule-mediated bone growth. Additionally, spinful nuclei incorporated as impurities within the Posner molecule will have a significant eﬀect on the phosphorus spin states.
D. Bonding Aggregation of Posner molecules has been proposed as an intermediate step in biomineralization of amorphous calcium phosphate, a precursor to hydroxyapatite (bone mineral). We approach this by studying the simplest form of aggregation: pairwise bonding. We consider bonding of Posner molecules with the S6 structure described in Section IIIA. A variety of bonding orientations for a pair of rigid molecules were tried; the most favorable is depicted in Fig. 4(a) and is similar to the relative ionic positions in hydroxyapatite. The molecules are mirror images of one another, oriented so that Ca2+ ions meet PO43 ions, and ions of like charges remain separated. The distance between centers and the relative orientation of the molecules were varied to find the minimum energyconfiguration.The resultingconfiguration, depicted in Fig. 4(a), has a binding energy of 1.04 eV (referenced to isolated molecules in vacuum). Starting from this configuration, the constituent ionswereallowedtorelax. Thisrelaxation gains another 3.95 eV, for a total bonding energy of 4.99 eV. The relaxed bonded configuration is shown in Fig. 4(b). We note that these calculations do not take the presence of solvent into account. We expect that this is a reasonable approximation when Posner molecules are close enough to bond, since there is not enough space for solvent molecules to enter between themoleculesandscreentheionicinteraction.Atlargerseparation, thesolventwilllikelyreducethebondingtendency,suppressingthe long-distance tail of the attraction. We have also explored the motion of two rigid molecules in a bonded pair with respect to one another. Specifically, we consider ‘‘rollingwithoutslipping’’rotation, i.e.,rotationbybothmolecules simultaneously in opposite directions, such that the mirror symmetryoftheconfigurationismaintained.Theenergylandscapefor thisrotationismappedoutbyrepeatingthedistance-optimization procedure for a set of rotation angles, finding the optimum distance for each orientation. The saddle-point configuration (shown in Fig. 4(c)) is at a rotation angle of f = 45 1, and the rotation barrier is 0.33 eV. We expect the rotation barrier for rigid molecules is a reasonable approximation in the early stages of the bonding process (before full relaxation).
E. Spin interactions 1. Phosphorus nuclear spin coupling. Coupling between phosphorus nuclear spins arises due to two factors: magnetic
dipole–dipole interaction and ‘‘indirect’’ spin–spin coupling.13 The rotational motion of the molecule tends to average out the dipole–dipole interaction and the anisotropic part of the indirect coupling, so we only consider the isotropic part of the indirect spin–spin coupling. This coupling between nuclei i and j is a sum of four terms:
which represent the diamagnetic spin–orbit (DSO), paramagnetic spin–orbit (PSO), spin-dipole (SD), and Fermi contact (FC) terms.13 Details of the calculation of nuclear spin–spin couplings were given in Section II.
The nuclear spins of phosphorus are arranged in two equilateral triangles, one on top of the other, centered on the molecule’s symmetry axis. The S6 symmetry restricts the couplings Jij to three values: nearest-neighbor J1, second-nearest-neighbor J2, and third-nearest-neighbor J3, as shown in Fig. 5. We find J1 = 0.178 Hz, J2 = 0.145 Hz, and J3 = 0.003 Hz. The threefold rotational symmetry of the Posner molecule ensures that the eﬀective Hamiltonian shares this same symmetry. Eigenstates of this rotation have an eigenvalue of ei2pt/3, where t can take values of 0,1. We call this quantum number t the ‘‘pseudospin’’. Eigenstates may be expressed using the notation  Pseudospin and rotations. The transformation properties of the nuclear spin states of a molecule under a symmetry transformation dictate the allowed values of the rotational angular momentum quantum number L. This eﬀect is most dramatic and well-studied in molecular hydrogen (H2). Parahydrogen, inwhich theprotonsform a spinsinglet,is restricted to even values of L by the requirement that the wavefunction be antisymmetric under exchange of the protons. Orthohydrogen
(with a proton spin triplet) is similarly restricted to odd values of L. These spin isomers have diﬀerent thermodynamic, scattering, and chemical properties.43 Likewise, for a Posner molecule with threefold rotational symmetry, the Fermi statistics of the 31P nuclei dictate the allowed rotational angular momentum about the symmetry axis. With three-fold rotational symmetry, the full wavefunction must be unchanged by a rotation by 2p/3, since such a rotation is equivalent to an even number of fermion swapping operations. The pseudospin t therefore constrains the angular momentum, L, to satisfy: L + t = 0 mod 3. We propose that this restriction may be important in the case of two Posner molecules (a, b) binding together. Indeed, the recently proposed Quantum Dynamical Selection rule,44 when generalized to the binding of two Posner molecules, predicts that chemical bonding implements a projective measurement onto a state with ta + tb = 0—essentially a result of the requirement that binding two Posner molecules stops any relative rotational motion. If the pair of Posner molecules subsequently unbinds, this constraint is predicted to be maintained, leaving the two molecules ‘‘pseudospin entangled’’. Thus, pair binding/unbinding of Posner molecules may provide a mechanism to quantum entangle nuclear spin states in multiple Posner molecules, a necessary precondition for the quantum brain concept.11 3. Decoherence. A decoherence time for the spins in a Posner molecule is the NMR spin–lattice relaxation time T1. The primary mechanism for decoherence is entanglement with external nuclear spins of protons in water molecules external to the Posner molecule. The T1 due to dipole–dipole interactions between an external spin M (e.g. a proton) and the phosphorus
As an illustration giving a rough estimate of this coherence time, we consider a proton (perhaps associated with a water molecule in the solvent) as the external spin, located 7 Å from the center of the Posner molecule along the symmetry axis (3.5Åfromthe apical Ca2+).Thisscenario gives T1 = 1.8106 s= 21 days. Since diﬀerent pseudospin sectors couple diﬀerently to environmental degrees of freedom, the decoherence times for the pseudospin quantum number may be even longer. The long-lived spin states in the Posner molecule could provide a platform for liquid-state NMR quantum computation, and are also key to the ‘‘quantum brain’’ concept set forth in ref. 11.
IV Conclusions
We have explored the structure, symmetry, and spectroscopic fingerprint of the Posner molecule, Ca9(PO4)6. We have shown that Posner molecules are stable in vacuum, and identified S6 symmetry as the prototypical symmetry. The calculated vibrational spectrum of the Posner molecule may serve as a spectroscopic fingerprint, assisting with the experimental identification of the Posner molecule either in vitro or in vivo. Impurity cations can take the place of a central calcium, with implications for both phosphorus spin properties and bone growth. We find that pairwise Posner molecule bonding is an important process, suggesting avenues for research in bone growth. Finally, we have shown that the Posner molecule is a promising host for nuclear spins maximally protected from environmental decoherence, with possible implications in liquid-state NMR quantum computation and medical imaging. We have identified the pseudospin quantum number t which could encode long-lived coherent quantum information in the Posner molecule and may provide a mechanism for entangling the molecule’s rotational degrees of freedom with its nuclear spin. This mechanism is central to the Posner molecule’s role as a neural qubit in the quantum brain concept.
Conflicts of interest
There are no conflicts of interest to declare.
Acknowledgements
We thank Bernard Kirtman, Daniel Ish, Jim Swift, and Leo Radzihovsky for fruitful discussions. M. P. A. F. is grateful to the Heising-Simons Foundation for support, to the National Science Foundation for support under Grant No. DMR-14-04230, and to the Caltech Institute of Quantum Information and Matter, an NSF Physics Frontiers Center with support of the Gordon and Betty Moore Foundation. Computational resources wereprovidedbytheExtremeScienceandEngineeringDiscovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1548562, and by the Center for Scientific Computing from the CNSI, MRL: an NSF MRSEC (DMR-1720256) and NSF CNS-0960316.


A New Spin on Neural Processing: Quantum Cognition
Carol P. Weingarten, P. Murali Doraiswamy, and Matthew P. A. Fisher
Although quantum mechanics is fundamental for understanding molecular mechanisms in physics and chemistry, it has usually been assumed to be unimportant for understanding molecular mechanisms of biological systems. However, there is increasing evidence that quantum mechanics is important for understanding some biological phenomena (Lambert et al., 2013), such as energy transfer in photosynthesis (Fassioli et al., 2014), navigation by birds using the earth’s magnetic ﬁeld (Hiscock et al., 2016), and electron and hydrogen tunneling in biochemical reactions (Klinman and Kohen, 2013). There have also been proposals that quantum mechanics may help explain aspects of brain function. Discussions about quantum mechanics and the brain began with questions on the role of measurement or observation in quantum mechanics (Stapp, 1991; Theise and Kafatos, 2013). Further developments began to highlight the possibility that quantum mechanics might help explain neural mechanisms involved in consciousness or synaptic function (Stapp, 1991; Beck and Eccles, 1992). Another topic that emerged was whether quantum mechanisms might be employed by the brain to perform calculations, i.e., the possibility of quantum computing in the brain (Penrose, 1989). For example, a model of consciousness was developed that involves quantum computations in neuronal microtubules (Tegmark, 2000; Penrose and Hameroﬀ, 2011; Hameroﬀ and Penrose, 2014a,b; Reimers et al., 2014; Craddock et al., 2015). Other proposals have focused on the quantum phenomenon of spin (see below). Hu and Wu (2004) suggested that nuclear spins of hydrogen, nitrogen, and phosphorus in neuronal cellular components and electron spins of diﬀusible oxygen and nitric oxide in the brain might mediate consciousness. Electron spins in the brain have also been suggested as a potential target of transcranial magnetic stimulation therapies (Chervyakov et al., 2015). Other perspectives have led to application of quantum probability theory to human decision making (Wang et al., 2014; Kvam et al., 2015). Finally, the above mentioned navigation by birds may involve a quantum mechanical cryptochrome radical-pair (spin dynamic) mechanism in neuronal retinal ganglion cells that transmit information to the brain (Mouritsen et al., 2004; Hiscock et al., 2016). Recently a new model for how the brain may store and process quantum information has been proposed (Fisher, 2015). The model includes speciﬁc biochemical components that could be employed for quantum processing in glutamatergic neurotransmission. It has potential relevance for molecular mechanisms underlying normal neural function, such as glutamatergic dependent neurocognitive systems, as well as psychiatric treatments such as lithium.
NUCLEAR SPINS AND QUANTUM PROCESSING/COMPUTING: NEURAL QUBITS
This model is based on a quantum phenomenon that underlies something already familiar to neuroscience—magnetic resonance imaging (MRI) (Atlas, 2009). MRI images are made by observing a quantum property of atoms called nuclear spin (Hore, 2011). The most abundant nuclear spin in the brain/body is that of the hydrogen nucleus (1H), or proton, that is found in water and numerous other molecules. Most MRI brain imaging is based on observations of proton nuclear spins. Another nuclear spin in the brain is that of phosphorus. Brain imaging of phosphorus nuclear spins has been conducted using magnetic resonance technologies such as magnetic resonance spectroscopy (MRS) and MRS imaging (MRSI). It is phosphorus nuclear spins that are the focus here. Classical computing is based on information in a binary digit or bit. Quantum processing or computing is based on quantum bits, or qubits, that enable much greater computing power than would be possible using a similar number of classical bits (Bennett and DiVincenzo, 2000; Nielsen and Chuang, 2010). The increase in computing power is the result of quantum phenomena such as superposition and entanglement (Horodecki et al., 2009). Entanglement plays a central role in this model for quantum processing in the brain and more will be said about this below. A variety of nuclear spins can be used as qubits (Vandersypen et al., 2001). Quantum computing with several nuclear spins residing on single molecules that are solvated in water has been realized although it has not been scalable (Nielsen and Chuang, 2010). In the model for quantum processing in the brain, the nuclear spin of phosphorus functions as a qubit, i.e., “neural qubit.”
QUANTUM ENTANGELED PHOSPHATES
Phosphorus is found in many biological substances including ATP, AMP, inorganic phosphates, bone, creatine, and phospholipids of cell and organelle membranes. The focus here is on inorganic phosphate HPO2− 4 and pyrophosphate P2O4− 7 (Figure 1). Pyrophosphate contains two phosphorus atoms. It is a well-known component of several intracellular and extracellular biochemical reactions, including adenylyl cyclase that converts ATP to the second-messenger cyclic-AMP; uridine diphosphate-glucose pyrophosphorylase; acyl-CoA synthetase; alkaline phosphatase, etc. (Lodish et al., 2000; Terkeltaub, 2001; Yepes et al., 2003). Pyrophosphate undergoes hydrolysis via enzymatic pyrophosphatases to produce two molecules of phosphate. The phosphate products have an interesting feature at the quantum level: their phosphorus nuclear spins will be predominantly quantum entangled in a very special singlet state (Fisher, 2015). When two spins are entangled in a singlet, a measurement of the state of one of the spins will dictate
FIGURE 1 | Proposal for quantum processing in the brain (Fisher, 2015). Enzymatic hydrolysis of extracellular pyrophosphate, in which phosphorus atoms can be in a quantum entangled singlet state (*P), results in quantum entangled phosphates. The entangled phosphates are incorporated into calcium phosphate Posner molecules producing quantum entangled Posner molecules (dashed line represents entanglement). Two pairs of Posner molecules can undergo binding reactions (squares) to form quantum entangled Posner dimers. Transport of entangled Posner molecules into glutamatergic neurons can be mediated by endocytosis into presynaptic vesicles and action of vesicular glutamate transporter (VGLUT). When entangled Posner molecules in different neurons undergo binding reactions and hydrolysis, this can lead to calcium mediated glutamate (diamond) release from presynaptic neurons and then non-local quantum correlations in postsynaptic ﬁring.
the result of a measurement on the state of the other spin. This eﬀect is independent of the distance between the two spins. Thus entanglement can lead to a distance independent (non-local) correlation between spins. Although entanglement may sound unusual—Einstein referred to it as “spooky action-at-a-distance”—it is a well observed phenomenon (Gottfried and Yan, 2003; Horodecki et al., 2009; Shalm et al., 2015).
POSNER MOLECULES AND QUANTUM PROCESSING IN GLUTAMATERGIC NEURONS
To be useful for neural quantum processing, the entangled phosphates must be transported into diﬀerent neurons where they can participate in biochemical reactions that are coupled to releaseofaneurotransmitter suchasglutamate.Thiscanresultin non-local,quantumcorrelatedpostynapticﬁringamongmultiple neurons (Figure 1). The intricate molecular environments of neural intracellular and extracellular spaces mean that important biochemical factors would be needed for quantum processing. A key component is a remarkable calcium phosphate molecule called a Posner molecule Ca9(PO4)6. It was identiﬁed as a structural “cluster” in a precursor phase for formation of bone mineral (hydroxyapatite) (Posner and Betts, 1975; Onuma and Ito, 1998; Oyane et al., 1999). Posner’s molecules have been observed in simulated body ﬂuids and were approximately spherical with a diameter around 0.87 nm (Dey et al., 2010; Wang et al., 2012). In the model for neural processing, quantum entangled phosphates are incorporated into Posner molecules, and then the quantum entangled Posner molecules are used for quantum memory storage and processing in glutamatergic neurons. Posner molecules are expected to have several interesting features useful for quantum processing. Firstly, since most naturally occurring isotopes of both calcium and oxygen have no nuclear spin and because of the rapid tumbling expected for the Posner molecules in solvent, the quantum entangled phosphorus nuclear spins are expected to be very protected, remaining coherent for times of a day, or possibly much longer. This would allow Posner molecules to function as a “qubit memory.” Second, chemical binding of two Posner molecules should be a nuclear spin dependent reaction, thereby inducing quantum entanglement between the binding of two separated Posner pairs—this functions as a “measurement” of spin states discussed above. Thirdly, once two bound Posner molecules start rotating about one another their nuclear spin states further entangle. And if/when this rotation stops a further nuclear spin “measurement” is implemented. Once at rest, Posner pairs are more susceptible to “proton attack” and can undergo hydrolysis and “melt” releasing Ca2+ into the cytoplasm. This could modulate calcium levels and, therefore, calcium mediated release of glutamate from presynaptic neurons. Finally, because Posner molecules contains six phosphorus atoms they could potentially mediate quantum entanglement and non-local quantum correlations in postsynaptic ﬁring across multiple neurons (Fisher, 2015). Note that an important step in quantum processing with glutamatergic neurons is the transport of quantum entangled Posnermoleculesintodiﬀerentneurons(Figure 1).Theproposal addressedthisandsuggestedthatitcouldoccurthroughvesicular glutamate transporter (VGLUT). As indicated by its name, VGLUT is the vesicular membrane protein that transports glutamate into presynaptic vesicles (Bellocchio et al., 2000; Takamori et al., 2000; Fremeau et al., 2004; Takamori, 2006). However, VGLUT has also been known as a brain sodium
(Na)- dependent inorganic phosphate (Pi) transporter (BNPI) (Werner et al., 1991; Ni et al., 1994, 1996; Bellocchio et al., 2000). In this proposal VGLUT has roles as both a glutamate and phosphate transporter. VGLUT’s role as a phosphate transporter is to mediate the movement of quantum entangled Posner’s molecules from extracellular space (where they are formed) to the cytoplasm of diﬀerent presynaptic neurons. This could occur via endocytosis, melting, and reformation of Posner’s molecules (Fisher, 2015). The result would be the presence of entangled Posner’s molecules in the cytoplasm of multiple presynaptic neurons, which could then lead to post-synaptic ﬁring that is quantum correlated across these neurons.
IMPLICATIONS FOR NEURAL PROCESSING AND NEUROPSYCHIATRIC TREATMENTS
Quantum correlations in postsynaptic ﬁring could be naturally involved in a variety of neural systems. These include systems involved in normal neurocognitive processing (thus the term quantum cognition). For example, glutamatergic neurons are neural components of diverse neurocognitive systems throughout the brain. Thus the proposed quantum processing could have a role in neural computation and information processing involved in many types of brain function. These could encompass a variety of normal or abnormal cognitive (including aﬀective and behavioral) functions. They could also be involved in mechanisms underlying treatments of neuropsychiatric disorders (Fisher, 2015). For example, magnetic ﬁelds can modulate nuclear spins—the basis for MRI—and an eﬀect on quantum processing might be a mechanism of transcranial magnetic stimulation treatments (Chervyakov et al., 2015). As another example, the mechanisms underlying lithium treatment of bipolar disorder remain obscure. Interestingly, lithium has two isotopes with nuclear spin (6Li, 7Li), and a remarkable experiment in 1986 found that the two isotopes had opposite eﬀects on the maternal behavior of rats—mothers fed 7Li, the predominant isotope in naturally occurring lithium, had suppressed behaviors with low alertness levels, while 6Li rats became very active with very high alertness (Sechzer et al., 1986). Quantum chemical calculations have shown that the Posner molecule is stabilized when two lithium atoms replace the central calcium atom (Fisher, 2015). This would alter the phosphorus nuclear spins and modify the quantum neural processing, oﬀering a possible mechanism for the action of lithium (and the diﬀerence between the two lithium isotopes). Perhaps neural quantum processing might also provide a biological architecture that could be “co-opted” and employed for development of laboratory (in vitro) quantum computing, similar to how studies of neural circuitry have been contributing todevelopmentofartiﬁcialintelligence.Forexample,asindicated by Fisher (2015), a laboratory procedure could be envisioned in which pyrophosphate would be enzymatically hydrolyzed in solution. The nuclear phosphorus spins of the released inorganic phosphates should be predominantly in a spin singlet state and
thus quantum entangled (e.g., Figure 1). In the presence of calcium these inorganic phosphates could then form Posner’s molecules. Some of these Posner’s molecules would be quantum entangled when they incorporated entangled phosphates. The entangled Posner’s molecules could then be used in applications of liquid-state nuclear magnetic resonance quantum computing methods (Vandersypen et al., 2001; Oliveira et al., 2007; Nielsen and Chuang, 2010).
CONCLUSIONS
Many studies will be needed to test and further develop this model. Numerous concrete and accessible experiments have been proposed (Fisher, 2015). If this proposal is validated it will radically change our understanding of neural mechanisms involved in normal neurocognitive processing as well as psychiatric disorders and treatments. Our fundamental understanding of the biological basis of the mysterious,
and powerful, nature of our cognitive capabilities will now be linked with the mysterious, and powerful, quantum world.

Building machines that learn and think like people
Brenden M. Lake (a1), Tomer D. Ullman (a2), Joshua B. Tenenbaum (a3) and Samuel J. Gershman (a4) 

DOI: https://doi.org/10.1017/S0140525X16001837
Published online by Cambridge University Press: 24 November 2016

Abstract
Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.

1. Introduction
Artificial intelligence (AI) has been a story of booms and busts, yet by any traditional measure of success, the last few years have been marked by exceptional progress. Much of this progress has come from recent advances in “deep learning,” characterized by learning large neural network-style models with multiple layers of representation (see Glossary in Table 1). These models have achieved remarkable gains in many domains spanning object recognition, speech recognition, and control (LeCun et al. 2015; Schmidhuber 2015). In object recognition, Krizhevsky et al. (2012) trained a deep convolutional neural network (ConvNet [LeCun et al. 1989]) that nearly halved the previous state-of-the-art error rate on the most challenging benchmark to date. In the years since, ConvNets continue to dominate, recently approaching human-level performance on some object recognition benchmarks (He et al. 2016; Russakovsky et al. 2015; Szegedy et al. 2014). In automatic speech recognition, hidden Markov models (HMMs) have been the leading approach since the late 1980s (Juang & Rabiner 1990), yet this framework has been chipped away piece by piece and replaced with deep learning components (Hinton et al. 2012). Now, the leading approaches to speech recognition are fully neural network systems (Graves et al. 2013; Hannun et al. 2014). Ideas from deep learning have also been applied to learning complex control problems. Mnih et al. (2015) combined ideas from deep learning and reinforcement learning to make a “deep reinforcement learning” algorithm that learns to play large classes of simple video games from just frames of pixels and the game score, achieving human- or superhuman-level performance on many of them (see also Guo et al. 2014; Schaul et al. 2016; Stadie et al. 2016). 

These accomplishments have helped neural networks regain their status as a leading paradigm in machine learning, much as they were in the late 1980s and early 1990s. The recent success of neural networks has captured attention beyond academia. In industry, companies such as Google and Facebook have active research divisions exploring these technologies, and object and speech recognition systems based on deep learning have been deployed in core products on smart phones and the web. The media have also covered many of the recent achievements of neural networks, often expressing the view that neural networks have achieved this recent success by virtue of their brain-like computation and, therefore, their ability to emulate human learning and human cognition.
In this article, we view this excitement as an opportunity to examine what it means for a machine to learn or think like a person. We first review some of the criteria previously offered by cognitive scientists, developmental psychologists, and artificial intelligence (AI) researchers. Second, we articulate what we view as the essential ingredients for building a machine that learns or thinks like a person, synthesizing theoretical ideas and experimental data from research in cognitive science. Third, we consider contemporary AI (and deep learning in particular) in the light of these ingredients, finding that deep learning models have yet to incorporate many of them, and so may be solving some problems in different ways than people do. We end by discussing what we view as the most plausible paths toward building machines that learn and think like people. This includes prospects for integrating deep learning with the core cognitive ingredients we identify, inspired in part by recent work fusing neural networks with lower-level building blocks from classic psychology and computer science (attention, working memory, stacks, queues) that have traditionally been seen as incompatible.
Beyond the specific ingredients in our proposal, we draw a broader distinction between two different computational approaches to intelligence. The statistical pattern recognition approach treats prediction as primary, usually in the context of a specific classification, regression, or control task. In this view, learning is about discovering features that have high-value states in common – a shared label in a classification setting or a shared value in a reinforcement learning setting – across a large, diverse set of training data.
The alternative approach treats models of the world as primary, where learning is the process of model building. Cognition is about using these models to understand the world, to explain what we see, to imagine what could have happened that didn't, or what could be true that isn't, and then planning actions to make it so. The difference between pattern recognition and model building, between prediction and explanation, is central to our view of human intelligence. Just as scientists seek to explain nature, not simply predict it, we see human thought as fundamentally a model building activity. We elaborate this key point with numerous examples below. We also discuss how pattern recognition, even if it is not the core of intelligence, can nonetheless support model building, through “model-free” algorithms that learn through experience how to make essential inferences more computationally efficient.
Before proceeding, we provide a few caveats about the goals of this article, and a brief overview of the key ideas.
1.1. What this article is not
For nearly as long as there have been neural networks, there have been critiques of neural networks (Crick 1989; Fodor & Pylyshyn 1988; Marcus 1998, 2001; Minsky & Papert 1969; Pinker & Prince 1988). Although we are critical of neural networks in this article, our goal is to build on their successes rather than dwell on their shortcomings. We see a role for neural networks in developing more human-like learning machines: They have been applied in compelling ways to many types of machine learning problems, demonstrating the power of gradient-based learning and deep hierarchies of latent variables. Neural networks also have a rich history as computational models of cognition (McClelland et al. 1986; Rumelhart et al. 1986b). It is a history we describe in more detail in the next section. At a more fundamental level, any computational model of learning must ultimately be grounded in the brain's biological neural networks.
We also believe that future generations of neural networks will look very different from the current state-of-the-art neural networks. They may be endowed with intuitive physics, theory of mind, causal reasoning, and other capacities we describe in the sections that follow. More structure and inductive biases could be built into the networks or learned from previous experience with related tasks, leading to more human-like patterns of learning and development. Networks may learn to effectively search for and discover new mental models or intuitive theories, and these improved models will, in turn, enable subsequent learning, allowing systems that learn-to-learn – using previous knowledge to make richer inferences from very small amounts of training data.
It is also important to draw a distinction between AI that purports to emulate or draw inspiration from aspects of human cognition and AI that does not. This article focuses on the former. The latter is a perfectly reasonable and useful approach to developing AI algorithms: avoiding cognitive or neural inspiration as well as claims of cognitive or neural plausibility. Indeed, this is how many researchers have proceeded, and this article has little pertinence to work conducted under this research strategy. 1 On the other hand, we believe that reverse engineering human intelligence can usefully inform AI and machine learning (and has already done so), especially for the types of domains and tasks that people excel at. Despite recent computational achievements, people are better than machines at solving a range of difficult computational problems, including concept learning, scene understanding, language acquisition, language understanding, speech recognition, and so on. Other human cognitive abilities remain difficult to understand computationally, including creativity, common sense, and general-purpose reasoning. As long as natural intelligence remains the best example of intelligence, we believe that the project of reverse engineering the human solutions to difficult computational problems will continue to inform and advance AI.
Finally, whereas we focus on neural network approaches to AI, we do not wish to give the impression that these are the only contributors to recent advances in AI. On the contrary, some of the most exciting recent progress has been in new forms of probabilistic machine learning (Ghahramani 2015). For example, researchers have developed automated statistical reasoning techniques (Lloyd et al. 2014), automated techniques for model building and selection (Grosse et al. 2012), and probabilistic programming languages (e.g., Gelman et al. 2015; Goodman et al. 2008; Mansinghka et al. 2014). We believe that these approaches will play important roles in future AI systems, and they are at least as compatible with the ideas from cognitive science we discuss here. However, a full discussion of those connections is beyond the scope of the current article.
1.2. Overview of the key ideas
The central goal of this article is to propose a set of core ingredients for building more human-like learning and thinking machines. We elaborate on each of these ingredients and topics in Section 4, but here we briefly overview the key ideas.
The first set of ingredients focuses on developmental “start-up software,” or cognitive capabilities present early in development. There are several reasons for this focus on development. If an ingredient is present early in development, it is certainly active and available well before a child or adult would attempt to learn the types of tasks discussed in this paper. This is true regardless of whether the early-present ingredient is itself learned from experience or innately present. Also, the earlier an ingredient is present, the more likely it is to be foundational to later development and learning.
We focus on two pieces of developmental start-up software (see Wellman & Gelman [1992] for a review of both). First is intuitive physics (sect. 4.1.1): Infants have primitive object concepts that allow them to track objects over time and to discount physically implausible trajectories. For example, infants know that objects will persist over time and that they are solid and coherent. Equipped with these general principles, people can learn more quickly and make more accurate predictions. Although a task may be new, physics still works the same way. A second type of software present in early development is intuitive psychology (sect. 4.1.2): Infants understand that other people have mental states like goals and beliefs, and this understanding strongly constrains their learning and predictions. A child watching an expert play a new video game can infer that the avatar has agency and is trying to seek reward while avoiding punishment. This inference immediately constrains other inferences, allowing the child to infer what objects are good and what objects are bad. These types of inferences further accelerate the learning of new tasks.
Our second set of ingredients focus on learning. Although there are many perspectives on learning, we see model building as the hallmark of human-level learning, or explaining observed data through the construction of causal models of the world (sect. 4.2.2). From this perspective, the early-present capacities for intuitive physics and psychology are also causal models of the world. A primary job of learning is to extend and enrich these models and to build analogous causally structured theories of other domains.
Compared with state-of-the-art algorithms in machine learning, human learning is distinguished by its richness and its efficiency. Children come with the ability and the desire to uncover the underlying causes of sparsely observed events and to use that knowledge to go far beyond the paucity of the data. It might seem paradoxical that people are capable of learning these richly structured models from very limited amounts of experience. We suggest that compositionality and learning-to-learn are ingredients that make this type of rapid model learning possible (sects. 4.2.1 and 4.2.3, respectively).
A final set of ingredients concerns how the rich models our minds build are put into action, in real time (sect. 4.3). It is remarkable how fast we are to perceive and to act. People can comprehend a novel scene in a fraction of a second, or a novel utterance in little more than the time it takes to say it and hear it. An important motivation for using neural networks in machine vision and speech systems is to respond as quickly as the brain does. Although neural networks are usually aiming at pattern recognition rather than model building, we discuss ways in which these “model-free” methods can accelerate slow model-based inferences in perception and cognition (sect. 4.3.1) (see Glossary in Table 1). By learning to recognize patterns in these inferences, the outputs of inference can be predicted without having to go through costly intermediate steps. Integrating neural networks that “learn to do inference” with rich model building learning mechanisms offers a promising way to explain how human minds can understand the world so well and so quickly.
We also discuss the integration of model-based and model-free methods in reinforcement learning (sect. 4.3.2.), an area that has seen rapid recent progress. Once a causal model of a task has been learned, humans can use the model to plan action sequences that maximize future reward. When rewards are used as the metric for successs in model building, this is known as model-based reinforcement learning. However, planning in complex models is cumbersome and slow, making the speed-accuracy trade-off unfavorable for real-time control. By contrast, model-free reinforcement learning algorithms, such as current instantiations of deep reinforcement learning, support fast control, but at the cost of inflexibility and possibly accuracy. We review evidence that humans combine model-based and model-free learning algorithms both competitively and cooperatively and that these interactions are supervised by metacognitive processes. The sophistication of human-like reinforcement learning has yet to be realized in AI systems, but this is an area where crosstalk between cognitive and engineering approaches is especially promising.
2. Cognitive and neural inspiration in artificial intelligence
The questions of whether and how AI should relate to human cognitive psychology are older than the terms artificial intelligence and cognitive psychology. Alan Turing suspected that it was easier to build and educate a child-machine than try to fully capture adult human cognition (Turing 1950). Turing pictured the child's mind as a notebook with “rather little mechanism and lots of blank sheets,” and the mind of a child-machine as filling in the notebook by responding to rewards and punishments, similar to reinforcement learning. This view on representation and learning echoes behaviorism, a dominant psychological tradition in Turing's time. It also echoes the strong empiricism of modern connectionist models – the idea that we can learn almost everything we know from the statistical patterns of sensory inputs.
Cognitive science repudiated the oversimplified behaviorist view and came to play a central role in early AI research (Boden 2006). Newell and Simon (1961) developed their “General Problem Solver” as both an AI algorithm and a model of human problem solving, which they subsequently tested experimentally (Newell & Simon 1972). AI pioneers in other areas of research explicitly referenced human cognition and even published papers in cognitive psychology journals (e.g., Bobrow & Winograd 1977; Hayes-Roth & Hayes-Roth 1979; Winograd 1972). For example, Schank (1972), writing in the journal Cognitive Psychology, declared that “We hope to be able to build a program that can learn, as a child does, how to do what we have described in this paper instead of being spoon-fed the tremendous information necessary” (p. 629).
A similar sentiment was expressed by Minsky (1974): “I draw no boundary between a theory of human thinking and a scheme for making an intelligent machine; no purpose would be served by separating these today since neither domain has theories good enough to explain—or to produce—enough mental capacity” (p. 6).
Much of this research assumed that human knowledge representation is symbolic and that reasoning, language, planning and vision could be understood in terms of symbolic operations. Parallel to these developments, a radically different approach was being explored based on neuron-like “sub-symbolic” computations (e.g., Fukushima 1980; Grossberg 1976; Rosenblatt 1958). The representations and algorithms used by this approach were more directly inspired by neuroscience than by cognitive psychology, although ultimately it would flower into an influential school of thought about the nature of cognition: parallel distributed processing (PDP) (McClelland et al. 1986; Rumelhart et al. 1986b). As its name suggests, PDP emphasizes parallel computation by combining simple units to collectively implement sophisticated computations. The knowledge learned by these neural networks is thus distributed across the collection of units rather than localized as in most symbolic data structures. The resurgence of recent interest in neural networks, more commonly referred to as “deep learning,” shares the same representational commitments and often even the same learning algorithms as the earlier PDP models. “Deep” refers to the fact that more powerful models can be built by composing many layers of representation (see LeCun et al. [2015] and Schmidhuber [2015] for recent reviews), still very much in the PDP style while utilizing recent advances in hardware and computing capabilities, as well as massive data sets, to learn deeper models.
It is also important to clarify that the PDP perspective is compatible with “model building” in addition to “pattern recognition.” Some of the original work done under the banner of PDP (Rumelhart et al. 1986b) is closer to model building than pattern recognition, whereas the recent large-scale discriminative deep learning systems more purely exemplify pattern recognition (see Bottou [2014] for a related discussion). But, as discussed, there is also a question of the nature of the learned representations within the model – their form, compositionality, and transferability – and the developmental start-up software that was used to get there. We focus on these issues in this article.
Neural network models and the PDP approach offer a view of the mind (and intelligence more broadly) that is sub-symbolic and often populated with minimal constraints and inductive biases to guide learning. Proponents of this approach maintain that many classic types of structured knowledge, such as graphs, grammars, rules, objects, structural descriptions, and programs, can be useful yet misleading metaphors for characterizing thought. These structures are more epiphenomenal than real, emergent properties of more fundamental sub-symbolic cognitive processes (McClelland et al. 2010). Compared with other paradigms for studying cognition, this position on the nature of representation is often accompanied by a relatively “blank slate” vision of initial knowledge and representation, much like Turing's blank notebook.
When attempting to understand a particular cognitive ability or phenomenon within this paradigm, a common scientific strategy is to train a relatively generic neural network to perform the task, adding additional ingredients only when necessary. This approach has shown that neural networks can behave as if they learned explicitly structured knowledge, such as a rule for producing the past tense of words (Rumelhart & McClelland 1986), rules for solving simple balance beam physics problems (McClelland 1988), or a tree to represent types of living things (plants and animals) and their distribution of properties (Rogers & McClelland 2004). Training large-scale relatively generic networks is also the best current approach for object recognition (He et al. 2016; Krizhevsky et al. 2012; Russakovsky et al. 2015; Szegedy et al. 2014), where the high-level feature representations of these convolutional nets have also been used to predict patterns of neural response in human and macaque IT cortex (Khaligh-Razavi & Kriegeskorte 2014; Kriegeskorte 2015; Yamins et al. 2014), as well as human typicality ratings (Lake et al. 2015b) and similarity ratings (Peterson et al. 2016) for images of common objects. Moreover, researchers have trained generic networks to perform structured and even strategic tasks, such as the recent work on using a Deep Q-learning Network (DQN) to play simple video games (Mnih et al. 2015) (see Glossary in Table 1). If neural networks have such broad application in machine vision, language, and control, and if they can be trained to emulate the rule-like and structured behaviors that characterize cognition, do we need more to develop truly human-like learning and thinking machines? How far can relatively generic neural networks bring us toward this goal?
3. Challenges for building more human-like machines
Although cognitive science has not yet converged on a single account of the mind or intelligence, the claim that a mind is a collection of general-purpose neural networks with few initial constraints is rather extreme in contemporary cognitive science. A different picture has emerged that highlights the importance of early inductive biases, including core concepts such as number, space, agency, and objects, as well as powerful learning algorithms that rely on prior knowledge to extract knowledge from small amounts of training data. This knowledge is often richly organized and theory-like in structure, capable of the graded inferences and productive capacities characteristic of human thought.
Here we present two challenge problems for machine learning and AI: learning simple visual concepts (Lake et al. 2015a) and learning to play the Atari game Frostbite (Mnih et al. 2015). We also use the problems as running examples to illustrate the importance of core cognitive ingredients in the sections that follow.
3.1. The Characters Challenge
The first challenge concerns handwritten character recognition, a classic problem for comparing different types of machine learning algorithms. Hofstadter (1985) argued that the problem of recognizing characters in all of the ways people do – both handwritten and printed – contains most, if not all, of the fundamental challenges of AI. Whether or not this statement is correct, it highlights the surprising complexity that underlies even “simple” human-level concepts like letters. More practically, handwritten character recognition is a real problem that children and adults must learn to solve, with practical applications ranging from reading envelope addresses or checks in an automated teller machine (ATM). Handwritten character recognition is also simpler than more general forms of object recognition; the object of interest is two-dimensional, separated from the background, and usually unoccluded. Compared with how people learn and see other types of objects, it seems possible, in the near term, to build algorithms that can see most of the structure in characters that people can see.
The standard benchmark is the Mixed National Institute of Standards and Technology (MNIST) data set for digit recognition, which involves classifying images of digits into the categories ‘0’ to ‘9’ (LeCun et al. 1998). The training set provides 6,000 images per class for a total of 60,000 training images. With a large amount of training data available, many algorithms achieve respectable performance, including K-nearest neighbors (5% test error), support vector machines (about 1% test error), and convolutional neural networks (below 1% test error [LeCun et al. 1998]). The best results achieved using deep convolutional nets are very close to human-level performance at an error rate of 0.2% (Ciresan et al. 2012). Similarly, recent results applying convolutional nets to the far more challenging ImageNet object recognition benchmark have shown that human-level performance is within reach on that data set as well (Russakovsky et al. 2015).
Although humans and neural networks may perform equally well on the MNIST digit recognition task and other large-scale image classification tasks, it does not mean that they learn and think in the same way. There are at least two important differences: people learn from fewer examples and they learn richer representations, a comparison true for both learning handwritten characters and for learning more general classes of objects (Fig. 1). People can learn to recognize a new handwritten character from a single example (Fig. 1A-i), allowing them to discriminate between novel instances drawn by other people and similar looking non-instances (Lake et al. 2015a; Miller et al. 2000). Moreover, people learn more than how to do pattern recognition: they learn a concept, that is, a model of the class that allows their acquired knowledge to be flexibly applied in new ways. In addition to recognizing new examples, people can also generate new examples (Fig. 1A-ii), parse a character into its most important parts and relations (Fig. 1A-iii) (Lake et al. 2012), and generate new characters given a small set of related characters (Fig. 1A-iv). These additional abilities come for free along with the acquisition of the underlying concept.
Even for these simple visual concepts, people are still better and more sophisticated learners than the best algorithms for character recognition. People learn a lot more from a lot less, and capturing these human-level learning abilities in machines is the Characters Challenge. We recently reported progress on this challenge using probabilistic program induction (Lake et al. 2015a) (see Glossary in Table 1), yet aspects of the full human cognitive ability remain out of reach. Although both people and models represent characters as a sequence of pen strokes and relations, people have a far richer repertoire of structural relations between strokes. Furthermore, people can efficiently integrate across multiple examples of a character to infer which have optional elements, such as the horizontal cross-bar in ‘7's, combining different variants of the same character into a single coherent representation. Additional progress may come by combining deep learning and probabilistic program induction to tackle even richer versions of the Characters Challenge.
3.2. The Frostbite Challenge
The second challenge concerns the Atari game Frostbite (Fig. 2), which was one of the control problems tackled by the DQN of Mnih et al. (2015). The DQN was a significant advance in reinforcement learning, showing that a single algorithm can learn to play a wide variety of complex tasks. The network was trained to play 49 classic Atari games, proposed as a test domain for reinforcement learning (Bellemare et al. 2013), impressively achieving human-level performance or above on 29 of the games. It did, however, have particular trouble with Frostbite and other games that required temporally extended planning strategies. 
In Frostbite, players control an agent (Frostbite Bailey) tasked with constructing an igloo within a time limit. The igloo is built piece by piece as the agent jumps on ice floes in water (Fig. 2A–C). The challenge is that the ice floes are in constant motion (moving either left or right), and ice floes only contribute to the construction of the igloo if they are visited in an active state (white, rather than blue). The agent may also earn extra points by gathering fish while avoiding a number of fatal hazards (falling in the water, snow geese, polar bears, etc.). Success in this game requires a temporally extended plan to ensure the agent can accomplish a sub-goal (such as reaching an ice floe) and then safely proceed to the next sub-goal. Ultimately, once all of the pieces of the igloo are in place, the agent must proceed to the igloo and complete the level before time expires (Fig. 2C).
The DQN learns to play Frostbite and other Atari games by combining a powerful pattern recognizer (a deep convolutional neural network) and a simple model-free reinforcement learning algorithm (Q-learning [Watkins & Dayan 1992]). These components allow the network to map sensory inputs (frames of pixels) onto a policy over a small set of actions, and both the mapping and the policy are trained to optimize long-term cumulative reward (the game score). The network embodies the strongly empiricist approach characteristic of most connectionist models: very little is built into the network apart from the assumptions about image structure inherent in convolutional networks, so the network has to essentially learn a visual and conceptual system from scratch for each new game. In Mnih et al. (2015), the network architecture and hyper-parameters were fixed, but the network was trained anew for each game, meaning the visual system and the policy are highly specialized for the games it was trained on. More recent work has shown how these game-specific networks can share visual features (Rusu et al. 2016) or be used to train a multitask network (Parisotto et al. 2016), achieving modest benefits of transfer when learning to play new games.
Although it is interesting that the DQN learns to play games at human-level performance while assuming very little prior knowledge, the DQN may be learning to play Frostbite and other games in a very different way than people do. One way to examine the differences is by considering the amount of experience required for learning. In Mnih et al. (2015), the DQN was compared with a professional gamer who received approximately 2 hours of practice on each of the 49 Atari games (although he or she likely had prior experience with some of the games). The DQN was trained on 200 million frames from each of the games, which equates to approximately 924 hours of game time (about 38 days), or almost 500 times as much experience as the human received. 2 Additionally, the DQN incorporates experience replay, where each of these frames is replayed approximately eight more times on average over the course of learning.
With the full 924 hours of unique experience and additional replay, the DQN achieved less than 10% of human-level performance during a controlled test session (see DQN in Fig. 3). More recent variants of the DQN perform better, and can even outperform the human tester (Schaul et al. 2016; Stadie et al. 2016; van Hasselt et al. 2016; Wang et al. 2016), reaching 83% of the professional gamer's score by incorporating smarter experience replay (Schaul et al. 2016), and 172% by using smarter replay and more efficient parameter sharing (Wang et al. 2016) (see DQN+ and DQN++ in Fig. 3). 3 But they require a lot of experience to reach this level. The learning curve for the model of Wang et al. (2016) shows performance is approximately 44% after 200 hours, 8% after 100 hours, and less than 2% after 5 hours (which is close to random play, approximately 1.5%). The differences between the human and machine learning curves suggest that they may be learning different kinds of knowledge, using different learning mechanisms, or both. The contrast becomes even more dramatic if we look at the very earliest stages of learning. Although both the original DQN and these more recent variants require multiple hours of experience to perform reliably better than random play, even non-professional humans can grasp the basics of the game after just a few minutes of play. We speculate that people do this by inferring a general schema to describe the goals of the game and the object types and their interactions, using the kinds of intuitive theories, model-building abilities and model-based planning mechanisms we describe below. Although novice players may make some mistakes, such as inferring that fish are harmful rather than helpful, they can learn to play better than chance within a few minutes. If humans are able to first watch an expert playing for a few minutes, they can learn even faster. In informal experiments with two of the authors playing Frostbite on a Javascript emulator (http://www.virtualatari.org/soft.php?soft=Frostbite), after watching videos of expert play on YouTube for just 2 minutes, we found that we were able to reach scores comparable to or better than the human expert reported in Mnih et al. (2015) after at most 15 to 20 minutes of total practice. 4 
There are other behavioral signatures that suggest fundamental differences in representation and learning between people and the DQN. For example, the game of Frostbite provides incremental rewards for reaching each active ice floe, providing the DQN with the relevant sub-goals for completing the larger task of building an igloo. Without these sub-goals, the DQN would have to take random actions until it accidentally builds an igloo and is rewarded for completing the entire level. In contrast, people likely do not rely on incremental scoring in the same way when figuring out how to play a new game. In Frostbite, it is possible to figure out the higher-level goal of building an igloo without incremental feedback; similarly, sparse feedback is a source of difficulty in other Atari 2600 games such as Montezuma's Revenge, in which people substantially outperform current DQN approaches.
The learned DQN network is also rather inflexible to changes in its inputs and goals. Changing the color or appearance of objects or changing the goals of the network would have devastating consequences on performance if the network is not retrained. Although any specific model is necessarily simplified and should not be held to the standard of general human intelligence, the contrast between DQN and human flexibility is striking nonetheless. For example, imagine you are tasked with playing Frostbite with any one of these new goals: 
1. Get the lowest possible score.
2. Get closest to 100, or 300, or 1,000, or 3,000, or any level, without going over.
3. Beat your friend, who's playing next to you, but just barely, not by too much, so as not to embarrass them.
4. Go as long as you can without dying.
5. Die as quickly as you can.
6. Pass each level at the last possible minute, right before the temperature timer hits zero and you die (i.e., come as close as you can to dying from frostbite without actually dying).
7. Get to the furthest unexplored level without regard for your score.
8. See if you can discover secret Easter eggs.
9. Get as many fish as you can.
10. Touch all of the individual ice floes on screen once and only once.
11. Teach your friend how to play as efficiently as possible.
This range of goals highlights an essential component of human intelligence: people can learn models and use them for arbitrary new tasks and goals. Although neural networks can learn multiple mappings or tasks with the same set of stimuli – adapting their outputs depending on a specified goal – these models require substantial training or reconfiguration to add new tasks (e.g., Collins & Frank 2013; Eliasmith et al. 2012; Rougier et al. 2005). In contrast, people require little or no retraining or reconfiguration, adding new tasks and goals to their repertoire with relative ease.
The Frostbite example is a particularly telling contrast when compared with human play. Even the best deep networks learn gradually over many thousands of game episodes, take a long time to reach good performance, and are locked into particular input and goal patterns. Humans, after playing just a small number of games over a span of minutes, can understand the game and its goals well enough to perform better than deep networks do after almost a thousand hours of experience. Even more impressively, people understand enough to invent or accept new goals, generalize over changes to the input, and explain the game to others. Why are people different? What core ingredients of human intelligence might the DQN and other modern machine learning methods be missing?
One might object that both the Frostbite and Characters challenges draw an unfair comparison between the speed of human learning and neural network learning. We discuss this objection in detail in Section 5, but we feel it is important to anticipate it here as well. To paraphrase one reviewer of an earlier draft of this article, “It is not that DQN and people are solving the same task differently. They may be better seen as solving different tasks. Human learners – unlike DQN and many other deep learning systems – approach new problems armed with extensive prior experience. The human is encountering one in a years-long string of problems, with rich overlapping structure. Humans as a result often have important domain-specific knowledge for these tasks, even before they ‘begin.’ The DQN is starting completely from scratch.”
We agree, and indeed this is another way of putting our point here. Human learners fundamentally take on different learning tasks than today's neural networks, and if we want to build machines that learn and think like people, our machines need to confront the kinds of tasks that human learners do, not shy away from them. People never start completely from scratch, or even close to “from scratch,” and that is the secret to their success. The challenge of building models of human learning and thinking then becomes: How do we bring to bear rich prior knowledge to learn new tasks and solve new problems so quickly? What form does that prior knowledge take, and how is it constructed, from some combination of inbuilt capacities and previous experience? The core ingredients we propose in the next section offer one route to meeting this challenge.
4. Core ingredients of human intelligence
In the Introduction, we laid out what we see as core ingredients of intelligence. Here we consider the ingredients in detail and contrast them with the current state of neural network modeling. Although these are hardly the only ingredients needed for human-like learning and thought (see our discussion of language in sect. 5), they are key building blocks, which are not present in most current learning-based AI systems – certainly not all present together – and for which additional attention may prove especially fruitful. We believe that integrating them will produce significantly more powerful and more human-like learning and thinking abilities than we currently see in AI systems.
Before considering each ingredient in detail, it is important to clarify that by “core ingredient” we do not necessarily mean an ingredient that is innately specified by genetics or must be “built in” to any learning algorithm. We intend our discussion to be agnostic with regards to the origins of the key ingredients. By the time a child or an adult is picking up a new character or learning how to play Frostbite, he or she is armed with extensive real-world experience that deep learning systems do not benefit from – experience that would be hard to emulate in any general sense. Certainly, the core ingredients are enriched by this experience, and some may even be a product of the experience itself. Whether learned, built in, or enriched, the key claim is that these ingredients play an active and important role in producing human-like learning and thought, in ways contemporary machine learning has yet to capture.
4.1. Developmental start-up software
Early in development, humans have a foundational understanding of several core domains (Spelke 2003; Spelke & Kinzler 2007). These domains include number (numerical and set operations), space (geometry and navigation), physics (inanimate objects and mechanics), and psychology (agents and groups). These core domains cleave cognition at its conceptual joints, and each domain is organized by a set of entities and abstract principles relating the entities to each other. The underlying cognitive representations can be understood as “intuitive theories,” with a causal structure resembling a scientific theory (Carey 2004; 2009; Gopnik et al. 2004; Gopnik & Meltzo 1999; Gweon et al. 2010; Schulz 2012b; Wellman & Gelman 1992; 1998). The “child as scientist” proposal further views the process of learning itself as also scientist-like, with recent experiments showing that children seek out new data to distinguish between hypotheses, isolate variables, test causal hypotheses, make use of the data-generating process in drawing conclusions, and learn selectively from others (Cook et al. 2011; Gweon et al. 2010; Schulz et al. 2007; Stahl & Feigenson 2015; Tsividis et al. 2013). We address the nature of learning mechanisms in Section 4.2.
Each core domain has been the target of a great deal of study and analysis, and together the domains are thought to be shared cross-culturally and partly with non-human animals. All of these domains may be important augmentations to current machine learning, though below, we focus in particular on the early understanding of objects and agents.
4.1.1. Intuitive physics
Young children have a rich knowledge of intuitive physics. Whether learned or innate, important physical concepts are present at ages far earlier than when a child or adult learns to play Frostbite, suggesting these resources may be used for solving this and many everyday physics-related tasks.
At the age of 2 months, and possibly earlier, human infants expect inanimate objects to follow principles of persistence, continuity, cohesion, and solidity. Young infants believe objects should move along smooth paths, not wink in and out of existence, not inter-penetrate and not act at a distance (Spelke 1990; Spelke et al. 1995). These expectations guide object segmentation in early infancy, emerging before appearance-based cues such as color, texture, and perceptual goodness (Spelke 1990).
These expectations also go on to guide later learning. At around 6 months, infants have already developed different expectations for rigid bodies, soft bodies, and liquids (Rips & Hespos 2015). Liquids, for example, are expected to go through barriers, while solid objects cannot (Hespos et al. 2009). By their first birthday, infants have gone through several transitions of comprehending basic physical concepts such as inertia, support, containment, and collisions (Baillargeon 2004; Baillargeon et al. 2009; Hespos & Baillargeon 2008).
There is no single agreed-upon computational account of these early physical principles and concepts, and previous suggestions have ranged from decision trees (Baillargeon et al. 2009), to cues, to lists of rules (Siegler & Chen 1998). A promising recent approach sees intuitive physical reasoning as similar to inference over a physics software engine, the kind of simulators that power modern-day animations and games (Bates et al. 2015; Battaglia et al. 2013; Gerstenberg et al. 2015; Sanborn et al. 2013). According to this hypothesis, people reconstruct a perceptual scene using internal representations of the objects and their physically relevant properties (such as mass, elasticity, and surface friction) and forces acting on objects (such as gravity, friction, or collision impulses). Relative to physical ground truth, the intuitive physical state representation is approximate and probabilistic, and oversimplified and incomplete in many ways. Still, it is rich enough to support mental simulations that can predict how objects will move in the immediate future, either on their own or in responses to forces we might apply.
This “intuitive physics engine” approach enables flexible adaptation to a wide range of everyday scenarios and judgments in a way that goes beyond perceptual cues. For example, (Fig. 4), a physics-engine reconstruction of a tower of wooden blocks from the game Jenga can be used to predict whether (and how) a tower will fall, finding close quantitative fits to how adults make these predictions (Battaglia et al. 2013), as well as simpler kinds of physical predictions that have been studied in infants (Téglás et al. 2011). Simulation-based models can also capture how people make hypothetical or counterfactual predictions: What would happen if certain blocks were taken away, more blocks were added, or the table supporting the tower was jostled? What if certain blocks were glued together, or attached to the table surface? What if the blocks were made of different materials (Styrofoam, lead, ice)? What if the blocks of one color were much heavier than those of other colors? Each of these physical judgments may require new features or new training for a pattern recognition account to work at the same level as the model-based simulator
What are the prospects for embedding or acquiring this kind of intuitive physics in deep learning systems? Connectionist models in psychology have previously been applied to physical reasoning tasks such as balance-beam rules (McClelland 1988; Shultz 2003) or rules relating to distance, velocity, and time in motion (Buckingham & Shultz 2000). However, these networks do not attempt to work with complex scenes as input, or a wide range of scenarios and judgments as in Figure 4. A recent paper from Facebook AI researchers (Lerer et al. 2016) represents an exciting step in this direction. Lerer et al. (2016) trained a deep convolutional network-based system (PhysNet) to predict the stability of block towers from simulated images similar to those in Figure 4A, but with much simpler configurations of two, three, or four cubical blocks stacked vertically. Impressively, PhysNet generalized to simple real images of block towers, matching human performance on these images, meanwhile exceeding human performance on synthetic images. Human and PhysNet confidence were also correlated across towers, although not as strongly as for the approximate probabilistic simulation models and experiments of Battaglia et al. (2013). One limitation is that PhysNet currently requires extensive training – between 100,000 and 200,000 scenes – to learn judgments for just a single task (will the tower fall?) on a narrow range of scenes (towers with two to four cubes). It has been shown to generalize, but also only in limited ways (e.g., from towers of two and three cubes to towers of four cubes). In contrast, people require far less experience to perform any particular task, and can generalize to many novel judgments and complex scenes with no new training required (although they receive large amounts of physics experience through interacting with the world more generally). Could deep learning systems such as PhysNet capture this flexibility, without explicitly simulating the causal interactions between objects in three dimensions? We are not sure, but we hope this is a challenge they will take on.
Alternatively, instead of trying to make predictions without simulating physics, could neural networks be trained to emulate a general-purpose physics simulator, given the right type and quantity of training data, such as the raw input experienced by a child? This is an active and intriguing area of research, but it too faces significant challenges. For networks trained on object classification, deeper layers often become sensitive to successively higher-level features, from edges to textures to shape-parts to full objects (Yosinski et al. 2014; Zeiler & Fergus 2014). For deep networks trained on physics-related data, it remains to be seen whether higher layers will encode objects, general physical properties, forces, and approximately Newtonian dynamics. A generic network trained on dynamic pixel data might learn an implicit representation of these concepts, but would it generalize broadly beyond training contexts as people's more explicit physical concepts do? Consider, for example, a network that learns to predict the trajectories of several balls bouncing in a box (Kodratoff & Michalski 2014). If this network has actually learned something like Newtonian mechanics, then it should be able to generalize to interestingly different scenarios – at a minimum different numbers of differently shaped objects, bouncing in boxes of different shapes and sizes and orientations with respect to gravity, not to mention more severe generalization tests such as all of the tower tasks discussed above, which also fall under the Newtonian domain. Neural network researchers have yet to take on this challenge, but we hope they will. Whether such models can be learned with the kind (and quantity) of data available to human infants is not clear, as we discuss further in Section 5.
It may be difficult to integrate object and physics-based primitives into deep neural networks, but the payoff in terms of learning speed and performance could be great for many tasks. Consider the case of learning to play Frostbite. Although it can be difficult to discern exactly how a network learns to solve a particular task, the DQN probably does not parse a Frostbite screenshot in terms of stable objects or sprites moving according to the rules of intuitive physics (Fig. 2). But incorporating a physics-engine–based representation could help DQNs learn to play games such as Frostbite in a faster and more general way, whether the physics knowledge is captured implicitly in a neural network or more explicitly in a simulator. Beyond reducing the amount of training data, and potentially improving the level of performance reached by the DQN, it could eliminate the need to retrain a Frostbite network if the objects (e.g., birds, ice floes, and fish) are slightly altered in their behavior, reward structure, or appearance. When a new object type such as a bear is introduced, as in the later levels of Frostbite (Fig. 2D), a network endowed with intuitive physics would also have an easier time adding this object type to its knowledge (the challenge of adding new objects was also discussed in Marcus [1998; 2001]). In this way, the integration of intuitive physics and deep learning could be an important step toward more human-like learning algorithms.
4.1.2. Intuitive psychology
Intuitive psychology is another early-emerging ability with an important influence on human learning and thought. Pre-verbal infants distinguish animate agents from inanimate objects. This distinction is partially based on innate or early-present detectors for low-level cues, such as the presence of eyes, motion initiated from rest, and biological motion (Johnson et al. 1998; Premack & Premack 1997; Schlottmann et al. 2006; Tremoulet & Feldman 2000). Such cues are often sufficient but not necessary for the detection of agency.
Beyond these low-level cues, infants also expect agents to act contingently and reciprocally, to have goals, and to take efficient actions toward those goals subject to constraints (Csibra 2008; Csibra et al. 2003; Spelke & Kinzler 2007). These goals can be socially directed; at around 3 months of age, infants begin to discriminate antisocial agents that hurt or hinder others from neutral agents (Hamlin 2013; Hamlin et al. 2010), and they later distinguish between anti-social, neutral, and pro-social agents (Hamlin et al. 2007; 2013).
It is generally agreed that infants expect agents to act in a goal-directed, efficient, and socially sensitive fashion (Spelke & Kinzler 2007). What is less agreed on is the computational architecture that supports this reasoning and whether it includes any reference to mental states and explicit goals.
One possibility is that intuitive psychology is simply cues “all the way down” (Schlottmann et al. 2013; Scholl & Gao 2013), though this would require more and more cues as the scenarios become more complex. Consider, for example, a scenario in which an agent A is moving toward a box, and an agent B moves in a way that blocks A from reaching the box. Infants and adults are likely to interpret B's behavior as “hindering” (Hamlin 2013). This inference could be captured by a cue that states, “If an agent's expected trajectory is prevented from completion, the blocking agent is given some negative association.”
Although the cue is easily calculated, the scenario is also easily changed to necessitate a different type of cue. Suppose A was already negatively associated (a “bad guy”); acting negatively toward A could then be seen as good (Hamlin 2013). Or suppose something harmful was in the box, which A did not know about. Now B would be seen as helping, protecting, or defending A. Suppose A knew there was something bad in the box and wanted it anyway. B could be seen as acting paternalistically. A cue-based account would be twisted into gnarled combinations such as, “If an expected trajectory is prevented from completion, the blocking agent is given some negative association, unless that trajectory leads to a negative outcome or the blocking agent is previously associated as positive, or the blocked agent is previously associated as negative, or….”
One alternative to a cue-based account is to use generative models of action choice, as in the Bayesian inverse planning, or Bayesian theory of mind (ToM), models of Baker et al. (2009) or the naive utility calculus models of Jara-Ettinger et al. (2015) (see also Jern and Kemp [2015] and Tauber and Steyvers [2011] and a related alternative based on predictive coding from Kilner et al. [2007]). These models formalize explicitly mentalistic concepts such as “goal,” “agent,” “planning,” “cost,” “efficiency,” and “belief,” used to describe core psychological reasoning in infancy. They assume adults and children treat agents as approximately rational planners who choose the most efficient means to their goals. Planning computations may be formalized as solutions to Markov decision processes (MDPs) or partially observable Markov decision processes (POMDPs), taking as input utility and belief functions defined over an agent's state-space and the agent's state-action transition functions, and returning a series of actions the agent should perform to most efficiently fulfill their goals (or maximize their utility). By simulating these planning processes, people can predict what agents might do next, or use inverse reasoning from observing a series of actions to infer the utilities and beliefs of agents in a scene. This is directly analogous to how simulation engines can be used for intuitive physics, to predict what will happen next in a scene or to infer objects’ dynamical properties from how they move. It yields similarly flexible reasoning abilities: Utilities and beliefs can be adjusted to take into account how agents might act for a wide range of novel goals and situations. Importantly, unlike in intuitive physics, simulation-based reasoning in intuitive psychology can be nested recursively to understand social interactions. We can think about agents thinking about other agents.
As in the case of intuitive physics, the success that generic deep networks will have in capturing intuitive psychological reasoning will depend in part on the representations humans use. Although deep networks have not yet been applied to scenarios involving theory of mind and intuitive psychology, they could probably learn visual cues, heuristics and summary statistics of a scene that happens to involve agents. 5 If that is all that underlies human psychological reasoning, a data-driven deep learning approach can likely find success in this domain.
However, it seems to us that any full formal account of intuitive psychological reasoning needs to include representations of agency, goals, efficiency, and reciprocal relations. As with objects and forces, it is unclear whether a complete representation of these concepts (agents, goals, etc.) could emerge from deep neural networks trained in a purely predictive capacity. Similar to the intuitive physics domain, it is possible that with a tremendous number of training trajectories in a variety of scenarios, deep learning techniques could approximate the reasoning found in infancy even without learning anything about goal-directed or socially directed behavior more generally. But this is also unlikely to resemble how humans learn, understand, and apply intuitive psychology unless the concepts are genuine. In the same way that altering the setting of a scene or the target of inference in a physics-related task may be difficult to generalize without an understanding of objects, altering the setting of an agent or their goals and beliefs is difficult to reason about without understanding intuitive psychology.
In introducing the Frostbite challenge, we discussed how people can learn to play the game extremely quickly by watching an experienced player for just a few minutes and then playing a few rounds themselves. Intuitive psychology provides a basis for efficient learning from others, especially in teaching settings with the goal of communicating knowledge efficiently (Shafto et al. 2014). In the case of watching an expert play Frostbite, whether or not there is an explicit goal to teach, intuitive psychology lets us infer the beliefs, desires, and intentions of the experienced player. For example, we can learn that the birds are to be avoided from seeing how the experienced player appears to avoid them. We do not need to experience a single example of encountering a bird, and watching Frostbite Bailey die because of the bird, to infer that birds are probably dangerous. It is enough to see that the experienced player's avoidance behavior is best explained as acting under that belief.
Similarly, consider how a sidekick agent (increasingly popular in video games) is expected to help a player achieve his or her goals. This agent can be useful in different ways in different circumstances, such as getting items, clearing paths, fighting, defending, healing, and providing information, all under the general notion of being helpful (Macindoe 2013). An explicit agent representation can predict how such an agent will be helpful in new circumstances, whereas a bottom-up pixel-based representation is likely to struggle.
There are several ways that intuitive psychology could be incorporated into contemporary deep learning systems. Although it could be built in, intuitive psychology may arise in other ways. Connectionists have argued that innate constraints in the form of hard-wired cortical circuits are unlikely (Elman 2005; Elman et al. 1996), but a simple inductive bias, for example, the tendency to notice things that move other things, can bootstrap reasoning about more abstract concepts of agency (Ullman et al. 2012a). 6 Similarly, a great deal of goal-directed and socially directed actions can also be boiled down to a simple utility calculus (e.g., Jara-Ettinger et al. 2015), in a way that could be shared with other cognitive abilities. Although the origins of intuitive psychology are still a matter of debate, it is clear that these abilities are early emerging and play an important role in human learning and thought, as exemplified in the Frostbite challenge and when learning to play novel video games more broadly.
4.2. Learning as rapid model building
Since their inception, neural networks models have stressed the importance of learning. There are many learning algorithms for neural networks, including the perceptron algorithm (Rosenblatt 1958), Hebbian learning (Hebb 1949), the BCM rule (Bienenstock et al. 1982), backpropagation (Rumelhart et al. 1986a), the wake-sleep algorithm (Hinton et al. 1995), and contrastive divergence (Hinton 2002). Whether the goal is supervised or unsupervised learning, these algorithms implement learning as a process of gradual adjustment of connection strengths. For supervised learning, the updates are usually aimed at improving the algorithm's pattern recognition capabilities. For unsupervised learning, the updates work toward gradually matching the statistics of the model's internal patterns with the statistics of the input data.
In recent years, machine learning has found particular success using backpropagation and large data sets to solve difficult pattern recognition problems (see Glossary in Table 1). Although these algorithms have reached human-level performance on several challenging benchmarks, they are still far from matching human-level learning in other ways. Deep neural networks often need more data than people do to solve the same types of problems, whether it is learning to recognize a new type of object or learning to play a new game. When learning the meanings of words in their native language, children make meaningful generalizations from very sparse data (Carey & Bartlett 1978; Landau et al. 1988; Markman 1989; Smith et al. 2002; Xu & Tenenbaum 2007; although see Horst & Samuelson 2008 regarding memory limitations). Children may only need to see a few examples of the concepts hairbrush, pineapple, and lightsaber, before they largely “get it,” grasping the boundary of the infinite set that defines each concept from the infinite set of all possible objects. Children are far more practiced than adults at learning new concepts, learning roughly 9 or 10 new words each day, after beginning to speak through the end of high school (Bloom 2000; Carey 1978). Yet the ability for rapid “one-shot” learning does not disappear in adulthood. An adult may need to see a single image or movie of a novel two-wheeled vehicle to infer the boundary between this concept and others, allowing him or her to discriminate new examples of that concept from similar-looking objects of a different type (Fig. 1B-i).
Contrasting with the efficiency of human learning, neural networks, by virtue of their generality as highly flexible function approximators, are notoriously data hungry (the bias/variance dilemma [Geman et al. 1992]). Benchmark tasks such as the ImageNet data set for object recognition provide hundreds or thousands of examples per class (Krizhevsky et al. 2012; Russakovsky et al. 2015): 1,000 hairbrushes, 1,000 pineapples, and so on. In the context of learning new, handwritten characters or learning to play Frostbite, the MNIST benchmark includes 6,000 examples of each handwritten digit (LeCun et al. 1998), and the DQN of Mnih et al. (2015) played each Atari video game for approximately 924 hours of unique training experience (Fig. 3). In both cases, the algorithms are clearly using information less efficiently than a person learning to perform the same tasks.
It is also important to mention that there are many classes of concepts that people learn more slowly. Concepts that are learned in school are usually far more challenging and more difficult to acquire, including mathematical functions, logarithms, derivatives, integrals, atoms, electrons, gravity, DNA, and evolution. There are also domains for which machine learners outperform human learners, such as combing through financial or weather data. But for the vast majority of cognitively natural concepts – the types of things that children learn as the meanings of words – people are still far better learners than machines. This is the type of learning we focus on in this section, which is more suitable for the enterprise of reverse engineering and articulating additional principles that make human learning successful. It also opens the possibility of building these ingredients into the next generation of machine learning and AI algorithms, with potential for making progress on learning concepts that are both easy and difficult for humans to acquire.
Even with just a few examples, people can learn remarkably rich conceptual models. One indicator of richness is the variety of functions that these models support (Markman & Ross 2003; Solomon et al. 1999). Beyond classification, concepts support prediction (Murphy & Ross 1994; Rips 1975), action (Barsalou 1983), communication (Markman & Makin 1998), imagination (Jern & Kemp 2013; Ward 1994), explanation (Lombrozo 2009; Williams & Lombrozo 2010), and composition (Murphy 1988; Osherson & Smith 1981). These abilities are not independent; rather they hang together and interact (Solomon et al. 1999), coming for free with the acquisition of the underlying concept. Returning to the previous example of a novel two-wheeled vehicle, a person can sketch a range of new instances (Fig. 1B-ii), parse the concept into its most important components (Fig. 1B-iii), or even create a new complex concept through the combination of familiar concepts (Fig. 1B-iv). Likewise, as discussed in the context of Frostbite, a learner who has acquired the basics of the game could flexibly apply his or her knowledge to an infinite set of Frostbite variants (sect. 3.2). The acquired knowledge supports reconfiguration to new tasks and new demands, such as modifying the goals of the game to survive, while acquiring as few points as possible, or to efficiently teach the rules to a friend.
This richness and flexibility suggest that learning as model building is a better metaphor than learning as pattern recognition. Furthermore, the human capacity for one-shot learning suggests that these models are built upon rich domain knowledge rather than starting from a blank slate (Mikolov et al. 2016; Mitchell et al. 1986). In contrast, much of the recent progress in deep learning has been on pattern recognition problems, including object recognition, speech recognition, and (model-free) video game learning, that use large data sets and little domain knowledge.
There has been recent work on other types of tasks, including learning generative models of images (Denton et al. 2015; Gregor et al. 2015), caption generation (Karpathy & Fei-Fei 2017; Vinyals et al. 2014; Xu et al. 2015), question answering (Sukhbaatar et al. 2015; Weston et al. 2015b), and learning simple algorithms (Graves et al. 2014; Grefenstette et al. 2015). We discuss question answering and learning simple algorithms in Section 6.1. Yet, at least for image and caption generation, these tasks have been mostly studied in the big data setting that is at odds with the impressive human ability to generalize from small data sets (although see Rezende et al. [2016] for a deep learning approach to the Character Challenge). And it has been difficult to learn neural network–style representations that effortlessly generalize new tasks that they were not trained on (see Davis & Marcus 2015; Marcus 1998; 2001). What additional ingredients may be needed to rapidly learn more powerful and more general-purpose representations?
A relevant case study is from our own work on the Characters Challenge (sect. 3.1; Lake 2014; Lake et al. 2015a). People and various machine learning approaches were compared on their ability to learn new handwritten characters from the world's alphabets. In addition to evaluating several types of deep learning models, we developed an algorithm using Bayesian program learning (BPL) that represents concepts as simple stochastic programs: structured procedures that generate new examples of a concept when executed (Fig. 5A). These programs allow the model to express causal knowledge about how the raw data are formed, and the probabilistic semantics allow the model to handle noise and perform creative tasks. Structure sharing across concepts is accomplished by the compositional re-use of stochastic primitives that can combine in new ways to create new concepts
Note that we are overloading the word model to refer to the BPL framework as a whole (which is a generative model), as well as the individual probabilistic models (or concepts) that it infers from images to represent novel handwritten characters. There is a hierarchy of models: a higher-level program that generates different types of concepts, which are themselves programs that can be run to generate tokens of a concept. Here, describing learning as “rapid model building” refers to the fact that BPL constructs generative models (lower-level programs) that produce tokens of a concept (Fig. 5B).
Learning models of this form allows BPL to perform a challenging one-shot classification task at human-level performance (Fig. 1A-i) and to outperform current deep learning models such as convolutional networks (Koch et al. 2015). 7 The representations that BPL learns also enable it to generalize in other, more creative, human-like ways, as evaluated using “visual Turing tests” (e.g., Fig. 5B). These tasks include generating new examples (Figs. 1A-ii and 5B), parsing objects into their essential components (Fig. 1A-iii), and generating new concepts in the style of a particular alphabet (Fig. 1A-iv). The following sections discuss the three main ingredients – compositionality, causality, and learning-to-learn – that were important to the success of this framework and, we believe, are important to understanding human learning as rapid model building more broadly. Although these ingredients fit naturally within a BPL or a probabilistic program induction framework, they could also be integrated into deep learning models and other types of machine learning algorithms, prospects we discuss in more detail below.
4.2.1. Compositionality
Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements. In computer programming, primitive functions can be combined to create new functions, and these new functions can be further combined to create even more complex functions. This function hierarchy provides an efficient description of higher-level functions, such as a hierarchy of parts for describing complex objects or scenes (Bienenstock et al. 1997). Compositionality is also at the core of productivity: an infinite number of representations can be constructed from a finite set of primitives, just as the mind can think an infinite number of thoughts, utter or understand an infinite number of sentences, or learn new concepts from a seemingly infinite space of possibilities (Fodor 1975; Fodor & Pylyshyn 1988; Marcus 2001; Piantadosi 2011).
Compositionality has been broadly influential in both AI and cognitive science, especially as it pertains to theories of object recognition, conceptual representation, and language. Here, we focus on compositional representations of object concepts for illustration. Structural description models represent visual concepts as compositions of parts and relations, which provides a strong inductive bias for constructing models of new concepts (Biederman 1987; Hummel & Biederman 1992; Marr & Nishihara 1978; van den Hengel et al. 2015; Winston 1975). For instance, the novel two-wheeled vehicle in Figure 1B might be represented as two wheels connected by a platform, which provides the base for a post, which holds the handlebars, and so on. Parts can themselves be composed of sub-parts, forming a “partonomy” of part-whole relationships (Miller & Johnson-Laird 1976; Tversky & Hemenway 1984). In the novel vehicle example, the parts and relations can be shared and re-used from existing related concepts, such as cars, scooters, motorcycles, and unicycles. Because the parts and relations are themselves a product of previous learning, their facilitation of the construction of new models is also an example of learning-to-learn, another ingredient that is covered below. Although compositionality and learning-to-learn fit naturally together, there are also forms of compositionality that rely less on previous learning, such as the bottom-up, parts-based representation of Hoffman and Richards (1984).
Learning models of novel handwritten characters can be operationalized in a similar way. Handwritten characters are inherently compositional, where the parts are pen strokes, and relations describe how these strokes connect to each other. Lake et al. (2015a) modeled these parts using an additional layer of compositionality, where parts are complex movements created from simpler sub-part movements. New characters can be constructed by combining parts, sub-parts, and relations in novel ways (Fig. 5). Compositionality is also central to the construction of other types of symbolic concepts beyond characters, where new spoken words can be created through a novel combination of phonemes (Lake et al. 2014), or a new gesture or dance move can be created through a combination of more primitive body movements.
An efficient representation for Frostbite should be similarly compositional and productive. A scene from the game is a composition of various object types, including birds, fish, ice floes, igloos, and so on (Fig. 2). Representing this compositional structure explicitly is both more economical and better for generalization, as noted in previous work on object-oriented reinforcement learning (Diuk et al. 2008). Many repetitions of the same objects are present at different locations in the scene, and therefore, representing each as an identical instance of the same object with the same properties is important for efficient representation and quick learning of the game. Further, new levels may contain different numbers and combinations of objects, where a compositional representation of objects – using intuitive physics and intuitive psychology as glue – would aid in making these crucial generalizations (Fig. 2D).
Deep neural networks have at least a limited notion of compositionality. Networks trained for object recognition encode part-like features in their deeper layers (Zeiler & Fergus 2014), whereby the presentation of new types of objects can activate novel combinations of feature detectors. Similarly, a DQN trained to play Frostbite may learn to represent multiple replications of the same object with the same features, facilitated by the invariance properties of a convolutional neural network architecture. Recent work has shown how this type of compositionality can be made more explicit, where neural networks can be used for efficient inference in more structured generative models (both neural networks and three-dimensional scene models) that explicitly represent the number of objects in a scene (Eslami et al. 2016). Beyond the compositionality inherent in parts, objects, and scenes, compositionality can also be important at the level of goals and sub-goals. Recent work on hierarchical DQNs shows that by providing explicit object representations to a DQN, and then defining sub-goals based on reaching those objects, DQNs can learn to play games with sparse rewards (such as Montezuma's Revenge) by combining these sub-goals together to achieve larger goals (Kulkarni et al. 2016).
We look forward to seeing these new ideas continue to develop, potentially providing even richer notions of compositionality in deep neural networks that lead to faster and more flexible learning. To capture the full extent of the mind's compositionality, a model must include explicit representations of objects, identity, and relations, all while maintaining a notion of “coherence” when understanding novel configurations. Coherence is related to our next principle, causality, which is discussed in the section that follows.
4.2.2. Causality
In concept learning and scene understanding, causal models represent hypothetical real-world processes that produce the perceptual observations. In control and reinforcement learning, causal models represent the structure of the environment, such as modeling state-to-state transitions or action/state-to-state transitions.
Concept learning and vision models that use causality are usually generative (as opposed to discriminative; see Glossary in Table 1), but not every generative model is also causal. Although a generative model describes a process for generating data, or at least assigns a probability distribution over possible data points, this generative process may not resemble how the data are produced in the real world. Causality refers to the subclass of generative models that resemble, at an abstract level, how the data are actually generated. Although generative neural networks such as Deep Belief Networks (Hinton et al. 2006) or variational auto-encoders (Gregor et al. 2016; Kingma et al. 2014) may generate compelling handwritten digits, they mark one end of the “causality spectrum,” because the steps of the generative process bear little resemblance to steps in the actual process of writing. In contrast, the generative model for characters using BPL does resemble the steps of writing, although even more causally faithful models are possible.
Causality has been influential in theories of perception. “Analysis-by-synthesis” theories of perception maintain that sensory data can be more richly represented by modeling the process that generated it (Bever & Poeppel 2010; Eden 1962; Halle & Stevens 1962; Neisser 1966). Relating data to their causal source provides strong priors for perception and learning, as well as a richer basis for generalizing in new ways and to new tasks. The canonical examples of this approach are speech and visual perception. For example, Liberman et al. (1967) argued that the richness of speech perception is best explained by inverting the production plan, at the level of vocal tract movements, to explain the large amounts of acoustic variability and the blending of cues across adjacent phonemes. As discussed, causality does not have to be a literal inversion of the actual generative mechanisms, as proposed in the motor theory of speech. For the BPL of learning handwritten characters, causality is operationalized by treating concepts as motor programs, or abstract causal descriptions of how to produce examples of the concept, rather than concrete configurations of specific muscles (Fig. 5A). Causality is an important factor in the model's success in classifying and generating new examples after seeing just a single example of a new concept (Lake et al. 2015a) (Fig. 5B).
Causal knowledge has also been shown to influence how people learn new concepts; providing a learner with different types of causal knowledge changes how he or she learns and generalizes. For example, the structure of the causal network underlying the features of a category influences how people categorize new examples (Rehder 2003; Rehder & Hastie 2001). Similarly, as related to the Characters Challenge, the way people learn to write a novel handwritten character influences later perception and categorization (Freyd 1983; 1987).
To explain the role of causality in learning, conceptual representations have been likened to intuitive theories or explanations, providing the glue that lets core features stick, whereas other equally applicable features wash away (Murphy & Medin 1985). Borrowing examples from Murphy and Medin (1985), the feature “flammable” is more closely attached to wood than money because of the underlying causal roles of the concepts, even though the feature is equally applicable to both. These causal roles derive from the functions of objects. Causality can also glue some features together by relating them to a deeper underlying cause, explaining why some features such as “can fly,” “has wings,” and “has feathers” co-occur across objects, whereas others do not.
Beyond concept learning, people also understand scenes by building causal models. Human-level scene understanding involves composing a story that explains the perceptual observations, drawing upon and integrating the ingredients of intuitive physics, intuitive psychology, and compositionality. Perception without these ingredients, and absent the causal glue that binds them, can lead to revealing errors. Consider image captions generated by a deep neural network (Fig. 6) (Karpathy & Fei-Fei 2017). In many cases, the network gets the key objects in a scene correct, but fails to understand the physical forces at work, the mental states of the people, or the causal relationships between the objects. In other words, it does not build the right causal model of the data.
There have been steps toward deep neural networks and related approaches that learn causal models. Lopez-Paz et al. (2015) introduced a discriminative, data-driven framework for distinguishing the direction of causality from examples. Although it outperforms existing methods on various causal prediction tasks, it is unclear how to apply the approach to inferring rich hierarchies of latent causal variables, as needed for the Frostbite Challenge and especially the Characters Challenge. Graves (2014) learned a generative model of cursive handwriting using a recurrent neural network trained on handwriting data. Although it synthesizes impressive examples of handwriting in various styles, it requires a large training corpus and has not been applied to other tasks. The DRAW network performs both recognition and generation of handwritten digits using recurrent neural networks with a window of attention, producing a limited circular area of the image at each time step (Gregor et al. 2015). A more recent variant of DRAW was applied to generating examples of a novel character from just a single training example (Rezende et al. 2016). The model demonstrates an impressive ability to make plausible generalizations that go beyond the training examples, yet it generalizes too broadly in other cases, in ways that are not especially human-like. It is not clear that it could yet pass any of the “visual Turing tests” in Lake et al. (2015a) (Fig. 5B), although we hope DRAW-style networks will continue to be extended and enriched, and could be made to pass these tests.
Incorporating causality may greatly improve these deep learning models; they were trained without access to causal data about how characters are actually produced, and without any incentive to learn the true causal process. An attentional window is only a crude approximation of the true causal process of drawing with a pen, and in Rezende et al. (2016) the attentional window is not pen-like at all, although a more accurate pen model could be incorporated. We anticipate that these sequential generative neural networks could make sharper one-shot inferences, with the goal of tackling the full Characters Challenge by incorporating additional causal, compositional, and hierarchical structure (and by continuing to use learning-to-learn, described next), potentially leading to a more computationally efficient and neurally grounded variant of the BPL model of handwritten characters (Fig. 5).
A causal model of Frostbite would have to be more complex, gluing together object representations and explaining their interactions with intuitive physics and intuitive psychology, much like the game engine that generates the game dynamics and, ultimately, the frames of pixel images. Inference is the process of inverting this causal generative model, explaining the raw pixels as objects and their interactions, such as the agent stepping on an ice floe to deactivate it or a crab pushing the agent into the water (Fig. 2). Deep neural networks could play a role in two ways: by serving as a bottom-up proposer to make probabilistic inference more tractable in a structured generative model (sect. 4.3.1) or by serving as the causal generative model if imbued with the right set of ingredients.
4.2.3. Learning-to-learn
When humans or machines make inferences that go far beyond the data, strong prior knowledge (or inductive biases or constraints) must be making up the difference (Geman et al. 1992; Griffiths et al. 2010; Tenenbaum et al. 2011). One way people acquire this prior knowledge is through “learning-to-learn,” a term introduced by Harlow (1949) and closely related to the machine learning notions of “transfer learning,” “multitask learning,” and “representation learning.” These terms refer to ways that learning a new task or a new concept can be accelerated through previous or parallel learning of other related tasks or other related concepts. The strong priors, constraints, or inductive bias needed to learn a particular task quickly are often shared to some extent with other related tasks. A range of mechanisms have been developed to adapt the learner's inductive bias as they learn specific tasks and then apply these inductive biases to new tasks.
In hierarchical Bayesian modeling (Gelman et al. 2004), a general prior on concepts is shared by multiple specific concepts, and the prior itself is learned over the course of learning the specific concepts (Salakhutdinov et al. 2012; 2013). These models have been used to explain the dynamics of human learning-to-learn in many areas of cognition, including word learning, causal learning, and learning intuitive theories of physical and social domains (Tenenbaum et al. 2011). In machine vision, for deep convolutional networks or other discriminative methods that form the core of recent recognition systems, learning-to-learn can occur through the sharing of features between the models learned for old objects or old tasks and the models learned for new objects or new tasks (Anselmi et al. 2016; Baxter 2000; Bottou 2014; Lopez-Paz et al. 2016; Rusu et al. 2016; Salakhutdinov et al. 2011; Srivastava & Salakhutdinov, 2013; Torralba et al. 2007; Zeiler & Fergus 2014). Neural networks can also learn-to-learn by optimizing hyper-parameters, including the form of their weight update rule (Andrychowicz et al. 2016), over a set of related tasks.
Although transfer learning and multitask learning are already important themes across AI, and in deep learning in particular, they have not yet led to systems that learn new tasks as rapidly and flexibly as humans do. Capturing more human-like learning-to-learn dynamics in deep networks and other machine learning approaches could facilitate much stronger transfer to new tasks and new problems. To gain the full benefit that humans get from learning-to-learn, however, AI systems might first need to adopt the more compositional (or more language-like, see sect. 5) and causal forms of representations that we have argued for above.
We can see this potential in both of our challenge problems. In the Characters Challenge as presented in Lake et al. (2015a), all viable models use “pre-training” on many character concepts in a background set of alphabets to tune the representations they use to learn new character concepts in a test set of alphabets. But to perform well, current neural network approaches require much more pre-training than do people or our Bayesian program learning approach. Humans typically learn only one or a few alphabets, and even with related drawing experience, this likely amounts to the equivalent of a few hundred character-like visual concepts at most. For BPL, pre-training with characters in only five alphabets (for around 150 character types in total) is sufficient to perform human-level one-shot classification and generation of new examples. With this level of pre-training, current neural networks perform much worse on classification and have not even attempted generation; they are still far from solving the Characters Challenge. 8 
We cannot be sure how people get to the knowledge they have in this domain, but we do understand how this works in BPL, and we think people might be similar. BPL transfers readily to new concepts because it learns about object parts, sub-parts, and relations, capturing learning about what each concept is like and what concepts are like in general. It is crucial that learning-to-learn occurs at multiple levels of the hierarchical generative process. Previously learned primitive actions and larger generative pieces can be re-used and re-combined to define new generative models for new characters (Fig. 5A). Further transfer occurs by learning about the typical levels of variability within a typical generative model. This provides knowledge about how far and in what ways to generalize when we have seen only one example of a new character, which on its own could not possibly carry any information about variance. BPL could also benefit from deeper forms of learning-to-learn than it currently does. Some of the important structure it exploits to generalize well is built in to the prior and not learned from the background pre-training, whereas people might learn this knowledge, and ultimately, a human-like machine learning system should as well.
Analogous learning-to-learn occurs for humans in learning many new object models, in vision and cognition: Consider the novel two-wheeled vehicle in Figure 1B, where learning-to-learn can operate through the transfer of previously learned parts and relations (sub-concepts such as wheels, motors, handle bars, attached, powered by) that reconfigure compositionally to create a model of the new concept. If deep neural networks could adopt similarly compositional, hierarchical, and causal representations, we expect they could benefit more from learning-to-learn.
In the Frostbite Challenge, and in video games more generally, there is a similar interdependence between the form of the representation and the effectiveness of learning-to-learn. People seem to transfer knowledge at multiple levels, from low-level perception to high-level strategy, exploiting compositionality at all levels. Most basically, they immediately parse the game environment into objects, types of objects, and causal relations between them. People also understand that video games like these have goals, which often involve approaching or avoiding objects based on their type. Whether the person is a child or a seasoned gamer, it seems obvious that interacting with the birds and fish will change the game state in some way, either good or bad, because video games typically yield costs or rewards for these types of interactions (e.g., dying or points). These types of hypotheses can be quite specific and rely on prior knowledge: When the polar bear first appears and tracks the agent's location during advanced levels (Fig. 2D), an attentive learner is sure to avoid it. Depending on the level, ice floes can be spaced far apart (Fig. 2A–C) or close together (Fig. 2D), suggesting the agent may be able to cross some gaps, but not others. In this way, general world knowledge and previous video games may help inform exploration and generalization in new scenarios, helping people learn maximally from a single mistake or avoid mistakes altogether.
Deep reinforcement learning systems for playing Atari games have had some impressive successes in transfer learning, but they still have not come close to learning to play new games as quickly as humans can. For example, Parisotto et al. (2016) present the “actor-mimic” algorithm that first learns 13 Atari games by watching an expert network play and trying to mimic the expert network action selection and/or internal states (for about 4 million frames of experience each, or 18.5 hours per game). This algorithm can then learn new games faster than a randomly initialized DQN: Scores that might have taken 4 or 5 million frames of learning to reach might now be reached after 1 or 2 million frames of practice. But anecdotally, we find that humans can still reach these scores with a few minutes of practice, requiring far less experience than the DQNs.
In sum, the interaction between representation and previous experience may be key to building machines that learn as fast as people. A deep learning system trained on many video games may not, by itself, be enough to learn new games as quickly as people. Yet, if such a system aims to learn compositionally structured causal models of each game – built on a foundation of intuitive physics and psychology – it could transfer knowledge more efficiently and thereby learn new games much more quickly.
4.3. Thinking Fast
The previous section focused on learning rich models from sparse data and proposed ingredients for achieving these human-like learning abilities. These cognitive abilities are even more striking when considering the speed of perception and thought: the amount of time required to understand a scene, think a thought, or choose an action. In general, richer and more structured models require more complex and slower inference algorithms, similar to how complex models require more data, making the speed of perception and thought all the more remarkable.
The combination of rich models with efficient inference suggests another way psychology and neuroscience may usefully inform AI. It also suggests an additional way to build on the successes of deep learning, where efficient inference and scalable learning are important strengths of the approach. This section discusses possible paths toward resolving the conflict between fast inference and structured representations, including Helmholtz machine–style approximate inference in generative models (Dayan et al. 1995; Hinton et al. 1995) and cooperation between model-free and model-based reinforcement learning systems.
4.3.1. Approximate inference in structured models
Hierarhical Bayesian models operating over probabilistic programs (Goodman et al. 2008; Lake et al. 2015a; Tenenbaum et al. 2011) are equipped to deal with theory-like structures and rich causal representations of the world, yet there are formidable algorithmic challenges for efficient inference. Computing a probability distribution over an entire space of programs is usually intractable, and often even finding a single high-probability program poses an intractable search problem. In contrast, whereas representing intuitive theories and structured causal models is less natural in deep neural networks, recent progress has demonstrated the remarkable effectiveness of gradient-based learning in high-dimensional parameter spaces. A complete account of learning and inference must explain how the brain does so much with limited computational resources (Gershman et al. 2015; Vul et al. 2014).
Popular algorithms for approximate inference in probabilistic machine learning have been proposed as psychological models (see Griffiths et al. [2012] for a review). Most prominently, it has been proposed that humans can approximate Bayesian inference using Monte Carlo methods, which stochastically sample the space of possible hypotheses and evaluate these samples according to their consistency with the data and prior knowledge (Bonawitz et al. 2014; Gershman et al. 2012; Ullman et al. 2012b; Vul et al. 2014). Monte Carlo sampling has been invoked to explain behavioral phenomena ranging from children's response variability (Bonawitz et al. 2014), to garden-path effects in sentence processing (Levy et al. 2009) and perceptual multistability (Gershman et al. 2012; Moreno-Bote et al. 2011). Moreover, we are beginning to understand how such methods could be implemented in neural circuits (Buesing et al. 2011; Huang & Rao 2014; Pecevski et al. 2011). 9 
Although Monte Carlo methods are powerful and come with asymptotic guarantees, it is challenging to make them work on complex problems like program induction and theory learning. When the hypothesis space is vast, and only a few hypotheses are consistent with the data, how can good models be discovered without exhaustive search? In at least some domains, people may not have an especially clever solution to this problem, instead grappling with the full combinatorial complexity of theory learning (Ullman et al. 2012b). Discovering new theories can be slow and arduous, as testified by the long time scale of cognitive development, and learning in a saltatory fashion (rather than through gradual adaptation) is characteristic of aspects of human intelligence, including discovery and insight during development (Schulz 2012b), problem-solving (Sternberg & Davidson 1995), and epoch-making discoveries in scientific research (Langley et al. 1987). Discovering new theories can also occur much more quickly. A person learning the rules of Frostbite will probably undergo a loosely ordered sequence of “Aha!” moments: He or she will learn that jumping on ice floes causes them to change color, that changing the color of ice floes causes an igloo to be constructed piece-by-piece, that birds make him or her lose points, that fish make him or her gain points, that he or she can change the direction of ice floes at the cost of one igloo piece, and so on. These little fragments of a “Frostbite theory” are assembled to form a causal understanding of the game relatively quickly, in what seems more like a guided process than arbitrary proposals in a Monte Carlo inference scheme. Similarly, as described in the Characters Challenge, people can quickly infer motor programs to draw a new character in a similarly guided processes.
For domains where program or theory learning occurs quickly, it is possible that people employ inductive biases not only to evaluate hypotheses, but also to guide hypothesis selection. Schulz (2012b) has suggested that abstract structural properties of problems contain information about the abstract forms of their solutions. Even without knowing the answer to the question, “Where is the deepest point in the Pacific Ocean?” one still knows that the answer must be a location on a map. The answer “20 inches” to the question, “What year was Lincoln born?” can be invalidated a priori, even without knowing the correct answer. In recent experiments, Tsividis et al. (2015) found that children can use high-level abstract features of a domain to guide hypothesis selection, by reasoning about distributional properties like the ratio of seeds to flowers, and dynamical properties like periodic or monotonic relationships between causes and effects (see also Magid et al. 2015).
How might efficient mappings from questions to a plausible subset of answers be learned? Recent work in AI, spanning both deep learning and graphical models, has attempted to tackle this challenge by “amortizing” probabilistic inference computations into an efficient feed-forward mapping (Eslami et al. 2014; Heess et al. 2013; Mnih & Gregor, 2014; Stuhlmüller et al. 2013). We can also think of this as “learning to do inference,” which is independent from the ideas of learning as model building discussed in the previous section. These feed-forward mappings can be learned in various ways, for example, using paired generative/recognition networks (Dayan et al. 1995; Hinton et al. 1995) and variational optimization (Gregor et al. 2015; Mnih & Gregor 2014; Rezende et al. 2014), or nearest-neighbor density estimation (Kulkarni et al. 2015a; Stuhlmüller et al. 2013). One implication of amortization is that solutions to different problems will become correlated because of the sharing of amortized computations. Some evidence for inferential correlations in humans was reported by Gershman and Goodman (2014). This trend is an avenue of potential integration of deep learning models with probabilistic models and probabilistic programming: Training neural networks to help perform probabilistic inference in a generative model or a probabilistic program (Eslami et al. 2016; Kulkarni et al. 2015b; Yildirim et al. 2015). Another avenue for potential integration is through differentiable programming (Dalrymple 2016), by ensuring that the program-like hypotheses are differentiable and thus learnable via gradient descent – a possibility discussed in the concluding section (Section 6.1).
4.3.2. Model-based and model-free reinforcement learning
The DQN introduced by Mnih et al. (2015) used a simple form of model-free reinforcement learning in a deep neural network that allows for fast selection of actions. There is indeed substantial evidence that the brain uses similar model-free learning algorithms in simple associative learning or discrimination learning tasks (see Niv 2009, for a review). In particular, the phasic firing of midbrain dopaminergic neurons is qualitatively (Schultz et al. 1997) and quantitatively (Bayer & Glimcher 2005) consistent with the reward prediction error that drives updating of model-free value estimates.
Model-free learning is not, however, the whole story. Considerable evidence suggests that the brain also has a model-based learning system, responsible for building a “cognitive map” of the environment and using it to plan action sequences for more complex tasks (Daw et al. 2005; Dolan & Dayan 2013). Model-based planning is an essential ingredient of human intelligence, enabling flexible adaptation to new tasks and goals; it is where all of the rich model-building abilities discussed in the previous sections earn their value as guides to action. As we argued in our discussion of Frostbite, one can design numerous variants of this simple video game that are identical except for the reward function; that is, governed by an identical environment model of state-action–dependent transitions. We conjecture that a competent Frostbite player can easily shift behavior appropriately, with little or no additional learning, and it is hard to imagine a way of doing that other than having a model-based planning approach in which the environment model can be modularly combined with arbitrary new reward functions and then deployed immediately for planning. One boundary condition on this flexibility is the fact that the skills become “habitized” with routine application, possibly reflecting a shift from model-based to model-free control. This shift may arise from a rational arbitration between learning systems to balance the trade-off between flexibility and speed (Daw et al. 2005; Keramati et al. 2011).
Similarly to how probabilistic computations can be amortized for efficiency (see previous section), plans can be amortized into cached values by allowing the model-based system to simulate training data for the model-free system (Sutton 1990). This process might occur offline (e.g., in dreaming or quiet wakefulness), suggesting a form of consolidation in reinforcement learning (Gershman et al. 2014). Consistent with the idea of cooperation between learning systems, a recent experiment demonstrated that model-based behavior becomes automatic over the course of training (Economides et al. 2015). Thus, a marriage of flexibility and efficiency might be achievable if we use the human reinforcement learning systems as guidance.
Intrinsic motivation also plays an important role in human learning and behavior (Berlyne 1966; Harlow 1950; Ryan & Deci 2007). Although much of the previous discussion assumes the standard view of behavior as seeking to maximize reward and minimize punishment, all externally provided rewards are reinterpreted according to the “internal value” of the agent, which may depend on the current goal and mental state. There may also be an intrinsic drive to reduce uncertainty and construct models of the environment (Edelman 2015; Schmidhuber 2015), closely related to learning-to-learn and multitask learning. Deep reinforcement learning is only just starting to address intrinsically motivated learning (Kulkarni et al. 2016; Mohamed & Rezende 2015).
5. Responses to common questions
In disussing the arguments in this article with colleagues, three lines of questioning or critiques have frequently arisen. We think it is helpful to address these points directly, to maximize the potential for moving forward together.
5.1. Comparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience
It may seem unfair to compare neural networks and humans on the amount of training experience required to perform a task, such as learning to play new Atari games or learning new handwritten characters, when humans have had extensive prior experience that these networks have not benefited from. People have had many hours playing other games, and experience reading or writing many other handwritten characters, not to mention experience in a variety of more loosely related tasks. If neural networks were “pre-trained” on the same experience, the argument goes, then they might generalize similarly to humans when exposed to novel tasks.
This has been the rationale behind multitask learning or transfer learning, a strategy with a long history that has shown some promising results recently with deep networks (e.g., Donahue et al. 2014; Luong et al. 2015; Parisotto et al. 2016). Furthermore, some deep learning advocates argue the human brain effectively benefits from even more experience through evolution. If deep learning researchers see themselves as trying to capture the equivalent of humans' collective evolutionary experience, this would be equivalent to a truly immense “pre-training” phase.
We agree that humans have a much richer starting point than neural networks when learning most new tasks, including learning a new concept or learning to play a new video game. That is the point of the “developmental start-up software” and other building blocks that we argued are key to creating this richer starting point. We are less committed to a particular story regarding the origins of the ingredients, including the relative roles of genetically programmed and experience-driven developmental mechanisms in building these components in early infancy. Either way, we see them as fundamental building blocks for facilitating rapid learning from sparse data.
Learning-to-learn across multiple tasks is conceivably one route to acquiring these ingredients, but simply training conventional neural networks on many related tasks may not be sufficient to generalize in human-like ways for novel tasks. As we argued in Section 4.2.3, successful learning-to-learn – or, at least, human-level transfer learning – is enabled by having models with the right representational structure, including the other building blocks discussed in this article. Learning-to-learn is a powerful ingredient, but it can be more powerful when operating over compositional representations that capture the underlying causal structure of the environment, while also building on intuitive physics and psychology.
Finally, we recognize that some researchers still hold out hope that if only they can just get big enough training data sets, sufficiently rich tasks, and enough computing power – far beyond what has been tried out so far – then deep learning methods might be sufficient to learn representations equivalent to what evolution and learning provide humans. We can sympathize with that hope, and believe it deserves further exploration, although we are not sure it is a realistic one. We understand in principle how evolution could build a brain with the cognitive ingredients we discuss here. Stochastic hill climbing is slow. It may require massively parallel exploration, over millions of years with innumerable dead ends, but it can build complex structures with complex functions if we are willing to wait long enough. In contrast, trying to build these representations from scratch using backpropagation, Deep Q-learning, or any stochastic gradient-descent weight update rule in a fixed network architecture, may be unfeasible regardless of how much training data are available. To build these representations from scratch might require exploring fundamental structural variations in the network's architecture, which gradient-based learning in weight space is not prepared to do. Although deep learning researchers do explore many such architectural variations, and have been devising increasingly clever and powerful ones recently, it is the researchers who are driving and directing this process. Exploration and creative innovation in the space of network architectures have not yet been made algorithmic. Perhaps they could, using genetic programming methods (Koza 1992) or other structure-search algorithms (Yamins et al. 2014). We think this would be a fascinating and promising direction to explore, but we may have to acquire more patience than machine-learning researchers typically express with their algorithms: the dynamics of structure search may look much more like the slow random hill climbing of evolution than the smooth, methodical progress of stochastic gradient descent. An alternative strategy is to build in appropriate infant-like knowledge representations and core ingredients as the starting point for our learning-based AI systems, or to build learning systems with strong inductive biases that guide them in this direction.
Regardless of which way an AI developer chooses to go, our main points are orthogonal to this objection. There are a set of core cognitive ingredients for human-like learning and thought. Deep learning models could incorporate these ingredients through some combination of additional structure and perhaps additional learning mechanisms, but for the most part have yet to do so. Any approach to human-like AI, whether based on deep learning or not, is likely to gain from incorporating these ingredients.
5.2. Biological plausibility suggests theories of intelligence should start with neural networks
We have focused on how cognitive science can motivate and guide efforts to engineer human-like AI, in contrast to some advocates of deep neural networks who cite neuroscience for inspiration. Our approach is guided by a pragmatic view that the clearest path to a computational formalization of human intelligence comes from understanding the “software” before the “hardware.” In the case of this article, we proposed key ingredients of this software in previous sections.
Nonetheless, a cognitive approach to intelligence should not ignore what we know about the brain. Neuroscience can provide valuable inspirations for both cognitive models and AI researchers: The centrality of neural networks and model-free reinforcement learning in our proposals for “thinking fast” (sect. 4.3) are prime exemplars. Neuroscience can also, in principle, impose constraints on cognitive accounts, at both the cellular and systems levels. If deep learning embodies brain-like computational mechanisms and those mechanisms are incompatible with some cognitive theory, then this is an argument against that cognitive theory and in favor of deep learning. Unfortunately, what we “know” about the brain is not all that clear-cut. Many seemingly well-accepted ideas regarding neural computation are in fact biologically dubious, or uncertain at best, and therefore should not disqualify cognitive ingredients that pose challenges for implementation within that approach.
For example, most neural networks use some form of gradient-based (e.g., backpropagation) or Hebbian learning. It has long been argued, however, that backpropagation is not biologically plausible. As Crick (1989) famously pointed out, backpropagation seems to require that information be transmitted backward along the axon, which does not fit with realistic models of neuronal function (although recent models circumvent this problem in various ways [Liao et al. 2015; Lillicrap et al. 2014; Scellier & Bengio 2016]). This has not prevented backpropagation from being put to good use in connectionist models of cognition or in building deep neural networks for AI. Neural network researchers must regard it as a very good thing, in this case, that concerns of biological plausibility did not hold back research on this particular algorithmic approach to learning. 10 We strongly agree: Although neuroscientists have not found any mechanisms for implementing backpropagation in the brain, neither have they produced definitive evidence against it. The existing data simply offer little constraint either way, and backpropagation has been of obviously great value in engineering today's best pattern recognition systems.
Hebbian learning is another case in point. In the form of long-term potentiation (LTP) and spike-timing dependent plasticity (STDP), Hebbian learning mechanisms are often cited as biologically supported (Bi & Poo 2001). However, the cognitive significance of any biologically grounded form of Hebbian learning is unclear. Gallistel and Matzel (2013) have persuasively argued that the critical interstimulus interval for LTP is orders of magnitude smaller than the intervals that are behaviorally relevant in most forms of learning. In fact, experiments that simultaneously manipulate the interstimulus and intertrial intervals demonstrate that no critical interval exists. Behavior can persist for weeks or months, whereas LTP decays to baseline over the course of days (Power et al. 1997). Learned behavior is rapidly re-acquired after extinction (Bouton 2004), whereas no such facilitation is observed for LTP (Jonge & Racine 1985). Most relevantly for our focus, it would be especially challenging to try to implement the ingredients described in this article using purely Hebbian mechanisms.
Claims of biological plausibility or implausibility usually rest on rather stylized assumptions about the brain that are wrong in many of their details. Moreover, these claims usually pertain to the cellular and synaptic levels, with few connections made to systems-level neuroscience and subcortical brain organization (Edelman 2015). Understanding which details matter and which do not requires a computational theory (Marr 1982). Moreover, in the absence of strong constraints from neuroscience, we can turn the biological argument around: Perhaps a hypothetical biological mechanism should be viewed with skepticism if it is cognitively implausible. In the long run, we are optimistic that neuroscience will eventually place more constraints on theories of intelligence. For now, we believe cognitive plausibility offers a surer foundation.
5.3. Language is essential for human intelligence. Why is it not more prominent here?
We have said little in this article about people's ability to communicate and think in natural language, a distinctively human cognitive capacity where machine capabilities strikingly lag. Certainly one could argue that language should be included on any short list of key ingredients in human intelligence: For example, Mikolov et al. (2016) featured language prominently in their recent paper sketching challenge problems and a road map for AI. Moreover, whereas natural language processing is an active area of research in deep learning (e.g., Bahdanau et al. 2015; Mikolov et al. 2013; Xu et al. 2015), it is widely recognized that neural networks are far from implementing human language abilities. The question is, how do we develop machines with a richer capacity for language?
We believe that understanding language and its role in intelligence goes hand-in-hand with understanding the building blocks discussed in this article. It is also true that language builds on the core abilities for intuitive physics, intuitive psychology, and rapid learning with compositional, causal models that we focus on. These capacities are in place before children master language, and they provide the building blocks for linguistic meaning and language acquisition (Carey 2009; Jackendoff 2003; Kemp 2007; O'Donnell 2015; Pinker 2007; Xu & Tenenbaum 2007). We hope that by better understanding these earlier ingredients and how to implement and integrate them computationally, we will be better positioned to understand linguistic meaning and acquisition in computational terms and to explore other ingredients that make human language possible.
What else might we need to add to these core ingredients to get language? Many researchers have speculated about key features of human cognition that give rise to language and other uniquely human modes of thought: Is it recursion, or some new kind of recursive structure building ability (Berwick & Chomsky 2016; Hauser et al. 2002)? Is it the ability to re-use symbols by name (Deacon 1998)? Is it the ability to understand others intentionally and build shared intentionality (Bloom 2000; Frank et al. 2009; Tomasello 2010)? Is it some new version of these things, or is it just more of the aspects of these capacities that are already present in infants? These are important questions for future work with the potential to expand the list of key ingredients; we did not intend our list to be complete.
Finally, we should keep in mind all of the ways that acquiring language extends and enriches the ingredients of cognition that we focus on in this article. The intuitive physics and psychology of infants are likely limited to reasoning about objects and agents in their immediate spatial and temporal vicinity and to their simplest properties and states. But with language, older children become able to reason about a much wider range of physical and psychological situations (Carey 2009). Language also facilitates more powerful learning-to-learn and compositionality (Mikolov et al. 2016), allowing people to learn more quickly and flexibly by representing new concepts and thoughts in relation to existing concepts (Lupyan & Bergen 2016; Lupyan & Clark 2015). Ultimately, the full project of building machines that learn and think like humans must have language at its core.
6. Looking forward
In the last few decades, AI and machine learning have made remarkable progress: Computer programs beat chess masters; AI systems beat Jeopardy champions; apps recognize photos of your friends; machines rival humans on large-scale object recognition; smart phones recognize (and, to a limited extent, understand) speech. The coming years promise still more exciting AI applications, in areas as varied as self-driving cars, medicine, genetics, drug design, and robotics. As a field, AI should be proud of these accomplishments, which have helped move research from academic journals into systems that improve our daily lives.
We should also be mindful of what AI has and has not achieved. Although the pace of progress has been impressive, natural intelligence is still by far the best example of intelligence. Machine performance may rival or exceed human performance on particular tasks, and algorithms may take inspiration from neuroscience or aspects of psychology, but it does not follow that the algorithm learns or thinks like a person. This is a higher bar worth reaching for, potentially leading to more powerful algorithms, while also helping unlock the mysteries of the human mind.
When comparing people with the current best algorithms in AI and machine learning, people learn from fewer data and generalize in richer and more flexible ways. Even for relatively simple concepts such as handwritten characters, people need to see just one or a few examples of a new concept before being able to recognize new examples, generate new examples, and generate new concepts based on related ones (Fig. 1A). So far, these abilities elude even the best deep neural networks for character recognition (Ciresan et al. 2012), which are trained on many examples of each concept and do not flexibly generalize to new tasks. We suggest that the comparative power and flexibility of people's inferences come from the causal and compositional nature of their representations.
We believe that deep learning and other learning paradigms can move closer to human-like learning and thought if they incorporate psychological ingredients, including those outlined in this article. Before closing, we discuss some recent trends that we see as some of the most promising developments in deep learning – trends we hope will continue and lead to more important advances.
6.1. Promising directions in deep learning
There has been recent interest in integrating psychological ingredients with deep neural networks, especially selective attention (Bahdanau et al. 2015; Mnih et al. 2014; Xu et al. 2015), augmented working memory (Graves et al. 2014; 2016; Grefenstette et al. 2015; Sukhbaatar et al. 2015; Weston et al. 2015b), and experience replay (McClelland et al. 1995; Mnih et al. 2015). These ingredients are lower-level than the key cognitive ingredients discussed in this article. yet they suggest a promising trend of using insights from cognitive psychology to improve deep learning, one that may be even furthered by incorporating higher-level cognitive ingredients.
Paralleling the human perceptual apparatus, selective attention forces deep learning models to process raw, perceptual data as a series of high-resolution “foveal glimpses” rather than all at once. Somewhat surprisingly, the incorporation of attention has led to substantial performance gains in a variety of domains, including in machine translation (Bahdanau et al. 2015), object recognition (Mnih et al. 2014), and image caption generation (Xu et al. 2015). Attention may help these models in several ways. It helps to coordinate complex, often sequential, outputs by attending to only specific aspects of the input, allowing the model to focus on smaller sub-tasks rather than solving an entire problem in one shot. For example, during caption generation, the attentional window has been shown to track the objects as they are mentioned in the caption, where the network may focus on a boy and then a Frisbee when producing a caption like, “A boy throws a Frisbee” (Xu et al. 2015). Attention also allows larger models to be trained without requiring every model parameter to affect every output or action. In generative neural network models, attention has been used to concentrate on generating particular regions of the image rather than the whole image at once (Gregor et al. 2015). This could be a stepping stone toward building more causal generative models in neural networks, such as a neural version of the Bayesian program learning model that could be applied to tackling the Characters Challenge (sect. 3.1).
Researchers are also developing neural networks with “working memories” that augment the shorter-term memory provided by unit activation and the longer-term memory provided by the connection weights (Graves et al. 2014; 2016; Grefenstette et al. 2015; Reed & Freitas 2016; Sukhbaatar et al. 2015; Weston et al. 2015b). These developments are also part of a broader trend toward “differentiable programming,” the incorporation of classic data structures, such as random access memory, stacks, and queues, into gradient-based learning systems (Dalrymple 2016). For example, the neural Turing machine (NTM) (Graves et al. 2014) and its successor the differentiable neural computer (DNC) (Graves et al. 2016) are neural networks augmented with a random access external memory with read and write operations that maintain end-to-end differentiability. The NTM has been trained to perform sequence-to-sequence prediction tasks such as sequence copying and sorting, and the DNC has been applied to solving block puzzles and finding paths between nodes in a graph after memorizing the graph. Additionally, neural programmer-interpreters learn to represent and execute algorithms such as addition and sorting from fewer examples, by observing input-output pairs (like the NTM and DNC), as well as execution traces (Reed & Freitas 2016). Each model seems to learn genuine programs from examples, albeit in a representation more like assembly language than a high-level programming language.
Although this new generation of neural networks has yet to tackle the types of challenge problems introduced in this article, differentiable programming suggests the intriguing possibility of combining the best of program induction and deep learning. The types of structured representations and model building ingredients discussed in this article – objects, forces, agents, causality, and compositionality – help explain important facets of human learning and thinking, yet they also bring challenges for performing efficient inference (sect. 4.3.1). Deep learning systems have not yet shown they can work with these representations, but they have demonstrated the surprising effectiveness of gradient descent in large models with high-dimensional parameter spaces. A synthesis of these approaches, able to perform efficient inference over programs that richly model
 the causal structure an infant sees in the world, would be a major step forward in building human-like AI.
Another example of combining pattern recognition and model-based search comes from recent AI research into the game Go. Go is considerably more difficult for AI than chess, and it was only recently that a computer program – AlphaGo – first beat a world-class player (Chouard 2016) by using a combination of deep convolutional neural networks (ConvNets) and Monte-Carlo Tree Search (Silver et al. 2016). Each of these components has made gains against artificial and real Go players (Gelly & Silver 2008; 2011; Silver et al. 2016; Tian & Zhu 2016), and the notion of combining pattern recognition and model-based search goes back decades in Go and other games. Showing that these approaches can be integrated to beat a human Go champion is an important AI accomplishment (see Fig. 7). Just as important, however, are the new questions and directions they open up for the long-term project of building genuinely human-like AI.
One worthy goal would be to build an AI system that beats a world-class player with the amount and kind of training human champions receive, rather than overpowering them with Google-scale computational resources. AlphaGo is initially trained on 28.4 million positions and moves from 160,000 unique games played by human experts; it then improves through reinforcement learning, playing 30 million more games against itself. Between the publication of Silver et al. (2016) and facing world champion Lee Sedol, AlphaGo was iteratively retrained several times in this way. The basic system always learned from 30 million games, but it played against successively stronger versions of itself, effectively learning from 100 million or more games altogether (D. Silver, personal communication, 2017). In contrast, Lee has probably played around 50,000 games in his entire life. Looking at numbers like these, it is impressive that Lee can even compete with AlphaGo. What would it take to build a professional-level Go AI that learns from only 50,000 games? Perhaps a system that combines the advances of AlphaGo with some of the complementary ingredients for intelligence we argue for here would be a route to that end.
Artificial intelligence could also gain much by trying to match the learning speed and flexibility of normal human Go players. People take a long time to master the game of Go, but as with the Frostbite and Characters challenges (sects. 3.1 and 3.2), humans can quickly learn the basics of the game through a combination of explicit instruction, watching others, and experience. Playing just a few games teaches a human enough to beat someone who has just learned the rules but never played before. Could AlphaGo model these earliest stages of real human learning curves? Human Go players can also adapt what they have learned to innumerable game variants. The Wikipedia page “Go variants” describes versions such as playing on bigger or smaller board sizes (ranging from 9 × 9 to 38 × 38, not just the usual 19 × 19 board), or playing on boards of different shapes and connectivity structures (rectangles, triangles, hexagons, even a map of the English city Milton Keynes). The board can be a torus, a mobius strip, a cube, or a diamond lattice in three dimensions. Holes can be cut in the board, in regular or irregular ways. The rules can be adapted to what is known as First Capture Go (the first player to capture a stone wins), NoGo (the player who avoids capturing any enemy stones longer wins), or Time Is Money Go (players begin with a fixed amount of time and at the end of the game, the number of seconds remaining on each player's clock is added to his or her score). Players may receive bonuses for creating certain stone patterns or capturing territory near certain landmarks. There could be four or more players, competing individually or in teams. In each of these variants, effective play needs to change from the basic game, but a skilled player can adapt, and does not simply have to relearn the game from scratch. Could AlphaGo quickly adapt to new variants of Go? Although techniques for handling variable-sized inputs in ConvNets may help in playing on different board sizes (Sermanet et al. 2014), the value functions and policies that AlphaGo learns seem unlikely to generalize as flexibly and automatically as people. Many of the variants described above would require significant reprogramming and retraining, directed by the smart humans who programmed AlphaGo, not the system itself. As impressive as AlphaGo is in beating the world's best players at the standard game – and it is extremely impressive – the fact that it cannot even conceive of these variants, let alone adapt to them autonomously, is a sign that it does not understand the game as humans do. Human players can understand these variants and adapt to them because they explicitly represent Go as a game, with a goal to beat an adversary who is playing to achieve the same goal he or she is, governed by rules about how stones can be placed on a board and how board positions are scored. Humans represent their strategies as a response to these constraints, such that if the game changes, they can begin to adjust their strategies accordingly.
In sum, Go presents compelling challenges for AI beyond matching world-class human performance, in trying to match human levels of understanding and generalization, based on the same kinds and amounts of data, explicit instructions, and opportunities for social learning afforded to people. In learning to play Go as quickly and as flexibly as they do, people are drawing on most of the cognitive ingredients this article has laid out. They are learning-to-learn with compositional knowledge. They are using their core intuitive psychology and aspects of their intuitive physics (spatial and object representations). And like AlphaGo, they are also integrating model-free pattern recognition with model-based search. We believe that Go AI systems could be built to do all of these things, potentially better capturing how humans learn and understand the game. We believe it would be richly rewarding for AI and cognitive science to pursue this challenge together and that such systems could be a compelling testbed for the principles this article suggests, as well as building on all of the progress to date that AlphaGo represents.
6.2. Future applications to practical AI problems
In this article, we suggested some ingredients for building computational models with more human-like learning and thought. These principles were explained in the context of the Characters and Frostbite Challenges, with special emphasis on reducing the amount of training data required and facilitating transfer to novel yet related tasks. We also see ways these ingredients can spur progress on core AI problems with practical applications. Here we offer some speculative thoughts on these applications. 
1. Scene understanding. Deep learning is moving beyond object recognition and toward scene understanding, as evidenced by a flurry of recent work focused on generating natural language captions for images (Karpathy & Fei-Fei 2017; Vinyals et al. 2014; Xu et al. 2015). Yet current algorithms are still better at recognizing objects than understanding scenes, often getting the key objects right but their causal relationships wrong (Fig. 6). We see compositionality, causality, intuitive physics, and intuitive psychology as playing an increasingly important role in reaching true scene understanding. For example, picture a cluttered garage workshop with screw drivers and hammers hanging from the wall, wood pieces and tools stacked precariously on a work desk, and shelving and boxes framing the scene. For an autonomous agent to effectively navigate and perform tasks in this environment, the agent would need intuitive physics to properly reason about stability and support. A holistic model of the scene would require the composition of individual object models, glued together by relations. Finally, causality helps infuse the recognition of existing tools or the learning of new ones with an understanding of their use, helping to connect different object models in the proper way (e.g., hammering a nail into a wall, or using a saw horse to support a beam being cut by a saw). If the scene includes people acting or interacting, it will be nearly impossible to understand their actions without thinking about their thoughts and especially their goals and intentions toward the other objects and agents they believe are present.
2. Autonomous agents and intelligent devices. Robots and personal assistants such as cell phones cannot be pre-trained on all possible concepts they may encounter. Like a child learning the meaning of new words, an intelligent and adaptive system should be able to learn new concepts from a small number of examples, as they are encountered naturally in the environment. Common concept types include new spoken words (names like “Ban Ki-Moon” and “Kofi Annan”), new gestures (a secret handshake and a “fist bump”), and new activities, and a human-like system would be able to learn both to recognize and to produce new instances from a small number of examples. As with handwritten characters, a system may be able to quickly learn new concepts by constructing them from pre-existing primitive actions, informed by knowledge of the underlying causal process and learning-to-learn.
3. Autonomous driving. Perfect autonomous driving requires intuitive psychology. Beyond detecting and avoiding pedestrians, autonomous cars could more accurately predict pedestrian behavior by inferring mental states, including their beliefs (e.g., Do they think it is safe to cross the street? Are they paying attention?) and desires (e.g., Where do they want to go? Do they want to cross? Are they retrieving a ball lost in the street?). Similarly, other drivers on the road have similarly complex mental states underlying their behavior (e.g., Does he or she want to change lanes? Pass another car? Is he or she swerving to avoid a hidden hazard? Is he or she distracted?). This type of psychological reasoning, along with other types of model-based causal and physical reasoning, are likely to be especially valuable in challenging and novel driving circumstances for which there are few relevant training data (e.g., navigating unusual construction zones, natural disasters).
4. Creative design. Creativity is often thought to be a pinnacle of human intelligence. Chefs design new dishes, musicians write new songs, architects design new buildings, and entrepreneurs start new businesses. Although we are still far from developing AI systems that can tackle these types of tasks, we see compositionality and causality as central to this goal. Many commonplace acts of creativity are combinatorial, meaning they are unexpected combinations of familiar concepts or ideas (Boden 1998; Ward 1994). As illustrated in Figure 1-iv, novel vehicles can be created as a combination of parts from existing vehicles, and similarly, novel characters can be constructed from the parts of stylistically similar characters, or familiar characters can be re-conceptualized in novel styles (Rehling 2001). In each case, the free combination of parts is not enough on its own: Although compositionality and learning-to-learn can provide the parts for new ideas, causality provides the glue that gives them coherence and purpose.
6.3. Toward more human-like learning and thinking machines
Since the birth of AI in the 1950s, people have wanted to build machines that learn and think like people. We hope researchers in AI, machine learning, and cognitive science will accept our challenge problems as a testbed for progress. Rather than just building systems that recognize handwritten characters and play Frostbite or Go as the end result of an asymptotic process, we suggest that deep learning and other computational paradigms should aim to tackle these tasks using as few training data as people need, and also to evaluate models on a range of human-like generalizations beyond the one task on which the model was trained. We hope that the ingredients outlined in this article will prove useful for working toward this goal: seeing objects and agents rather than features, building causal models and not just recognizing patterns, recombining representations without needing to retrain, and learning-to-learn rather than starting from scratch

